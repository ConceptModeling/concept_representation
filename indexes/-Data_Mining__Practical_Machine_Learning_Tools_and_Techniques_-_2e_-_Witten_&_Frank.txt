Data MiningPractical Machine Learning Tools and TechniquesP088407-FM.qxd  5/3/05  2:21 PM  Page iThe Morgan Kaufmann Series in Data Management SystemsSeries Editor:Jim Gray,Microsoft ResearchData Mining: Practical Machine LearningTools and Techniques,Second EditionIan H.Witten and Eibe FrankFuzzy Modeling and Genetic Algorithms forData Mining and ExplorationEarl CoxData Modeling Essentials,Third EditionGraeme C.Simsion and Graham C.WittLocation-Based ServicesJochen Schiller and Agnès VoisardDatabase Modeling with Microsoft® Visio forEnterprise ArchitectsTerry Halpin,Ken Evans,Patrick Hallock,and Bill MacleanDesigning Data-Intensive Web ApplicationsStefano Ceri,Piero Fraternali,Aldo Bongio,Marco Brambilla,Sara Comai,andMaristella MateraMining the Web: Discovering Knowledgefrom Hypertext DataSoumen ChakrabartiAdvanced SQL: 1999—UnderstandingObject-Relational and Other AdvancedFeaturesJim MeltonDatabase Tuning: Principles,Experiments,and Troubleshooting TechniquesDennis Shasha and Philippe BonnetSQL: 1999—Understanding RelationalLanguage ComponentsJim Melton and Alan R.SimonInformation Visualization in Data Miningand Knowledge DiscoveryEdited by Usama Fayyad,Georges G.Grinstein,and Andreas WierseTransactional Information Systems: Theory,Algorithms,and the Practice ofConcurrencyControl and RecoveryGerhard Weikum and Gottfried VossenSpatial Databases: With Application to GISPhilippe Rigaux,Michel Scholl,and AgnèsVoisardInformation Modeling and RelationalDatabases: From Conceptual Analysis toLogical DesignTerry HalpinComponent Database SystemsEdited by Klaus R.Dittrich and AndreasGeppertManaging Reference Data in EnterpriseDatabases: Binding Corporate Data to theWider WorldMalcolm ChisholmData Mining: Concepts and TechniquesJiawei Han and Micheline KamberUnderstanding SQL and Java Together: AGuide to SQLJ,JDBC,and RelatedTechnologiesJim Melton and Andrew EisenbergDatabase: Principles,Programming,andPerformance,Second EditionPatrick O’Neil and Elizabeth O’NeilThe Object Data Standard: ODMG 3.0Edited by R.G.G.Cattell,Douglas K.Barry,Mark Berler,JeffEastman,DavidJordan,Craig Russell,OlafSchadow,Torsten Stanienda,and Fernando VelezData on the Web: From Relations toSemistructured Data and XMLSerge Abiteboul,Peter Buneman,and DanSuciuData Mining: Practical Machine LearningTools and Techniques with JavaImplementationsIan H.Witten and Eibe FrankJoe Celko’s SQL for Smarties: Advanced SQLProgramming,Second EditionJoe CelkoJoe Celko’s Data and Databases: Concepts inPracticeJoe CelkoDeveloping Time-Oriented DatabaseApplications in SQLRichard T.SnodgrassWeb Farming for the Data WarehouseRichard D.HackathornDatabase Modeling & Design,Third EditionToby J.TeoreyManagement ofHeterogeneous andAutonomous Database SystemsEdited by Ahmed Elmagarmid,MarekRusinkiewicz,and Amit ShethObject-Relational DBMSs: Tracking the NextGreat Wave,Second EditionMichael Stonebraker and Paul Brown,withDorothy MooreA Complete Guide to DB2 UniversalDatabaseDon ChamberlinUniversal Database Management: A Guideto Object/Relational TechnologyCynthia Maro SaraccoReadings in Database Systems,Third EditionEdited by Michael Stonebraker and JosephM.HellersteinUnderstanding SQL’s Stored Procedures: AComplete Guide to SQL/PSMJim MeltonPrinciples ofMultimedia Database SystemsV.S.SubrahmanianPrinciples ofDatabase Query Processing forAdvanced ApplicationsClement T.Yu and Weiyi MengAdvanced Database SystemsCarlo Zaniolo,Stefano Ceri,ChristosFaloutsos,Richard T.Snodgrass,V.S.Subrahmanian,and Roberto ZicariPrinciples ofTransaction Processing for theSystems ProfessionalPhilip A.Bernstein and Eric NewcomerUsing the New DB2: IBM’s Object-RelationalDatabase SystemDon ChamberlinDistributed AlgorithmsNancy A.LynchActive Database Systems: Triggers and RulesFor Advanced Database ProcessingEdited by Jennifer Widom and Stefano CeriMigrating Legacy Systems: Gateways,Interfaces & the Incremental ApproachMichael L.Brodie and Michael StonebrakerAtomic TransactionsNancy Lynch,Michael Merritt,WilliamWeihl,and Alan FeketeQuery Processing For Advanced DatabaseSystemsEdited by Johann Christoph Freytag,DavidMaier,and Gottfried VossenTransaction Processing: Concepts andTechniquesJim Gray and Andreas ReuterBuilding an Object-Oriented DatabaseSystem: The Story ofO2Edited by François Bancilhon,ClaudeDelobel,and Paris KanellakisDatabase Transaction Models For AdvancedApplicationsEdited by Ahmed K.ElmagarmidA Guide to Developing Client/Server SQLApplicationsSetrag Khoshaﬁan,Arvola Chan,AnnaWong,and Harry K.T.WongThe Benchmark Handbook For Databaseand Transaction Processing Systems,SecondEditionEdited by Jim GrayCamelot and Avalon: A DistributedTransaction FacilityEdited by Jeffrey L.Eppinger,Lily B.Mummert,and Alfred Z.SpectorReadings in Object-Oriented DatabaseSystemsEdited by Stanley B.Zdonik and DavidMaierP088407-FM.qxd  5/3/05  5:42 PM  Page iiData MiningPractical Machine Learning Tools and Techniques,Second EditionIan H. WittenDepartment ofComputer ScienceUniversity ofWaikatoEibe FrankDepartment ofComputer ScienceUniversity ofWaikatoAMSTERDAM•BOSTON•HEIDELBERG•LONDONNEW YORK•OXFORD•PARIS•SAN DIEGOSAN FRANCISCO•SINGAPORE•SYDNEY•TOKYOMORGAN KAUFMANN PUBLISHERS IS AN IMPRINT OF ELSEVIERP088407-FM.qxd  4/30/05  10:55 AM  Page iiiPublisher:Diane CerraPublishing Services Manager:Simon CrumpProject Manager:Brandy LillyEditorial Assistant:Asma StephanCover Design:Yvo Riezebos DesignCover Image:Getty ImagesComposition:SNP Best-set Typesetter Ltd.,Hong KongTechnical Illustration:Dartmouth Publishing,Inc.Copyeditor:Graphic World Inc.Proofreader:Graphic World Inc.Indexer:Graphic World Inc.Interior printer:The Maple-Vail Book Manufacturing GroupCover printer:Phoenix Color CorpMorgan Kaufmann Publishers is an imprint ofElsevier.500 Sansome Street,Suite 400,San Francisco,CA 94111This book is printed on acid-free paper.© 2005 by Elsevier Inc.All rights reserved.Designations used by companies to distinguish their products are often claimed as trademarksor registered trademarks.In all instances in which Morgan Kaufmann Publishers is aware ofaclaim,the product names appear in initial capital or all capital letters.Readers,however,shouldcontact the appropriate companies for more complete information regarding trademarks andregistration.No part ofthis publication may be reproduced,stored in a retrieval system,or transmitted inany form or by any means—electronic,mechanical,photocopying,scanning,or otherwise—without prior written permission ofthe publisher.Permissions may be sought directly from Elsevier’s Science & Technology Rights Department inOxford,UK:phone:(+44) 1865 843830,fax:(+44) 1865 853333,e-mail:permissions@elsevier.com.uk.You may also complete your request on-line via the Elsevierhomepage (http://elsevier.com) by selecting “Customer Support”and then “ObtainingPermissions.”Library ofCongress Cataloging-in-Publication DataWitten,I.H.(Ian H.)Data mining :practical machine learning tools and techniques / Ian H.Witten,Eibe Frank.– 2nd ed.p.cm.– (Morgan Kaufmann series in data management systems)Includes bibliographical references and index.ISBN:0-12-088407-01.Data mining.I.Frank,Eibe.II.Title.III.Series.QA76.9.D343W58 2005006.3–dc222005043385For information on all Morgan Kaufmann publications,visit our Web site at www.mkp.comor www.books.elsevier.comPrinted in the United States ofAmerica050607080954321Working together to grow libraries in developing countrieswww.elsevier.com  |  www.bookaid.org  |  www.sabre.orgP088407-FM.qxd  5/3/05  2:22 PM  Page ivForewordJim Gray,Series EditorMicrosoft ResearchTechnology now allows us to capture and store vast quantities ofdata.Findingpatterns,trends,and anomalies in these datasets,and summarizing them with simple quantitative models,is one ofthe grand challenges ofthe infor-mation age—turning data into information and turning information intoknowledge.There has been stunning progress in data mining and machine learning.Thesynthesis ofstatistics,machine learning,information theory,and computing hascreated a solid science,with a ﬁrm mathematical base,and with very powerfultools.Witten and Frank present much ofthis progress in this book and in thecompanion implementation ofthe key algorithms.As such,this is a milestonein the synthesis ofdata mining,data analysis,information theory,and machinelearning.Ifyou have not been following this ﬁeld for the last decade,this is agreat way to catch up on this exciting progress.Ifyou have,then Witten andFrank’s presentation and the companion open-source workbench,called Weka,will be a useful addition to your toolkit.They present the basic theory ofautomatically extracting models from data,and then validating those models.The book does an excellent job ofexplainingthe various models (decision trees,association rules,linear models,clustering,Bayes nets,neural nets) and how to apply them in practice.With this basis,theythen walk through the steps and pitfalls ofvarious approaches.They describehow to safely scrub datasets,how to build models,and how to evaluate a model’spredictive quality.Most ofthe book is tutorial,but Part II broadly describes howcommercial systems work and gives a tour ofthe publicly available data miningworkbench that the authors provide through a website.This Weka workbenchhas a graphical user interface that leads you through data mining tasks and hasexcellent data visualization tools that help understand the models.It is a greatcompanion to the text and a useful and popular tool in its own right.vP088407-FM.qxd  5/3/05  2:23 PM  Page vThis book presents this new discipline in a very accessible form:as a text both to train the next generation ofpractitioners and researchers and to informlifelong learners like myself.Witten and Frank have a passion for simple andelegant solutions.They approach each topic with this mindset,grounding allconcepts in concrete examples,and urging the reader to consider the simpletechniques ﬁrst,and then progress to the more sophisticated ones ifthe simpleones prove inadequate.Ifyou are interested in databases,and have not been following the machinelearning ﬁeld,this book is a great way to catch up on this exciting progress.Ifyou have data that you want to analyze and understand,this book and the asso-ciated Weka toolkit are an excellent way to start.viFOREWORDP088407-FM.qxd  5/3/05  2:23 PM  Page viContentsForewordvPrefacexxiiiUpdated and revised contentxxviiAcknowledgmentsxxixPart IMachine learning tools and techniques11What’s it all about?31.1Data mining and machine learning4Describing structural patterns6Machine learning7Data mining91.2Simple examples:The weather problem and others9The weather problem10Contact lenses: An idealized problem13Irises: A classic numeric dataset15CPU performance: Introducing numeric prediction16Labor negotiations: A more realistic example17Soybean classiﬁcation: A classic machine learning success181.3Fielded applications22Decisions involving judgment22Screening images23Load forecasting24Diagnosis25Marketing and sales26Other applications28viiP088407-FM.qxd  4/30/05  10:55 AM  Page vii1.4Machine learning and statistics291.5Generalization as search30Enumerating the concept space31Bias321.6Data mining and ethics351.7Further reading372Input: Concepts, instances, and attributes412.1What’s a concept?422.2What’s in an example?452.3What’s in an attribute?492.4Preparing the input52Gathering the data together52ARFF format53Sparse data55Attribute types56Missing values58Inaccurate values59Getting to know your data602.5Further reading603Output: Knowledge representation613.1Decision tables623.2Decision trees623.3Classiﬁcation rules653.4Association rules693.5Rules with exceptions703.6Rules involving relations733.7Trees for numeric prediction763.8Instance-based representation763.9Clusters813.10Further reading82viiiCONTENTSP088407-FM.qxd  4/30/05  10:55 AM  Page viii4Algorithms: The basic methods834.1Inferring rudimentary rules84Missing values and numeric attributes86Discussion884.2Statistical modeling88Missing values and numeric attributes92Bayesian models for document classiﬁcation94Discussion964.3Divide-and-conquer:Constructing decision trees97Calculating information100Highly branching attributes102Discussion1054.4Covering algorithms:Constructing rules105Rules versus trees107A simple covering algorithm107Rules versus decision lists1114.5Mining association rules112Item sets113Association rules113Generating rules efﬁciently117Discussion1184.6Linear models119Numeric prediction: Linear regression119Linear classiﬁcation: Logistic regression121Linear classiﬁcation using the perceptron124Linear classiﬁcation using Winnow1264.7Instance-based learning128The distance function128Finding nearest neighbors efﬁciently129Discussion1354.8Clustering136Iterative distance-based clustering137Faster distance calculations138Discussion1394.9Further reading139CONTENTSixP088407-FM.qxd  4/30/05  10:55 AM  Page ix5Credibility: Evaluating what’s been learned1435.1Training and testing1445.2Predicting performance1465.3Cross-validation1495.4Other estimates151Leave-one-out151The bootstrap1525.5Comparing data mining methods1535.6Predicting probabilities157Quadratic loss function158Informational loss function159Discussion1605.7Counting the cost161Cost-sensitive classiﬁcation164Cost-sensitive learning165Lift charts166ROC curves168Recall–precision curves171Discussion172Cost curves1735.8Evaluating numeric prediction1765.9The minimum description length principle1795.10Applying the MDL principle to clustering1835.11Further reading1846Implementations: Real machine learning schemes1876.1Decision trees189Numeric attributes189Missing values191Pruning192Estimating error rates193Complexity ofdecision tree induction196From trees to rules198C4.5: Choices and options198Discussion1996.2Classiﬁcation rules200Criteria for choosing tests200Missing values,numeric attributes201xCONTENTSP088407-FM.qxd  4/30/05  10:55 AM  Page xGenerating good rules202Using global optimization205Obtaining rules from partial decision trees207Rules with exceptions210Discussion2136.3Extending linear models214The maximum margin hyperplane215Nonlinear class boundaries217Support vector regression219The kernel perceptron222Multilayer perceptrons223Discussion2356.4Instance-based learning235Reducing the number ofexemplars236Pruning noisy exemplars236Weighting attributes237Generalizing exemplars238Distance functions for generalized exemplars239Generalized distance functions241Discussion2426.5Numeric prediction243Model trees244Building the tree245Pruning the tree245Nominal attributes246Missing values246Pseudocode for model tree induction247Rules from model trees250Locally weighted linear regression251Discussion2536.6Clustering254Choosing the number ofclusters254Incremental clustering255Category utility260Probability-based clustering262The EM algorithm265Extending the mixture model266Bayesian clustering268Discussion2706.7Bayesian networks271Making predictions272Learning Bayesian networks276CONTENTSxiP088407-FM.qxd  4/30/05  10:55 AM  Page xiSpeciﬁc algorithms278Data structures for fast learning280Discussion2837Transformations: Engineering the input and output2857.1Attribute selection288Scheme-independent selection290Searching the attribute space292Scheme-speciﬁc selection2947.2Discretizing numeric attributes296Unsupervised discretization297Entropy-based discretization298Other discretization methods302Entropy-based versus error-based discretization302Converting discrete to numeric attributes3047.3Some useful transformations305Principal components analysis306Random projections309Text to attribute vectors309Time series3117.4Automatic data cleansing312Improving decision trees312Robust regression313Detecting anomalies3147.5Combining multiple models315Bagging316Bagging with costs319Randomization320Boosting321Additive regression325Additive logistic regression327Option trees328Logistic model trees331Stacking332Error-correcting output codes3347.6Using unlabeled data337Clustering for classiﬁcation337Co-training339EM and co-training3407.7Further reading341xiiCONTENTSP088407-FM.qxd  4/30/05  10:55 AM  Page xii8Moving on: Extensions and applications3458.1Learning from massive datasets3468.2Incorporating domain knowledge3498.3Text and Web mining3518.4Adversarial situations3568.5Ubiquitous data mining3588.6Further reading361Part IIThe Weka machine learning workbench3639Introduction to Weka3659.1What’s in Weka?3669.2How do you use it?3679.3What else can you do?3689.4How do you get it?36810The Explorer36910.1Getting started369Preparing the data370Loading the data into the Explorer370Building a decision tree373Examining the output373Doing it again377Working with models377When things go wrong37810.2Exploring the Explorer380Loading and ﬁltering ﬁles380Training and testing learning schemes384Do it yourself: The User Classiﬁer388Using a metalearner389Clustering and association rules391Attribute selection392Visualization39310.3Filtering algorithms393Unsupervised attribute ﬁlters395Unsupervised instance ﬁlters400Supervised ﬁlters401CONTENTSxiiiP088407-FM.qxd  4/30/05  10:55 AM  Page xiii10.4Learning algorithms403Bayesian classiﬁers403Trees406Rules408Functions409Lazy classiﬁers413Miscellaneous classiﬁers41410.5Metalearning algorithms414Bagging and randomization414Boosting416Combining classiﬁers417Cost-sensitive learning417Optimizing performance417Retargeting classiﬁers for different tasks41810.6Clustering algorithms41810.7Association-rule learners41910.8Attribute selection420Attribute subset evaluators422Single-attribute evaluators422Search methods42311The Knowledge Flow interface42711.1Getting started42711.2The Knowledge Flow components43011.3Conﬁguring and connecting the components43111.4Incremental learning43312The Experimenter43712.1Getting started438Running an experiment439Analyzing the results44012.2Simple setup44112.3Advanced setup44212.4The Analyze panel44312.5Distributing processing over several machines445xivCONTENTSP088407-FM.qxd  4/30/05  10:55 AM  Page xiv13The command-line interface44913.1Getting started44913.2The structure ofWeka450Classes,instances,and packages450The weka.core package451The weka.classiﬁers package453Other packages455Javadoc indices45613.3Command-line options456Generic options456Scheme-speciﬁc options45814Embedded machine learning46114.1A simple data mining application46114.2Going through the code462main()462MessageClassiﬁer()462updateData()468classifyMessage()46815Writing new learning schemes47115.1An example classiﬁer471buildClassiﬁer()472makeTree()472computeInfoGain()480classifyInstance()480main()48115.2Conventions for implementing classiﬁers483References485Index505About the authors525CONTENTSxvP088407-FM.qxd  5/3/05  9:13 AM  Page xvP088407-FM.qxd  4/30/05  10:55 AM  Page xviList of FiguresFigure 1.1Rules for the contact lens data.13Figure 1.2Decision tree for the contact lens data.14Figure 1.3Decision trees for the labor negotiations data.19Figure 2.1A family tree and two ways ofexpressing the sister-ofrelation.46Figure 2.2ARFF ﬁle for the weather data.54Figure 3.1Constructing a decision tree interactively:(a) creating arectangular test involving petallengthand petalwidthand (b)the resulting (unﬁnished) decision tree.64Figure 3.2Decision tree for a simple disjunction.66Figure 3.3The exclusive-or problem.67Figure 3.4Decision tree with a replicated subtree.68Figure 3.5Rules for the Iris data.72Figure 3.6The shapes problem.73Figure 3.7Models for the CPU performance data:(a) linear regression,(b) regression tree,and (c) model tree.77Figure 3.8Different ways ofpartitioning the instance space.79Figure 3.9Different ways ofrepresenting clusters.81Figure 4.1Pseudocode for 1R.85Figure 4.2Tree stumps for the weather data.98Figure 4.3Expanded tree stumps for the weather data.100Figure 4.4Decision tree for the weather data.101Figure 4.5Tree stump for the ID codeattribute.103Figure 4.6Covering algorithm:(a) covering the instances and (b) thedecision tree for the same problem.106Figure 4.7The instance space during operation ofa covering algorithm.108Figure 4.8Pseudocode for a basic rule learner.111Figure 4.9Logistic regression:(a) the logit transform and (b) an examplelogistic regression function.122xviiP088407-FM.qxd  4/30/05  10:55 AM  Page xviiFigure 4.10The perceptron:(a) learning rule and (b) representation as a neural network.125Figure 4.11The Winnow algorithm:(a) the unbalanced version and (b) the balanced version.127Figure 4.12A kD-tree for four training instances:(a) the tree and (b)instances and splits.130Figure 4.13Using a kD-tree to ﬁnd the nearest neighbor ofthe star.131Figure 4.14Ball tree for 16 training instances:(a) instances and balls and(b) the tree.134Figure 4.15Ruling out an entire ball (gray) based on a target point (star)and its current nearest neighbor.135Figure 4.16A ball tree:(a) two cluster centers and their dividing line and(b) the corresponding tree.140Figure 5.1A hypothetical lift chart.168Figure 5.2A sample ROC curve.169Figure 5.3ROC curves for two learning methods.170Figure 5.4Effects ofvarying the probability threshold:(a) the error curveand (b) the cost curve.174Figure 6.1Example ofsubtree raising,where node C is “raised”tosubsume node B.194Figure 6.2Pruning the labor negotiations decision tree.196Figure 6.3Algorithm for forming rules by incremental reduced-errorpruning.205Figure 6.4RIPPER:(a) algorithm for rule learning and (b) meaning ofsymbols.206Figure 6.5Algorithm for expanding examples into a partial tree.208Figure 6.6Example ofbuilding a partial tree.209Figure 6.7Rules with exceptions for the iris data.211Figure 6.8A maximum margin hyperplane.216Figure 6.9Support vector regression:(a) e=1,(b) e=2,and (c) e=0.5.221Figure 6.10Example datasets and corresponding perceptrons.225Figure 6.11Step versus sigmoid:(a) step function and (b) sigmoidfunction.228Figure 6.12Gradient descent using the error function x2+1.229Figure 6.13Multilayer perceptron with a hidden layer.231Figure 6.14A boundary between two rectangular classes.240Figure 6.15Pseudocode for model tree induction.248Figure 6.16Model tree for a dataset with nominal attributes.250Figure 6.17Clustering the weather data.256xviiiLIST OF FIGURESP088407-FM.qxd  4/30/05  10:55 AM  Page xviiiFigure 6.18Hierarchical clusterings ofthe iris data.259Figure 6.19A two-class mixture model.264Figure 6.20A simple Bayesian network for the weather data.273Figure 6.21Another Bayesian network for the weather data.274Figure 6.22The weather data:(a) reduced version and (b) correspondingAD tree.281Figure 7.1Attribute space for the weather dataset.293Figure 7.2Discretizing the temperatureattribute using the entropymethod.299Figure 7.3The result ofdiscretizing the temperatureattribute.300Figure 7.4Class distribution for a two-class,two-attribute problem.303Figure 7.5Principal components transform ofa dataset:(a) variance ofeach component and (b) variance plot.308Figure 7.6Number ofinternational phone calls from Belgium,1950–1973.314Figure 7.7Algorithm for bagging.319Figure 7.8Algorithm for boosting.322Figure 7.9Algorithm for additive logistic regression.327Figure 7.10Simple option tree for the weather data.329Figure 7.11Alternating decision tree for the weather data.330Figure 10.1The Explorer interface.370Figure 10.2Weather data:(a) spreadsheet,(b) CSV format,and (c) ARFF.371Figure 10.3The Weka Explorer:(a) choosing the Explorer interface and(b) reading in the weather data.372Figure 10.4Using J4.8:(a) ﬁnding it in the classiﬁers list and (b) theClassifytab.374Figure 10.5Output from the J4.8 decision tree learner.375Figure 10.6Visualizing the result ofJ4.8 on the iris dataset:(a) the treeand (b) the classiﬁer errors.379Figure 10.7Generic object editor:(a) the editor,(b) more information(click More),and (c) choosing a converter (click Choose).381Figure 10.8Choosing a ﬁlter:(a) the ﬁltersmenu,(b) an object editor,and(c) more information (click More).383Figure 10.9The weather data with two attributes removed.384Figure 10.10Processing the CPU performance data with M5¢.385Figure 10.11Output from the M5¢program for numeric prediction.386Figure 10.12Visualizing the errors:(a) from M5¢and (b) from linearregression.388LIST OF FIGURESxixP088407-FM.qxd  4/30/05  10:55 AM  Page xixFigure 10.13Working on the segmentation data with the User Classiﬁer:(a) the data visualizer and (b) the tree visualizer.390Figure 10.14Conﬁguring a metalearner for boosting decision stumps.391Figure 10.15Output from the Apriori program for association rules.392Figure 10.16Visualizing the Iris dataset.394Figure 10.17Using Weka’s metalearner for discretization:(a) conﬁguringFilteredClassiﬁer,and (b) the menu ofﬁlters.402Figure 10.18Visualizing a Bayesian network for the weather data (nominalversion):(a) default output,(b) a version with themaximum number ofparents set to 3in the searchalgorithm,and (c) probability distribution table for thewindynode in (b).406Figure 10.19Changing the parameters for J4.8.407Figure 10.20Using Weka’s neural-network graphical user interface.411Figure 10.21Attribute selection:specifying an evaluator and a searchmethod.420Figure 11.1The Knowledge Flow interface.428Figure 11.2Conﬁguring a data source:(a) the right-click menu and (b) the ﬁle browser obtained from the Conﬁguremenu item.429Figure 11.3Operations on the Knowledge Flow components.432Figure 11.4A Knowledge Flow that operates incrementally:(a) theconﬁguration and (b) the strip chart output.434Figure 12.1An experiment:(a) setting it up,(b) the results ﬁle,and (c) a spreadsheet with the results.438Figure 12.2Statistical test results for the experiment in Figure 12.1.440Figure 12.3Setting up an experiment in advanced mode.442Figure 12.4Rows and columns ofFigure 12.2:(a) row ﬁeld,(b) columnﬁeld,(c) result ofswapping the row and column selections,and (d) substituting Runfor Datasetas rows.444Figure 13.1Using Javadoc:(a) the front page and (b) the weka.corepackage.452Figure 13.2DecisionStump:A class ofthe weka.classiﬁers.treespackage.454Figure 14.1Source code for the message classiﬁer.463Figure 15.1Source code for the ID3 decision tree learner.473xxLIST OF FIGURESP088407-FM.qxd  5/3/05  2:24 PM  Page xxList of TablesTable 1.1The contact lens data.6Table 1.2The weather data.11Table 1.3Weather data with some numeric attributes.12Table 1.4The iris data.15Table 1.5The CPU performance data.16Table 1.6The labor negotiations data.18Table 1.7The soybean data.21Table 2.1Iris data as a clustering problem.44Table 2.2Weather data with a numeric class.44Table 2.3Family tree represented as a table.47Table 2.4The sister-ofrelation represented in a table.47Table 2.5Another relation represented as a table.49Table 3.1A new iris ﬂower.70Table 3.2Training data for the shapes problem.74Table 4.1Evaluating the attributes in the weather data.85Table 4.2The weather data with counts and probabilities.89Table 4.3A new day.89Table 4.4The numeric weather data with summary statistics.93Table 4.5Another new day.94Table 4.6The weather data with identiﬁcation codes.103Table 4.7Gain ratio calculations for the tree stumps ofFigure 4.2.104Table 4.8Part ofthe contact lens data for which astigmatism =yes.109Table 4.9Part ofthe contact lens data for which astigmatism =yesandtear production rate =normal.110Table 4.10Item sets for the weather data with coverage 2 or greater.114Table 4.11Association rules for the weather data.116Table 5.1Conﬁdence limits for the normal distribution.148xxiP088407-FM.qxd  4/30/05  10:55 AM  Page xxiTable 5.2Conﬁdence limits for Student’s distribution with 9 degrees offreedom.155Table 5.3Different outcomes ofa two-class prediction.162Table 5.4Different outcomes ofa three-class prediction:(a) actual and(b) expected.163Table 5.5Default cost matrixes:(a) a two-class case and (b) a three-classcase.164Table 5.6Data for a lift chart.167Table 5.7Different measures used to evaluate the false positive versus thefalse negative tradeoff.172Table 5.8Performance measures for numeric prediction.178Table 5.9Performance measures for four numeric prediction models.179Table 6.1Linear models in the model tree.250Table 7.1Transforming a multiclass problem into a two-class one:(a) standard method and (b) error-correcting code.335Table 10.1Unsupervised attribute ﬁlters.396Table 10.2Unsupervised instance ﬁlters.400Table 10.3Supervised attribute ﬁlters.402Table 10.4Supervised instance ﬁlters.402Table 10.5Classiﬁer algorithms in Weka.404Table 10.6Metalearning algorithms in Weka.415Table 10.7Clustering algorithms.419Table 10.8Association-rule learners.419Table 10.9Attribute evaluation methods for attribute selection.421Table 10.10Search methods for attribute selection.421Table 11.1Visualization and evaluation components.430Table 13.1Generic options for learning schemes in Weka.457Table 13.2Scheme-speciﬁc options for the J4.8 decision tree learner.458Table 15.1Simple learning schemes in Weka.472xxiiLIST OF TABLESP088407-FM.qxd  5/3/05  2:24 PM  Page xxiiPrefaceThe convergence ofcomputing and communication has produced a society thatfeeds on information.Yet most ofthe information is in its raw form:data.Ifdatais characterized as recorded facts,then informationis the set ofpatterns,or expectations,that underlie the data.There is a huge amount ofinformationlocked up in databases—information that is potentially important but has notyet been discovered or articulated.Our mission is to bring it forth.Data mining is the extraction ofimplicit,previously unknown,and poten-tially useful information from data.The idea is to build computer programs thatsift through databases automatically,seeking regularities or patterns.Strong pat-terns,iffound,will likely generalize to make accurate predictions on future data.Ofcourse,there will be problems.Many patterns will be banal and uninterest-ing.Others will be spurious,contingent on accidental coincidences in the par-ticular dataset used.In addition real data is imperfect:Some parts will begarbled,and some will be missing.Anything discovered will be inexact:Therewill be exceptions to every rule and cases not covered by any rule.Algorithmsneed to be robust enough to cope with imperfect data and to extract regulari-ties that are inexact but useful.Machine learning provides the technical basis ofdata mining.It is used toextract information from the raw data in databases—information that isexpressed in a comprehensible form and can be used for a variety ofpurposes.The process is one ofabstraction:taking the data,warts and all,and inferringwhatever structure underlies it.This book is about the tools and techniques ofmachine learning used in practical data mining for ﬁnding,and describing,structural patterns in data.As with any burgeoning new technology that enjoys intense commercialattention,the use ofdata mining is surrounded by a great deal ofhype in thetechnical—and sometimes the popular—press.Exaggerated reports appear ofthe secrets that can be uncovered by setting learning algorithms loose on oceansofdata.But there is no magic in machine learning,no hidden power,noxxiiiP088407-FM.qxd  4/30/05  10:55 AM  Page xxiiialchemy.Instead,there is an identiﬁable body ofsimple and practical techniquesthat can often extract useful information from raw data.This book describesthese techniques and shows how they work.We interpret machine learning as the acquisition ofstructural descriptionsfrom examples.The kind ofdescriptions found can be used for prediction,explanation,and understanding.Some data mining applications focus on pre-diction:forecasting what will happen in new situations from data that describewhat happened in the past,often by guessing the classiﬁcation ofnew examples.But we are equally—perhaps more—interested in applications in which theresult of“learning”is an actual description ofa structure that can be used toclassify examples.This structural description supports explanation,under-standing,and prediction.In our experience,insights gained by the applications’users are ofmost interest in the majority ofpractical data mining applications;indeed,this is one ofmachine learning’s major advantages over classical statis-tical modeling.The book explains a variety ofmachine learning methods.Some are peda-gogically motivated:simple schemes designed to explain clearly how the basicideas work.Others are practical:real systems used in applications today.Manyare contemporary and have been developed only in the last few years.A comprehensive software resource,written in the Java language,has beencreated to illustrate the ideas in the book.Called the Waikato Environment forKnowledge Analysis,or Weka1for short,it is available as source code on theWorld Wide Web at http://www.cs.waikato.ac.nz/ml/weka.It is a full,industrial-strength implementation ofessentially all the techniques covered in this book.It includes illustrative code and working implementations ofmachine learningmethods.It offers clean,spare implementations ofthe simplest techniques,designed to aid understanding ofthe mechanisms involved.It also provides aworkbench that includes full,working,state-of-the-art implementations ofmany popular learning schemes that can be used for practical data mining orfor research.Finally,it contains a framework,in the form ofa Java class library,that supports applications that use embedded machine learning and even theimplementation ofnew learning schemes.The objective ofthis book is to introduce the tools and techniques formachine learning that are used in data mining.After reading it,you will under-stand what these techniques are and appreciate their strengths and applicabil-ity.Ifyou wish to experiment with your own data,you will be able to do thiseasily with the Weka software.xxivPREFACE1Found only on the islands ofNew Zealand,the weka(pronounced to rhyme with Mecca)is a ﬂightless bird with an inquisitive nature.P088407-FM.qxd  4/30/05  10:55 AM  Page xxivThe book spans the gulfbetween the intensely practical approach taken bytrade books that provide case studies on data mining and the more theoretical,principle-driven exposition found in current textbooks on machine learning.(A briefdescription ofthese books appears in the Further readingsection at theend ofChapter 1.) This gulfis rather wide.To apply machine learning tech-niques productively,you need to understand something about how they work;this is not a technology that you can apply blindly and expect to get good results.Different problems yield to different techniques,but it is rarely obvious whichtechniques are suitable for a given situation:you need to know something aboutthe range ofpossible solutions.We cover an extremely wide range oftechniques.We can do this because,unlike many trade books,this volume does not promoteany particular commercial software or approach.We include a large number ofexamples,but they use illustrative datasets that are small enough to allow youto follow what is going on.Real datasets are far too large to show this (and inany case are usually company conﬁdential).Our datasets are chosen not to illustrate actual large-scale practical problems but to help you understand whatthe different techniques do,how they work,and what their range ofapplicationis.The book is aimed at the technically aware general reader interested in theprinciples and ideas underlying the current practice ofdata mining.It will also be ofinterest to information professionals who need to become acquaintedwith this new technology and to all those who wish to gain a detailed technicalunderstanding ofwhat machine learning involves.It is written for an eclecticaudience ofinformation systems practitioners,programmers,consultants,developers,information technology managers,speciﬁcation writers,patentexaminers,and curious laypeople—as well as students and professors—whoneed an easy-to-read book with lots ofillustrations that describes what themajor machine learning techniques are,what they do,how they are used,andhow they work.It is practically oriented,with a strong “how to”ﬂavor,andincludes algorithms,code,and implementations.All those involved in practicaldata mining will beneﬁt directly from the techniques described.The book isaimed at people who want to cut through to the reality that underlies the hypeabout machine learning and who seek a practical,nonacademic,unpretentiousapproach.We have avoided requiring any speciﬁc theoretical or mathematicalknowledge except in some sections marked by a light gray bar in the margin.These contain optional material,often for the more technical or theoreticallyinclined reader,and may be skipped without loss ofcontinuity.The book is organized in layers that make the ideas accessible to readers whoare interested in grasping the basics and to those who would like more depth oftreatment,along with full details on the techniques covered.We believe that con-sumers ofmachine learning need to have some idea ofhow the algorithms theyuse work.It is often observed that data models are only as good as the personPREFACExxvP088407-FM.qxd  5/3/05  2:24 PM  Page xxvwho interprets them,and that person needs to know something about how themodels are produced to appreciate the strengths,and limitations,ofthe tech-nology.However,it is not necessary for all data model users to have a deepunderstanding ofthe ﬁner details ofthe algorithms.We address this situation by describing machine learning methods at succes-sive levels ofdetail.You will learn the basic ideas,the topmost level,by readingthe ﬁrst three chapters.Chapter 1 describes,through examples,what machinelearning is and where it can be used;it also provides actual practical applica-tions.Chapters 2 and 3 cover the kinds ofinput and output—or knowledge representation—involved.Different kinds ofoutput dictate different styles ofalgorithm,and at the next level Chapter 4 describes the basic methods ofmachine learning,simpliﬁed to make them easy to comprehend.Here the prin-ciples involved are conveyed in a variety ofalgorithms without getting into intricate details or tricky implementation issues.To make progress in the appli-cation ofmachine learning techniques to particular data mining problems,it isessential to be able to measure how well you are doing.Chapter 5,which can beread out ofsequence,equips you to evaluate the results obtained from machinelearning,addressing the sometimes complex issues involved in performanceevaluation.At the lowest and most detailed level,Chapter 6 exposes in naked detail thenitty-gritty issues ofimplementing a spectrum ofmachine learning algorithms,including the complexities necessary for them to work well in practice.Althoughmany readers may want to ignore this detailed information,it is at this level thatthe full,working,tested implementations ofmachine learning schemes in Wekaare written.Chapter 7 describes practical topics involved with engineering theinput to machine learning—for example,selecting and discretizing attributes—and covers several more advanced techniques for reﬁning and combining theoutput from different learning techniques.The ﬁnal chapter ofPart I looks tothe future.The book describes most methods used in practical machine learning.However,it does not cover reinforcement learning,because it is rarely appliedin practical data mining;genetic algorithm approaches,because these are justan optimization technique;or relational learning and inductive logic program-ming,because they are rarely used in mainstream data mining applications.The data mining system that illustrates the ideas in the book is described inPart II to clearly separate conceptual material from the practical aspects ofhowto use it.You can skip to Part II directly from Chapter 4 ifyou are in a hurry toanalyze your data and don’t want to be bothered with the technical details.Java has been chosen for the implementations ofmachine learning tech-niques that accompany this book because,as an object-oriented programminglanguage,it allows a uniform interface to learning schemes and methods for pre-and postprocessing.We have chosen Java instead ofC++,Smalltalk,or otherxxviPREFACEP088407-FM.qxd  4/30/05  10:55 AM  Page xxviobject-oriented languages because programs written in Java can be run onalmost any computer without having to be recompiled,having to undergo com-plicated installation procedures,or—worst ofall—having to change the code.A Java program is compiled into byte-code that can be executed on any com-puter equipped with an appropriate interpreter.This interpreter is called theJava virtual machine.Java virtual machines—and,for that matter,Java compil-ers—are freely available for all important platforms.Like all widely used programming languages,Java has received its share ofcriticism.Although this is not the place to elaborate on such issues,in severalcases the critics are clearly right.However,ofall currently available program-ming languages that are widely supported,standardized,and extensively docu-mented,Java seems to be the best choice for the purpose ofthis book.Its maindisadvantage is speed ofexecution—or lack ofit.Executing a Java program isseveral times slower than running a corresponding program written in C lan-guage because the virtual machine has to translate the byte-code into machinecode before it can be executed.In our experience the difference is a factor ofthree to ﬁve ifthe virtual machine uses a just-in-time compiler.Instead oftrans-lating each byte-code individually,a just-in-time compilertranslates wholechunks ofbyte-code into machine code,thereby achieving signiﬁcant speedup.However,ifthis is still to slow for your application,there are compilers thattranslate Java programs directly into machine code,bypassing the byte-codestep.This code cannot be executed on other platforms,thereby sacriﬁcing oneofJava’s most important advantages.Updated and revised contentWe ﬁnished writing the ﬁrst edition ofthis book in 1999 and now,in April 2005,are just polishing this second edition.The areas ofdata mining and machinelearning have matured in the intervening years.Although the core ofmaterialin this edition remains the same,we have made the most ofour opportunity toupdate it to reﬂect the changes that have taken place over 5 years.There havebeen errors to ﬁx,errors that we had accumulated in our publicly available errataﬁle.Surprisingly few were found,and we hope there are even fewer in thissecond edition.(The errata for the second edition may be found through thebook’s home page at http://www.cs.waikato.ac.nz/ml/weka/book.html.) We havethoroughly edited the material and brought it up to date,and we practicallydoubled the number ofreferences.The most enjoyable part has been addingnew material.Here are the highlights.Bowing to popular demand,we have added comprehensive information onneural networks:the perceptron and closely related Winnow algorithm inSection 4.6 and the multilayer perceptron and backpropagation algorithm PREFACExxviiP088407-FM.qxd  4/30/05  10:55 AM  Page xxviiin Section 6.3.We have included more recent material on implementing nonlinear decision boundaries using both the kernel perceptron and radial basisfunction networks.There is a new section on Bayesian networks,again inresponse to readers’requests,with a description ofhow to learn classiﬁers basedon these networks and how to implement them efﬁciently using all-dimensionstrees.The Weka machine learning workbench that accompanies the book,a widelyused and popular feature ofthe ﬁrst edition,has acquired a radical new look inthe form ofan interactive interface—or rather,three separate interactive inter-faces—that make it far easier to use.The primary one is the Explorer,whichgives access to all ofWeka’s facilities using menu selection and form ﬁlling.Theothers are the Knowledge Flow interface,which allows you to design conﬁgu-rations for streamed data processing,and the Experimenter,with which you setup automated experiments that run selected machine learning algorithms withdifferent parameter settings on a corpus ofdatasets,collect performance statis-tics,and perform signiﬁcance tests on the results.These interfaces lower the barfor becoming a practicing data miner,and we include a full description ofhowto use them.However,the book continues to stand alone,independent ofWeka,and to underline this we have moved all material on the workbench into a sep-arate Part II at the end ofthe book.In addition to becoming far easier to use,Weka has grown over the last 5years and matured enormously in its data mining capabilities.It now includesan unparalleled range ofmachine learning algorithms and related techniques.The growth has been partly stimulated by recent developments in the ﬁeld andpartly led by Weka users and driven by demand.This puts us in a position inwhich we know a great deal about what actual users ofdata mining want,andwe have capitalized on this experience when deciding what to include in thisnew edition.The earlier chapters,containing more general and foundational material,have suffered relatively little change.We have added more examples ofﬁeldedapplications to Chapter 1,a new subsection on sparse data and a little on stringattributes and date attributes to Chapter 2,and a description ofinteractive deci-sion tree construction,a useful and revealing technique to help you grapple withyour data using manually built decision trees,to Chapter 3.In addition to introducing linear decision boundaries for classiﬁcation,theinfrastructure for neural networks,Chapter 4 includes new material on multi-nomial Bayes models for document classiﬁcation and on logistic regression.Thelast 5 years have seen great interest in data mining for text,and this is reﬂectedin our introduction to string attributes in Chapter 2,multinomial Bayes for doc-ument classiﬁcation in Chapter 4,and text transformations in Chapter 7.Chapter 4 includes a great deal ofnew material on efﬁcient data structures forsearching the instance space:kD-trees and the recently invented ball trees.ThesexxviiiPREFACEP088407-FM.qxd  4/30/05  10:55 AM  Page xxviiiare used to ﬁnd nearest neighbors efﬁciently and to accelerate distance-basedclustering.Chapter 5 describes the principles ofstatistical evaluation ofmachine learn-ing,which have not changed.The main addition,apart from a note on the Kappastatistic for measuring the success ofa predictor,is a more detailed treatmentofcost-sensitive learning.We describe how to use a classiﬁer,built withouttaking costs into consideration,to make predictions that are sensitive to cost;alternatively,we explain how to take costs into account during the trainingprocess to build a cost-sensitive model.We also cover the popular new tech-nique ofcost curves.There are several additions to Chapter 6,apart from the previously men-tioned material on neural networks and Bayesian network classiﬁers.Moredetails—gory details—are given ofthe heuristics used in the successful RIPPERrule learner.We describe how to use model trees to generate rules for numericprediction.We show how to apply locally weighted regression to classiﬁcationproblems.Finally,we describe the X-means clustering algorithm,which is a bigimprovement on traditional k-means.Chapter 7 on engineering the input and output has changed most,becausethis is where recent developments in practical machine learning have been con-centrated.We describe new attribute selection schemes such as race search andthe use ofsupport vector machines and new methods for combining modelssuch as additive regression,additive logistic regression,logistic model trees,andoption trees.We give a full account ofLogitBoost (which was mentioned in theﬁrst edition but not described).There is a new section on useful transforma-tions,including principal components analysis and transformations for textmining and time series.We also cover recent developments in using unlabeleddata to improve classiﬁcation,including the co-training and co-EM methods.The ﬁnal chapter ofPart I on new directions and different perspectives hasbeen reworked to keep up with the times and now includes contemporary chal-lenges such as adversarial learning and ubiquitous data mining.AcknowledgmentsWriting the acknowledgments is always the nicest part! A lot ofpeople havehelped us,and we relish this opportunity to thank them.This book has arisenout ofthe machine learning research project in the Computer Science Depart-ment at the University ofWaikato,New Zealand.We have received generousencouragement and assistance from the academic staffmembers on that project:John Cleary,Sally Jo Cunningham,Matt Humphrey,Lyn Hunt,Bob McQueen,Lloyd Smith,and Tony Smith.Special thanks go to Mark Hall,BernhardPfahringer,and above all GeoffHolmes,the project leader and source ofinspi-PREFACExxixP088407-FM.qxd  4/30/05  10:55 AM  Page xxixration.All who have worked on the machine learning project here have con-tributed to our thinking:we would particularly like to mention Steve Garner,Stuart Inglis,and Craig Nevill-Manning for helping us to get the project offtheground in the beginning when success was less certain and things were moredifﬁcult.The Weka system that illustrates the ideas in this book forms a crucial com-ponent ofit.It was conceived by the authors and designed and implemented byEibe Frank,along with Len Trigg and Mark Hall.Many people in the machinelearning laboratory at Waikato made signiﬁcant contributions.Since the ﬁrstedition ofthe book the Weka team has expanded considerably:so many peoplehave contributed that it is impossible to acknowledge everyone properly.We aregrateful to Remco Bouckaert for his implementation ofBayesian networks,DaleFletcher for many database-related aspects,AshrafKibriya and Richard Kirkbyfor contributions far too numerous to list,Niels Landwehr for logistic modeltrees,Abdelaziz Mahoui for the implementation ofK*,Stefan Mutter for asso-ciation rule mining,Gabi Schmidberger and Malcolm Ware for numerous mis-cellaneous contributions,Tony Voyle for least-median-of-squares regression,Yong Wang for Pace regression and the implementation ofM5¢,and Xin Xu forJRip,logistic regression,and many other contributions.Our sincere thanks goto all these people for their dedicated work and to the many contributors toWeka from outside our group at Waikato.Tucked away as we are in a remote (but very pretty) corner ofthe SouthernHemisphere,we greatly appreciate the visitors to our department who play a crucial role in acting as sounding boards and helping us to develop our thinking.We would like to mention in particular Rob Holte,Carl Gutwin,andRussell Beale,each ofwhom visited us for several months;David Aha,whoalthough he only came for a few days did so at an early and fragile stage oftheproject and performed a great service by his enthusiasm and encouragement;and Kai Ming Ting,who worked with us for 2 years on many ofthe topicsdescribed in Chapter 7 and helped to bring us into the mainstream ofmachinelearning.Students at Waikato have played a signiﬁcant role in the development oftheproject.Jamie Littin worked on ripple-down rules and relational learning.BrentMartin explored instance-based learning and nested instance-based representa-tions.Murray Fife slaved over relational learning,and Nadeeka Madapathageinvestigated the use offunctional languages for expressing machine learningalgorithms.Other graduate students have inﬂuenced us in numerous ways,par-ticularly Gordon Paynter,YingYing Wen,and Zane Bray,who have worked withus on text mining.Colleagues Steve Jones and Malika Mahoui have also madefar-reaching contributions to these and other machine learning projects.Morerecently we have learned much from our many visiting students from Freiburg,including Peter Reutemann and Nils Weidmann.xxxPREFACEP088407-FM.qxd  4/30/05  10:55 AM  Page xxxIan Witten would like to acknowledge the formative role ofhis former stu-dents at Calgary,particularly Brent Krawchuk,Dave Maulsby,Thong Phan,andTanja Mitrovic,all ofwhom helped him develop his early ideas in machinelearning,as did faculty members Bruce MacDonald,Brian Gaines,and DavidHill at Calgary and John Andreae at the University ofCanterbury.Eibe Frank is indebted to his former supervisor at the University ofKarlsruhe,Klaus-Peter Huber (now with SAS Institute),who infected him withthe fascination ofmachines that learn.On his travels Eibe has beneﬁted frominteractions with Peter Turney,Joel Martin,and Berry de Bruijn in Canada andwith Luc de Raedt,Christoph Helma,Kristian Kersting,Stefan Kramer,UlrichRückert,and Ashwin Srinivasan in Germany.Diane Cerra and Asma Stephan ofMorgan Kaufmann have worked hard toshape this book,and Lisa Royse,our production editor,has made the processgo smoothly.Bronwyn Webster has provided excellent support at the Waikatoend.We gratefully acknowledge the unsung efforts ofthe anonymous reviewers,one ofwhom in particular made a great number ofpertinent and constructivecomments that helped us to improve this book signiﬁcantly.In addition,wewould like to thank the librarians ofthe Repository ofMachine Learning Data-bases at the University ofCalifornia,Irvine,whose carefully collected datasetshave been invaluable in our research.Our research has been funded by the New Zealand Foundation for Research,Science and Technology and the Royal Society ofNew Zealand Marsden Fund.The Department ofComputer Science at the University ofWaikato has gener-ously supported us in all sorts ofways,and we owe a particular debt ofgratitude to Mark Apperley for his enlightened leadership and warm encour-agement.Part ofthe ﬁrst edition was written while both authors were visitingthe University ofCalgary,Canada,and the support ofthe Computer Sciencedepartment there is gratefully acknowledged—as well as the positive and helpfulattitude ofthe long-suffering students in the machine learning course on whomwe experimented.In producing the second edition Ian was generously supported by Canada’sInformatics Circle ofResearch Excellence and by the University ofLethbridgein southern Alberta,which gave him what all authors yearn for—a quiet spacein pleasant and convivial surroundings in which to work.Last,and most ofall,we are grateful to our families and partners.Pam,Anna,and Nikki were all too well aware ofthe implications ofhaving an author in thehouse (“not again!”) but let Ian go ahead and write the book anyway.Julie wasalways supportive,even when Eibe had to burn the midnight oil in the machinelearning lab,and Immo and Ollig provided exciting diversions.Between us wehail from Canada,England,Germany,Ireland,and Samoa:New Zealand hasbrought us together and provided an ideal,even idyllic,place to do this work.PREFACExxxiP088407-FM.qxd  4/30/05  10:55 AM  Page xxxiP088407-FM.qxd  4/30/05  10:55 AM  Page xxxiipartIMachine Learning Tools and TechniquesP088407-Ch001.qxd  4/30/05  11:11 AM  Page 1P088407-Ch001.qxd  4/30/05  11:11 AM  Page 2Human in vitrofertilization involves collecting several eggs from a woman’sovaries,which,after fertilization with partner or donor sperm,produce severalembryos.Some ofthese are selected and transferred to the woman’s uterus.Theproblem is to select the “best”embryos to use—the ones that are most likely tosurvive.Selection is based on around 60 recorded features ofthe embryos—characterizing their morphology,oocyte,follicle,and the sperm sample.Thenumber offeatures is sufﬁciently large that it is difﬁcult for an embryologist toassess them all simultaneously and correlate historical data with the crucialoutcome ofwhether that embryo did or did not result in a live child.In aresearch project in England,machine learning is being investigated as a tech-nique for making the selection,using as training data historical records ofembryos and their outcome.Every year,dairy farmers in New Zealand have to make a tough business deci-sion:which cows to retain in their herd and which to sell offto an abattoir.Typi-cally,one-ﬁfth ofthe cows in a dairy herd are culled each year near the end ofthe milking season as feed reserves dwindle.Each cow’s breeding and milk pro-chapter1What’s It All About?3P088407-Ch001.qxd  4/30/05  11:11 AM  Page 3duction history inﬂuences this decision.Other factors include age (a cow isnearing the end ofits productive life at 8 years),health problems,history ofdif-ﬁcult calving,undesirable temperament traits (kicking or jumping fences),andnot being in calffor the following season.About 700 attributes for each ofseveral million cows have been recorded over the years.Machine learning isbeing investigated as a way ofascertaining what factors are taken into accountby successful farmers—not to automate the decision but to propagate their skillsand experience to others.Life and death.From Europe to the antipodes.Family and business.Machinelearning is a burgeoning new technology for mining knowledge from data,atechnology that a lot ofpeople are starting to take seriously.1.1Data mining and machine learningWe are overwhelmed with data.The amount ofdata in the world,in our lives,seems to go on and on increasing—and there’s no end in sight.Omnipresentpersonal computers make it too easy to save things that previously we wouldhave trashed.Inexpensive multigigabyte disks make it too easy to postpone deci-sions about what to do with all this stuff—we simply buy another disk and keepit all.Ubiquitous electronics record our decisions,our choices in the super-market,our ﬁnancial habits,our comings and goings.We swipe our way throughthe world,every swipe a record in a database.The World Wide Web overwhelmsus with information;meanwhile,every choice we make is recorded.And all theseare just personal choices:they have countless counterparts in the world ofcom-merce and industry.We would all testify to the growing gap between the gener-ationofdata and our understandingofit.As the volume ofdata increases,inexorably,the proportion ofit that people understand decreases,alarmingly.Lying hidden in all this data is information,potentially useful information,thatis rarely made explicit or taken advantage of.This book is about looking for patterns in data.There is nothing new aboutthis.People have been seeking patterns in data since human life began.Huntersseek patterns in animal migration behavior,farmers seek patterns in cropgrowth,politicians seek patterns in voter opinion,and lovers seek patterns intheir partners’responses.A scientist’s job (like a baby’s) is to make sense ofdata,to discover the patterns that govern how the physical world works and encap-sulate them in theories that can be used for predicting what will happen in newsituations.The entrepreneur’s job is to identify opportunities,that is,patternsin behavior that can be turned into a proﬁtable business,and exploit them.In data mining,the data is stored electronically and the search is automated—or at least augmented—by computer.Even this is not particularly new.Econo-mists,statisticians,forecasters,and communication engineers have long worked4CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 4with the idea that patterns in data can be sought automatically,identiﬁed,validated,and used for prediction.What is new is the staggering increase inopportunities for ﬁnding patterns in data.The unbridled growth ofdatabasesin recent years,databases on such everyday activities as customer choices,bringsdata mining to the forefront ofnew business technologies.It has been estimatedthat the amount ofdata stored in the world’s databases doubles every 20months,and although it would surely be difﬁcult to justify this ﬁgure in anyquantitative sense,we can all relate to the pace ofgrowth qualitatively.As theﬂood ofdata swells and machines that can undertake the searching becomecommonplace,the opportunities for data mining increase.As the world growsin complexity,overwhelming us with the data it generates,data mining becomesour only hope for elucidating the patterns that underlie it.Intelligently analyzeddata is a valuable resource.It can lead to new insights and,in commercial set-tings,to competitive advantages.Data mining is about solving problems by analyzing data already present indatabases.Suppose,to take a well-worn example,the problem is ﬁckle customerloyalty in a highly competitive marketplace.A database ofcustomer choices,along with customer proﬁles,holds the key to this problem.Patterns ofbehavior offormer customers can be analyzed to identify distinguishing charac-teristics ofthose likely to switch products and those likely to remain loyal.Oncesuch characteristics are found,they can be put to work to identify present cus-tomers who are likely to jump ship.This group can be targeted for special treat-ment,treatment too costly to apply to the customer base as a whole.Morepositively,the same techniques can be used to identify customers who might beattracted to another service the enterprise provides,one they are not presentlyenjoying,to target them for special offers that promote this service.In today’shighly competitive,customer-centered,service-oriented economy,data is theraw material that fuels business growth—ifonly it can be mined.Data mining is deﬁned as the process ofdiscovering patterns in data.Theprocess must be automatic or (more usually) semiautomatic.The patterns discovered must be meaningful in that they lead to some advantage,usually an economic advantage.The data is invariably present in substantial quantities.How are the patterns expressed? Useful patterns allow us to make nontrivialpredictions on new data.There are two extremes for the expression ofa pattern:as a black box whose innards are effectively incomprehensible and as a trans-parent box whose construction reveals the structure ofthe pattern.Both,we areassuming,make good predictions.The difference is whether or not the patternsthat are mined are represented in terms ofa structure that can be examined,reasoned about,and used to inform future decisions.Such patterns we call struc-turalbecause they capture the decision structure in an explicit way.In otherwords,they help to explain something about the data.1.1DATA MINING AND MACHINE LEARNING5P088407-Ch001.qxd  4/30/05  11:11 AM  Page 5Now,ﬁnally,we can say what this book is about.It is about techniques forﬁnding and describing structural patterns in data.Most ofthe techniques thatwe cover have developed within a ﬁeld known as machine learning.But ﬁrst letus look at what structural patterns are.Describing structural patternsWhat is meant by structural patterns?How do you describe them? And whatform does the input take? We will answer these questions by way ofillustrationrather than by attempting formal,and ultimately sterile,deﬁnitions.There willbe plenty ofexamples later in this chapter,but let’s examine one right now toget a feeling for what we’re talking about.Look at the contact lens data in Table 1.1.This gives the conditions underwhich an optician might want to prescribe soft contact lenses,hard contactlenses,or no contact lenses at all;we will say more about what the individual6CHAPTER 1|WHAT’S IT ALL ABOUT?Table 1.1The contact lens data.SpectacleTear productionRecommendedAgeprescriptionAstigmatismratelensesyoungmyopenoreducednoneyoungmyopenonormalsoftyoungmyopeyesreducednoneyoungmyopeyesnormalhardyounghypermetropenoreducednoneyounghypermetropenonormalsoftyounghypermetropeyesreducednoneyounghypermetropeyesnormalhardpre-presbyopicmyopenoreducednonepre-presbyopicmyopenonormalsoftpre-presbyopicmyopeyesreducednonepre-presbyopicmyopeyesnormalhardpre-presbyopichypermetropenoreducednonepre-presbyopichypermetropenonormalsoftpre-presbyopichypermetropeyesreducednonepre-presbyopichypermetropeyesnormalnonepresbyopicmyopenoreducednonepresbyopicmyopenonormalnonepresbyopicmyopeyesreducednonepresbyopicmyopeyesnormalhardpresbyopichypermetropenoreducednonepresbyopichypermetropenonormalsoftpresbyopichypermetropeyesreducednonepresbyopichypermetropeyesnormalnoneP088407-Ch001.qxd  4/30/05  11:11 AM  Page 6features mean later.Each line ofthe table is one ofthe examples.Part ofa struc-tural description ofthis information might be as follows:If tear production rate =reduced then recommendation =noneOtherwise, if age =young and astigmatic =no then recommendation =softStructural descriptions need not necessarily be couched as rules such as these.Decision trees,which specify the sequences ofdecisions that need to be madeand the resulting recommendation,are another popular means ofexpression.This example is a very simplistic one.First,all combinations ofpossiblevalues are represented in the table.There are 24 rows,representing three possi-ble values ofage and two values each for spectacle prescription,astigmatism,and tear production rate (3 ¥2 ¥2 ¥2 =24).The rules do not really general-ize from the data;they merely summarize it.In most learning situations,the setofexamples given as input is far from complete,and part ofthe job is to gen-eralize to other,new examples.You can imagine omitting some ofthe rows inthe table for which tear production rate is reducedand still coming up with theruleIf tear production rate =reduced then recommendation =nonewhich would generalize to the missing rows and ﬁll them in correctly.Second,values are speciﬁed for all the features in all the examples.Real-life datasetsinvariably contain examples in which the values ofsome features,for somereason or other,are unknown—for example,measurements were not taken orwere lost.Third,the preceding rules classify the examples correctly,whereasoften,because oferrors or noisein the data,misclassiﬁcations occur even on thedata that is used to train the classiﬁer.Machine learningNow that we have some idea about the inputs and outputs,let’s turn to machinelearning.What is learning,anyway? What is machine learning? These are philo-sophic questions,and we will not be much concerned with philosophy in thisbook;our emphasis is ﬁrmly on the practical.However,it is worth spending afew moments at the outset on fundamental issues,just to see how tricky theyare,before rolling up our sleeves and looking at machine learning in practice.Our dictionary deﬁnes “to learn”as follows:To get knowledge ofby study,experience,or being taught;To become aware by information or from observation;To commit to memory;To be informed of,ascertain;To receive instruction.1.1DATA MINING AND MACHINE LEARNING7P088407-Ch001.qxd  4/30/05  11:11 AM  Page 7These meanings have some shortcomings when it comes to talking about com-puters.For the ﬁrst two,it is virtually impossible to test whether learning hasbeen achieved or not.How do you know whether a machine has got knowledgeofsomething? You probably can’t just ask it questions;even ifyou could,youwouldn’t be testing its ability to learn but would be testing its ability to answerquestions.How do you know whether it has become aware ofsomething? Thewhole question ofwhether computers can be aware,or conscious,is a burningphilosophic issue.As for the last three meanings,although we can see what theydenote in human terms,merely “committing to memory”and “receivinginstruction”seem to fall far short ofwhat we might mean by machine learning.They are too passive,and we know that computers ﬁnd these tasks trivial.Instead,we are interested in improvements in performance,or at least in thepotential for performance,in new situations.You can “commit something tomemory”or “be informed ofsomething”by rote learning without being able toapply the new knowledge to new situations.You can receive instruction withoutbeneﬁting from it at all.Earlier we deﬁned data mining operationally as the process ofdiscoveringpatterns,automatically or semiautomatically,in large quantities ofdata—andthe patterns must be useful.An operational deﬁnition can be formulated in thesame way for learning:Things learn when they change their behavior in a way that makes themperform better in the future.This ties learning to performance rather than knowledge.You can test learningby observing the behavior and comparing it with past behavior.This is a muchmore objective kind ofdeﬁnition and appears to be far more satisfactory.But there’s still a problem.Learning is a rather slippery concept.Lots ofthingschange their behavior in ways that make them perform better in the future,yetwe wouldn’t want to say that they have actually learned.A good example is acomfortable slipper.Has it learnedthe shape ofyour foot? It has certainlychanged its behavior to make it perform better as a slipper! Yet we would hardlywant to call this learning.In everyday language,we often use the word“train-ing”to denote a mindless kind oflearning.We train animals and even plants,although it would be stretching the word a bit to talk oftraining objects suchas slippers that are not in any sense alive.But learning is different.Learningimplies thinking.Learning implies purpose.Something that learns has to do sointentionally.That is why we wouldn’t say that a vine has learned to grow rounda trellis in a vineyard—we’d say it has been trained.Learning without purposeis merely training.Or,more to the point,in learning the purpose is the learner’s,whereas in training it is the teacher’s.Thus on closer examination the second deﬁnition oflearning,in operational,performance-oriented terms,has its own problems when it comes to talking about8CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 8computers.To decide whether something has actually learned,you need to seewhether it intended to or whether there was any purpose involved.That makesthe concept moot when applied to machines because whether artifacts can behavepurposefully is unclear.Philosophic discussions ofwhat is really meant by “learn-ing,”like discussions ofwhat is really meant by “intention”or “purpose,”arefraught with difﬁculty.Even courts oflaw ﬁnd intention hard to grapple with.Data miningFortunately,the kind oflearning techniques explained in this book do notpresent these conceptual problems—they are called machine learning withoutreally presupposing any particular philosophic stance about what learning actu-ally is.Data mining is a practical topic and involves learning in a practical,nota theoretical,sense.We are interested in techniques for ﬁnding and describingstructural patterns in data as a tool for helping to explain that data and makepredictions from it.The data will take the form ofa set ofexamples—examplesofcustomers who have switched loyalties,for instance,or situations in whichcertain kinds ofcontact lenses can be prescribed.The output takes the form ofpredictions about new examples—a prediction ofwhether a particular customerwill switch or a prediction ofwhat kind oflens will be prescribed under givencircumstances.But because this book is about ﬁnding and describingpatternsin data,the output may also include an actual description ofa structure thatcan be used to classify unknown examples to explain the decision.As well asperformance,it is helpful to supply an explicit representation ofthe knowledgethat is acquired.In essence,this reﬂects both deﬁnitions oflearning consideredpreviously:the acquisition ofknowledge and the ability to use it.Many learning techniques look for structural descriptions ofwhat is learned,descriptions that can become fairly complex and are typically expressed as setsofrules such as the ones described previously or the decision trees describedlater in this chapter.Because they can be understood by people,these descrip-tions serve to explain what has been learned and explain the basis for new pre-dictions.Experience shows that in many applications ofmachine learning todata mining,the explicit knowledge structures that are acquired,the structuraldescriptions,are at least as important,and often very much more important,than the ability to perform well on new examples.People frequently use datamining to gain knowledge,not just predictions.Gaining knowledge from datacertainly sounds like a good idea ifyou can do it.To ﬁnd out how,read on!1.2Simple examples: The weather problem and othersWe use a lot ofexamples in this book,which seems particularly appropriate con-sidering that the book is all about learning from examples! There are several1.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS9P088407-Ch001.qxd  4/30/05  11:11 AM  Page 9standard datasets that we will come back to repeatedly.Different datasets tendto expose new issues and challenges,and it is interesting and instructive to havein mind a variety ofproblems when considering learning methods.In fact,theneed to work with different datasets is so important that a corpus containingaround 100 example problems has been gathered together so that different algo-rithms can be tested and compared on the same set ofproblems.The illustrations in this section are all unrealistically simple.Serious appli-cation ofdata mining involves thousands,hundreds ofthousands,or even mil-lions ofindividual cases.But when explaining what algorithms do and how theywork,we need simple examples that capture the essence ofthe problem but aresmall enough to be comprehensible in every detail.We will be working with theillustrations in this section throughout the book,and they are intended to be“academic”in the sense that they will help us to understand what is going on.Some actual ﬁelded applications oflearning techniques are discussed in Section1.3,and many more are covered in the books mentioned in the Further readingsection at the end ofthe chapter.Another problem with actual real-life datasets is that they are often propri-etary.No one is going to share their customer and product choice database withyou so that you can understand the details oftheir data mining application andhow it works.Corporate data is a valuable asset,one whose value has increasedenormously with the development ofdata mining techniques such as thosedescribed in this book.Yet we are concerned here with understanding how themethods used for data mining work and understanding the details ofthesemethods so that we can trace their operation on actual data.That is why ourillustrations are simple ones.But they are not simplistic:they exhibit the fea-tures ofreal datasets.The weather problemThe weather problem is a tiny dataset that we will use repeatedly to illustratemachine learning methods.Entirely ﬁctitious,it supposedly concerns the con-ditions that are suitable for playing some unspeciﬁed game.In general,instancesin a dataset are characterized by the values offeatures,or attributes,that measuredifferent aspects ofthe instance.In this case there are four attributes:outlook,temperature,humidity,and windy.The outcome is whether to play or not.In its simplest form,shown in Table 1.2,all four attributes have values thatare symbolic categories rather than numbers.Outlook can be sunny,overcast,orrainy;temperature can be hot,mild,or cool;humidity can be highor normal;and windy can be trueor false.This creates 36 possible combinations (3 ¥3 ¥2 ¥2 =36),ofwhich 14 are present in the set ofinput examples.A set ofrules learned from this information—not necessarily a very goodone—might look as follows:10CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 10If outlook =sunny and humidity =high then play =noIf outlook =rainy and windy =truethen play =noIf outlook =overcastthen play =yesIf humidity =normalthen play =yesIf none of the abovethen play =yesThese rules are meant to be interpreted in order:the ﬁrst one,then ifit doesn’tapply the second,and so on.A set ofrules that are intended to be interpretedin sequence is called a decision list.Interpreted as a decision list,the rules correctly classify all ofthe examples in the table,whereas taken individually,outofcontext,some ofthe rules are incorrect.For example,the rule if humidity =normal then play = yesgets one ofthe examples wrong (check which one).The meaning ofa set ofrules depends on how it is interpreted—not surprisingly!In the slightly more complex form shown in Table 1.3,two ofthe attributes—temperature and humidity—have numeric values.This means that any learn-ing method must create inequalities involving these attributes rather thansimple equality tests,as in the former case.This is called a numeric-attributeproblem—in this case,a mixed-attribute problembecause not all attributes arenumeric.Now the ﬁrst rule given earlier might take the following form:If outlook =sunny and humidity >83 then play =noA slightly more complex process is required to come up with rules that involvenumeric tests.1.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS11Table 1.2The weather data.OutlookTemperatureHumidityWindyPlaysunnyhothighfalsenosunnyhothightruenoovercasthothighfalseyesrainymildhighfalseyesrainycoolnormalfalseyesrainycoolnormaltruenoovercastcoolnormaltrueyessunnymildhighfalsenosunnycoolnormalfalseyesrainymildnormalfalseyessunnymildnormaltrueyesovercastmildhightrueyesovercasthotnormalfalseyesrainymildhightruenoP088407-Ch001.qxd  4/30/05  11:11 AM  Page 11The rules we have seen so far are classiﬁcation rules:they predict the classiﬁ-cation ofthe example in terms ofwhether to play or not.It is equally possibleto disregard the classiﬁcation and just look for any rules that strongly associatedifferent attribute values.These are called association rules.Many associationrules can be derived from the weather data in Table 1.2.Some good ones are asfollows:If temperature =coolthen humidity =normalIf humidity =normal and windy =false then play =yesIf outlook =sunny and play =nothen humidity =highIf windy =false and play =nothen outlook =sunnyand humidity =high.All these rules are 100% correct on the given data;they make no false predic-tions.The ﬁrst two apply to four examples in the dataset,the third to threeexamples,and the fourth to two examples.There are many other rules:in fact,nearly 60 association rules can be found that apply to two or more examples ofthe weather data and are completely correct on this data.Ifyou look for rulesthat are less than 100% correct,then you will ﬁnd many more.There are somany because unlike classiﬁcation rules,association rules can “predict”any ofthe attributes,not just a speciﬁed class,and can even predict more than onething.For example,the fourth rule predicts both that outlookwill be sunnyandthat humiditywill be high.12CHAPTER 1|WHAT’S IT ALL ABOUT?Table 1.3Weather data with some numeric attributes.OutlookTemperatureHumidityWindyPlaysunny8585falsenosunny8090truenoovercast8386falseyesrainy7096falseyesrainy6880falseyesrainy6570truenoovercast6465trueyessunny7295falsenosunny6970falseyesrainy7580falseyessunny7570trueyesovercast7290trueyesovercast8175falseyesrainy7191truenoP088407-Ch001.qxd  4/30/05  11:11 AM  Page 12Contact lenses: An idealized problemThe contact lens data introduced earlier tells you the kind ofcontact lens to pre-scribe,given certain information about a patient.Note that this example isintended for illustration only:it grossly oversimpliﬁes the problem and shouldcertainly not be used for diagnostic purposes!The ﬁrst column ofTable 1.1 gives the age ofthe patient.In case you’re won-dering,presbyopiais a form oflongsightedness that accompanies the onset ofmiddle age.The second gives the spectacle prescription:myopemeans short-sighted and hypermetropemeans longsighted.The third shows whether thepatient is astigmatic,and the fourth relates to the rate oftear production,whichis important in this context because tears lubricate contact lenses.The ﬁnalcolumn shows which kind oflenses to prescribe:hard,soft,or none.All possi-ble combinations ofthe attribute values are represented in the table.A sample set ofrules learned from this information is shown in Figure 1.1.This is a rather large set ofrules,but they do correctly classify all the examples.These rules are complete and deterministic:they give a unique prescription forevery conceivable example.Generally,this is not the case.Sometimes there aresituations in which no rule applies;other times more than one rule may apply,resulting in conﬂicting recommendations.Sometimes probabilities or weights1.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS13If tear production rate = reduced then recommendation = noneIf age = young and astigmatic = no and   tear production rate = normal then recommendation = softIf age = pre-presbyopic and astigmatic = no and   tear production rate = normal then recommendation = softIf age = presbyopic and spectacle prescription = myope and   astigmatic = no then recommendation = noneIf spectacle prescription = hypermetrope and astigmatic = no and   tear production rate = normal then recommendation = softIf spectacle prescription = myope and astigmatic = yes and   tear production rate = normal then recommendation = hardIf age = young and astigmatic = yes and   tear production rate = normal then recommendation = hardIf age = pre-presbyopic and   spectacle prescription = hypermetrope and astigmatic = yes   then recommendation = noneIf age = presbyopic and spectacle prescription = hypermetrope   and astigmatic = yes then recommendation = noneFigure1.1Rules for the contact lens data.P088407-Ch001.qxd  4/30/05  11:11 AM  Page 13may be associated with the rules themselves to indicate that some are moreimportant,or more reliable,than others.You might be wondering whether there is a smaller rule set that performs aswell.Ifso,would you be better offusing the smaller rule set and,ifso,why?These are exactly the kinds ofquestions that will occupy us in this book.Becausethe examples form a complete set for the problem space,the rules do no morethan summarize all the information that is given,expressing it in a different andmore concise way.Even though it involves no generalization,this is often a veryuseful thing to do! People frequently use machine learning techniques to gaininsight into the structure oftheir data rather than to make predictions for newcases.In fact,a prominent and successful line ofresearch in machine learningbegan as an attempt to compress a huge database ofpossible chess endgamesand their outcomes into a data structure ofreasonable size.The data structurechosen for this enterprise was not a set ofrules but a decision tree.Figure 1.2 shows a structural description for the contact lens data in the formofa decision tree,which for many purposes is a more concise and perspicuousrepresentation ofthe rules and has the advantage that it can be visualized moreeasily.(However,this decision tree—in contrast to the rule set given in Figure1.1—classiﬁes two examples incorrectly.) The tree calls ﬁrst for a test on tearproduction rate,and the ﬁrst two branches correspond to the two possible out-comes.Iftear production rateis reduced(the left branch),the outcome is none.Ifit is normal(the right branch),a second test is made,this time on astigma-tism.Eventually,whatever the outcome ofthe tests,a leafofthe tree is reached14CHAPTER 1|WHAT’S IT ALL ABOUT?normaltear production ratereducedhypermetropemyopenoneastigmatismsofthardnonespectacle prescriptionyesnoFigure1.2Decision tree for thecontact lens data.P088407-Ch001.qxd  4/30/05  11:11 AM  Page 14that dictates the contact lens recommendation for that case.The question ofwhat is the most natural and easily understood format for the output from amachine learning scheme is one that we will return to in Chapter 3.Irises: A classic numeric datasetThe iris dataset,which dates back to seminal work by the eminent statisticianR.A.Fisher in the mid-1930s and is arguably the most famous dataset used indata mining,contains 50 examples each ofthree types ofplant:Iris setosa,Irisversicolor,and Iris virginica.It is excerpted in Table 1.4.There are four attrib-utes:sepal length,sepal width,petal length,and petal width(all measured in cen-timeters).Unlike previous datasets,all attributes have values that are numeric.The following set ofrules might be learned from this dataset:If petal length <2.45 then Iris setosaIf sepal width <2.10 then Iris versicolorIf sepal width <2.45 and petal length <4.55 then Iris versicolorIf sepal width <2.95 and petal width <1.35 then Iris versicolorIf petal length ≥2.45 and petal length <4.45 then Iris versicolorIf sepal length ≥5.85 and petal length <4.75 then Iris versicolor1.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS15Table 1.4The iris data.Sepal Sepal widthPetal length Petal widthlength (cm)(cm)(cm)(cm)Type15.13.51.40.2Iris setosa24.93.01.40.2Iris setosa34.73.21.30.2Iris setosa44.63.11.50.2Iris setosa55.03.61.40.2Iris setosa...517.03.24.71.4Iris versicolor526.43.24.51.5Iris versicolor536.93.14.91.5Iris versicolor545.52.34.01.3Iris versicolor556.52.84.61.5Iris versicolor...1016.33.36.02.5Iris virginica1025.82.75.11.9Iris virginica1037.13.05.92.1Iris virginica1046.32.95.61.8Iris virginica1056.53.05.82.2Iris virginica...P088407-Ch001.qxd  4/30/05  11:11 AM  Page 15If sepal width <2.55 and petal length <4.95 and petal width <1.55 then Iris versicolorIf petal length ≥2.45 and petal length <4.95 and petal width <1.55 then Iris versicolorIf sepal length ≥6.55 and petal length <5.05 then Iris versicolorIf sepal width <2.75 and petal width <1.65 and sepal length <6.05 then Iris versicolorIf sepal length ≥5.85 and sepal length <5.95 and petal length <4.85 then Iris versicolorIf petal length ≥5.15 then Iris virginicaIf petal width ≥1.85 then Iris virginicaIf petal width ≥1.75 and sepal width <3.05 then Iris virginicaIf petal length ≥4.95 and petal width <1.55 then Iris virginicaThese rules are very cumbersome,and we will see in Chapter 3 how morecompact rules can be expressed that convey the same information.CPU performance: Introducing numeric predictionAlthough the iris dataset involves numeric attributes,the outcome—the type ofiris—is a category,not a numeric value.Table 1.5 shows some data for whichthe outcome and the attributes are numeric.It concerns the relative perform-ance ofcomputer processing power on the basis ofa number ofrelevant attributes;each row represents 1 of209 different computer conﬁgurations.The classic way ofdealing with continuous prediction is to write the outcomeas a linear sum ofthe attribute values with appropriate weights,for example:16CHAPTER 1|WHAT’S IT ALL ABOUT?Table 1.5The CPU performance data.MainCyclememory (KB)CacheChannelstime (ns)Min.Max.(KB)Min.Max.PerformanceMYCTMMINMMAXCACHCHMINCHMAXPRP112525660002561612819822980003200032832269329800032000328322204298000320003283217252980001600032816132...2071252000800002145220848051280003200672094801000400000045P088407-Ch001.qxd  4/30/05  11:11 AM  Page 16PRP =-55.9 +0.0489 MYCT +0.0153 MMIN +0.0056 MMAX +0.6410 CACH -0.2700 CHMIN +1.480 CHMAX.(The abbreviated variable names are given in the second row ofthe table.) Thisis called a regression equation,and the process ofdetermining the weights iscalled regression,a well-known procedure in statistics that we will review inChapter 4.However,the basic regression method is incapable ofdiscoveringnonlinear relationships (although variants do exist—indeed,one will bedescribed in Section 6.3),and in Chapter 3 we will examine different represen-tations that can be used for predicting numeric quantities.In the iris and central processing unit (CPU) performance data,all the attributes have numeric values.Practical situations frequently present a mixtureofnumeric and nonnumeric attributes.Labor negotiations: A more realistic exampleThe labor negotiations dataset in Table 1.6 summarizes the outcome ofCana-dian contract negotiations in 1987 and 1988.It includes all collective agreementsreached in the business and personal services sector for organizations with atleast 500 members (teachers,nurses,university staff,police,etc.).Each case con-cerns one contract,and the outcome is whether the contract is deemed accept-ableor unacceptable.The acceptable contracts are ones in which agreementswere accepted by both labor and management.The unacceptable ones are eitherknown offers that fell through because one party would not accept them oracceptable contracts that had been signiﬁcantly perturbed to the extent that,inthe view ofexperts,they would not have been accepted.There are 40 examples in the dataset (plus another 17 which are normallyreserved for test purposes).Unlike the other tables here,Table 1.6 presents theexamples as columns rather than as rows;otherwise,it would have to bestretched over several pages.Many ofthe values are unknown or missing,asindicated by question marks.This is a much more realistic dataset than the others we have seen.It con-tains many missing values,and it seems unlikely that an exact classiﬁcation canbe obtained.Figure 1.3 shows two decision trees that represent the dataset.Figure 1.3(a)is simple and approximate:it doesn’t represent the data exactly.For example,itwill predict badfor some contracts that are actually marked good.But it doesmake intuitive sense:a contract is bad (for the employee!) ifthe wage increasein the ﬁrst year is too small (less than 2.5%).Ifthe ﬁrst-year wage increase islarger than this,it is good ifthere are lots ofstatutory holidays (more than 10days).Even ifthere are fewer statutory holidays,it is good ifthe ﬁrst-year wageincrease is large enough (more than 4%).1.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS17P088407-Ch001.qxd  4/30/05  11:11 AM  Page 17Figure 1.3(b) is a more complex decision tree that represents the samedataset.In fact,this is a more accurate representation ofthe actual dataset thatwas used to create the tree.But it is not necessarily a more accurate representa-tion ofthe underlying concept ofgood versus bad contracts.Look down the leftbranch.It doesn’t seem to make sense intuitively that,ifthe working hoursexceed 36,a contract is bad ifthere is no health-plan contribution or a fullhealth-plan contribution but is good ifthere is a halfhealth-plan contribution.It is certainly reasonable that the health-plan contribution plays a role in thedecision but not ifhalfis good and both full and none are bad.It seems likelythat this is an artifact ofthe particular values used to create the decision treerather than a genuine feature ofthe good versus bad distinction.The tree in Figure 1.3(b) is more accurate on the data that was used to trainthe classiﬁer but will probably perform less well on an independent set oftestdata.It is “overﬁtted”to the training data—it follows it too slavishly.The treein Figure 1.3(a) is obtained from the one in Figure 1.3(b) by a process ofpruning,which we will learn more about in Chapter 6.Soybean classiﬁcation: A classic machine learning successAn often-quoted early success story in the application ofmachine learning topractical problems is the identiﬁcation ofrules for diagnosing soybean diseases.The data is taken from questionnaires describing plant diseases.There are about18CHAPTER 1|WHAT’S IT ALL ABOUT?Table 1.6The labor negotiations data.AttributeType123...40durationyears1232wage increase 1st yearpercentage2%4%4.3%4.5wage increase 2nd yearpercentage?5%4.4%4.0wage increase 3rd yearpercentage????cost of living adjustment{none, tcf, tc}nonetcf?noneworking hours per weekhours28353840pension{none, ret-allw, empl-cntr}none???standby paypercentage?13%??shift-work supplementpercentage?5%4%4education allowance{yes, no}yes???statutory holidaysdays11151212vacation{below-avg, avg, gen}avggengenavglong-term disability assistance{yes, no}no??yesdental plan contribution{none, half, full}none?fullfullbereavement assistance{yes, no}no??yeshealth plan contribution{none, half, full}none?fullhalfacceptability of contract{good, bad}badgoodgoodgoodP088407-Ch001.qxd  4/30/05  11:11 AM  Page 181.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS19wage increase first yearbad≤ 2.5statutory holidays> 2.5good> 10wage increase first year≤ 10bad≤ 4 good> 4(a)≤ 2.5statutory holidays> 2.5bad≤ 36health plan contribution> 36good> 10wage increasefirst yearwage increasefirst yearworking hoursper week≤ 10badnonegoodhalfbadfullbad≤ 4good> 4(b)Figure1.3Decision trees for the labor negotiations data.P088407-Ch001.qxd  4/30/05  11:11 AM  Page 19680 examples,each representing a diseased plant.Plants were measured on 35attributes,each one having a small set ofpossible values.Examples are labeledwith the diagnosis ofan expert in plant biology:there are 19 disease categoriesaltogether—horrible-sounding diseases such as diaporthe stem canker,rhizoc-tonia root rot,and bacterial blight,to mention just a few.Table 1.7 gives the attributes,the number ofdifferent values that each canhave,and a sample record for one particular plant.The attributes are placed intodifferent categories just to make them easier to read.Here are two example rules,learned from this data:If[leaf condition is normal andstem condition is abnormal andstem cankers is below soil line andcanker lesion color is brown]thendiagnosis is rhizoctonia root rotIf[leaf malformation is absent andstem condition is abnormal andstem cankers is below soil line andcanker lesion color is brown]thendiagnosis is rhizoctonia root rotThese rules nicely illustrate the potential role ofprior knowledge—often calleddomain knowledge—in machine learning,because the only difference betweenthe two descriptions is leafcondition is normal versus leafmalformation isabsent.Now,in this domain,ifthe leafcondition is normal then leafmalfor-mation is necessarily absent,so one ofthese conditions happens to be a specialcase ofthe other.Thus ifthe ﬁrst rule is true,the second is necessarily true aswell.The only time the second rule comes into play is when leafmalformationis absent but leafcondition is notnormal,that is,when something other thanmalformation is wrong with the leaf.This is certainly not apparent from a casualreading ofthe rules.Research on this problem in the late 1970s found that these diagnostic rulescould be generated by a machine learning algorithm,along with rules for everyother disease category,from about 300 training examples.These training examples were carefully selected from the corpus ofcases as being quite differ-ent from one another—“far apart”in the example space.At the same time,theplant pathologist who had produced the diagnoses was interviewed,and hisexpertise was translated into diagnostic rules.Surprisingly,the computer-generated rules outperformed the expert-derived rules on the remaining testexamples.They gave the correct disease top ranking 97.5% ofthe time com-pared with only 72% for the expert-derived rules.Furthermore,not only did20CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 201.2SIMPLE EXAMPLES:THE WEATHER PROBLEM AND OTHERS21Table 1.7The soybean data.NumberAttributeof valuesSample valueEnvironmenttime of occurrence7Julyprecipitation3above normaltemperature3normalcropping history4same as last yearhail damage2yesdamaged area4scatteredseverity3severeplant height2normalplant growth2abnormalseed treatment3fungicidegermination3less than 80%Seedcondition2normalmold growth2absentdiscoloration2absentsize2normalshriveling2absentFruitcondition of fruit pods3normalfruit spots5—Leafcondition2abnormalleaf spot size3—yellow leaf spot halo3absentleaf spot margins3—shredding2absentleaf malformation2absentleaf mildew growth3absentStemcondition2abnormalstem lodging2yesstem cankers4above soil linecanker lesion color3—fruiting bodies on stems2presentexternal decay of stem3ﬁrm and drymycelium on stem2absentinternal discoloration3nonesclerotia2absentRootcondition3normalDiagnosisdiaporthe stem19cankerP088407-Ch001.qxd  4/30/05  11:11 AM  Page 21the learning algorithm ﬁnd rules that outperformed those ofthe expert collab-orator,but the same expert was so impressed that he allegedly adopted the dis-covered rules in place ofhis own!1.3Fielded applicationsThe examples that we opened with are speculative research projects,not pro-duction systems.And the preceding illustrations are toy problems:they aredeliberately chosen to be small so that we can use them to work through algo-rithms later in the book.Where’s the beef? Here are some applications ofmachine learning that have actually been put into use.Being ﬁelded applications,the illustrations that follow tend to stress the useoflearning in performance situations,in which the emphasis is on ability toperform well on new examples.This book also describes the use oflearningsystems to gain knowledge from decision structures that are inferred from thedata.We believe that this is as important—probably even more important inthe long run—a use ofthe technology as merely making high-performance pre-dictions.Still,it will tend to be underrepresented in ﬁelded applications becausewhen learning techniques are used to gain insight,the result is not normally asystem that is put to work as an application in its own right.Nevertheless,inthree ofthe examples that follow,the fact that the decision structure is com-prehensible is a key feature in the successful adoption ofthe application.Decisions involving judgmentWhen you apply for a loan,you have to ﬁll out a questionnaire that asks for relevant ﬁnancial and personal information.This information is used by theloan company as the basis for its decision as to whether to lend you money.Suchdecisions are typically made in two stages.First,statistical methods are used todetermine clear “accept”and “reject”cases.The remaining borderline cases aremore difﬁcult and call for human judgment.For example,one loan companyuses a statistical decision procedure to calculate a numeric parameter based onthe information supplied in the questionnaire.Applicants are accepted ifthisparameter exceeds a preset threshold and rejected ifit falls below a secondthreshold.This accounts for 90% ofcases,and the remaining 10% are referredto loan ofﬁcers for a decision.On examining historical data on whether appli-cants did indeed repay their loans,however,it turned out that halfofthe bor-derline applicants who were granted loans actually defaulted.Although it wouldbe tempting simply to deny credit to borderline customers,credit industry pro-fessionals pointed out that ifonly their repayment future could be reliably deter-mined it is precisely these customers whose business should be wooed;they tendto be active customers ofa credit institution because their ﬁnances remain in a22CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 22chronically volatile condition.A suitable compromise must be reached betweenthe viewpoint ofa company accountant,who dislikes bad debt,and that ofasales executive,who dislikes turning business away.Enter machine learning.The input was 1000 training examples ofborderlinecases for which a loan had been made that speciﬁed whether the borrower hadﬁnally paid offor defaulted.For each training example,about 20 attributes wereextracted from the questionnaire,such as age,years with current employer,yearsat current address,years with the bank,and other credit cards possessed.Amachine learning procedure was used to produce a small set ofclassiﬁcationrules that made correct predictions on two-thirds ofthe borderline cases in anindependently chosen test set.Not only did these rules improve the success rateofthe loan decisions,but the company also found them attractive because theycould be used to explain to applicants the reasons behind the decision.Althoughthe project was an exploratory one that took only a small development effort,the loan company was apparently so pleased with the result that the rules wereput into use immediately.Screening imagesSince the early days ofsatellite technology,environmental scientists have beentrying to detect oil slicks from satellite images to give early warning ofecolog-ical disasters and deter illegal dumping.Radar satellites provide an opportunityfor monitoring coastal waters day and night,regardless ofweather conditions.Oil slicks appear as dark regions in the image whose size and shape evolvedepending on weather and sea conditions.However,other look-alike darkregions can be caused by local weather conditions such as high wind.Detectingoil slicks is an expensive manual process requiring highly trained personnel whoassess each region in the image.A hazard detection system has been developed to screen images for subse-quent manual processing.Intended to be marketed worldwide to a wide varietyofusers—government agencies and companies—with different objectives,applications,and geographic areas,it needs to be highly customizable to indi-vidual circumstances.Machine learning allows the system to be trained onexamples ofspills and nonspills supplied by the user and lets the user controlthe tradeoffbetween undetected spills and false alarms.Unlike other machinelearning applications,which generate a classiﬁer that is then deployed in theﬁeld,here it is the learning method itselfthat will be deployed.The input is a set ofraw pixel images from a radar satellite,and the outputis a much smaller set ofimages with putative oil slicks marked by a coloredborder.First,standard image processing operations are applied to normalize theimage.Then,suspicious dark regions are identiﬁed.Several dozen attributes are extracted from each region,characterizing its size,shape,area,intensity,1.3FIELDED APPLICATIONS23P088407-Ch001.qxd  4/30/05  11:11 AM  Page 23sharpness and jaggedness ofthe boundaries,proximity to other regions,andinformation about the background in the vicinity ofthe region.Finally,stan-dard learning techniques are applied to the resulting attribute vectors.Several interesting problems were encountered.One is the scarcity oftrain-ing data.Oil slicks are (fortunately) very rare,and manual classiﬁcation isextremely costly.Another is the unbalanced nature ofthe problem:ofthe manydark regions in the training data,only a very small fraction are actual oil slicks.A third is that the examples group naturally into batches,with regions drawnfrom each image forming a single batch,and background characteristics varyfrom one batch to another.Finally,the performance task is to serve as a ﬁlter,and the user must be provided with a convenient means ofvarying the false-alarm rate.Load forecastingIn the electricity supply industry,it is important to determine future demandfor power as far in advance as possible.Ifaccurate estimates can be made forthe maximum and minimum load for each hour,day,month,season,and year,utility companies can make signiﬁcant economies in areas such as setting theoperating reserve,maintenance scheduling,and fuel inventory management.An automated load forecasting assistant has been operating at a major utilitysupplier over the past decade to generate hourly forecasts 2 days in advance.Theﬁrst step was to use data collected over the previous 15 years to create a sophis-ticated load model manually.This model had three components:base load forthe year,load periodicity over the year,and the effect ofholidays.To normalizefor the base load,the data for each previous year was standardized by subtract-ing the average load for that year from each hourly reading and dividing by thestandard deviation over the year.Electric load shows periodicity at three fun-damental frequencies:diurnal,where usage has an early morning minimum andmidday and afternoon maxima;weekly,where demand is lower at weekends;and seasonal,where increased demand during winter and summer for heatingand cooling,respectively,creates a yearly cycle.Major holidays such as Thanks-giving,Christmas,and New Year’s Day show signiﬁcant variation from thenormal load and are each modeled separately by averaging hourly loads for thatday over the past 15 years.Minor ofﬁcial holidays,such as Columbus Day,arelumped together as school holidays and treated as an offset to the normaldiurnal pattern.All ofthese effects are incorporated by reconstructing a year’sload as a sequence oftypical days,ﬁtting the holidays in their correct position,and denormalizing the load to account for overall growth.Thus far,the load model is a static one,constructed manually from histori-cal data,and implicitly assumes “normal”climatic conditions over the year.Theﬁnal step was to take weather conditions into account using a technique that24CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 24locates the previous day most similar to the current circumstances and uses thehistorical information from that day as a predictor.In this case the predictionis treated as an additive correction to the static load model.To guard againstoutliers,the eight most similar days are located and their additive correctionsaveraged.A database was constructed oftemperature,humidity,wind speed,and cloud cover at three local weather centers for each hour ofthe 15-year historical record,along with the difference between the actual load and that predicted by the static model.A linear regression analysis was performed todetermine the relative effects ofthese parameters on load,and the coefﬁcientswere used to weight the distance function used to locate the most similar days.The resulting system yielded the same performance as trained human fore-casters but was far quicker—taking seconds rather than hours to generate a dailyforecast.Human operators can analyze the forecast’s sensitivity to simulatedchanges in weather and bring up for examination the “most similar”days thatthe system used for weather adjustment.DiagnosisDiagnosis is one ofthe principal application areas ofexpert systems.Althoughthe handcrafted rules used in expert systems often perform well,machine learn-ing can be useful in situations in which producing rules manually is too laborintensive.Preventative maintenance ofelectromechanical devices such as motors andgenerators can forestall failures that disrupt industrial processes.Techniciansregularly inspect each device,measuring vibrations at various points to deter-mine whether the device needs servicing.Typical faults include shaft misalign-ment,mechanical loosening,faulty bearings,and unbalanced pumps.Aparticular chemical plant uses more than 1000 different devices,ranging fromsmall pumps to very large turbo-alternators,which until recently were diag-nosed by a human expert with 20 years ofexperience.Faults are identiﬁed bymeasuring vibrations at different places on the device’s mounting and usingFourier analysis to check the energy present in three different directions at eachharmonic ofthe basic rotation speed.This information,which is very noisybecause oflimitations in the measurement and recording procedure,is studiedby the expert to arrive at a diagnosis.Although handcrafted expert system ruleshad been developed for some situations,the elicitation process would have tobe repeated several times for different types ofmachinery;so a learningapproach was investigated.Six hundred faults,each comprising a set ofmeasurements along with theexpert’s diagnosis,were available,representing 20 years ofexperience in theﬁeld.About halfwere unsatisfactory for various reasons and had to be discarded;the remainder were used as training examples.The goal was not to determine1.3FIELDED APPLICATIONS25P088407-Ch001.qxd  4/30/05  11:11 AM  Page 25whether or not a fault existed,but to diagnose the kind offault,given that onewas there.Thus there was no need to include fault-free cases in the training set.The measured attributes were rather low level and had to be augmented by inter-mediate concepts,that is,functions ofbasic attributes,which were deﬁned inconsultation with the expert and embodied some causal domain knowledge.The derived attributes were run through an induction algorithm to produce aset ofdiagnostic rules.Initially,the expert was not satisﬁed with the rulesbecause he could not relate them to his own knowledge and experience.Forhim,mere statistical evidence was not,by itself,an adequate explanation.Further background knowledge had to be used before satisfactory rules weregenerated.Although the resulting rules were quite complex,the expert likedthem because he could justify them in light ofhis mechanical knowledge.Hewas pleased that a third ofthe rules coincided with ones he used himselfandwas delighted to gain new insight from some ofthe others.Performance tests indicated that the learned rules were slightly superior tothe handcrafted ones that had previously been elicited from the expert,and thisresult was conﬁrmed by subsequent use in the chemical factory.It is interestingto note,however,that the system was put into use not because ofits good per-formance but because the domain expert approved ofthe rules that had beenlearned.Marketing and salesSome ofthe most active applications ofdata mining have been in the area ofmarketing and sales.These are domains in which companies possess massivevolumes ofprecisely recorded data,data which—it has only recently been real-ized—is potentially extremely valuable.In these applications,predictions them-selves are the chiefinterest:the structure ofhow decisions are made is oftencompletely irrelevant.We have already mentioned the problem ofﬁckle customer loyalty and thechallenge ofdetecting customers who are likely to defect so that they can bewooed back into the fold by giving them special treatment.Banks were earlyadopters ofdata mining technology because oftheir successes in the use ofmachine learning for credit assessment.Data mining is now being used toreduce customer attrition by detecting changes in individual banking patternsthat may herald a change ofbank or even life changes—such as a move toanother city—that could result in a different bank being chosen.It may reveal,for example,a group ofcustomers with above-average attrition rate who domost oftheir banking by phone after hours when telephone response is slow.Data mining may determine groups for whom new services are appropriate,such as a cluster ofproﬁtable,reliable customers who rarely get cash advancesfrom their credit card except in November and December,when they are pre-26CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 26pared to pay exorbitant interest rates to see them through the holiday season.Inanother domain,cellular phone companies ﬁght churnby detecting patterns ofbehavior that could beneﬁt from new services,and then advertise such servicesto retain their customer base.Incentives provided speciﬁcally to retain existingcustomers can be expensive,and successful data mining allows them to be pre-cisely targeted to those customers where they are likely to yield maximum beneﬁt.Market basket analysisis the use ofassociation techniques to ﬁnd groups ofitems that tend to occur together in transactions,typically supermarket check-out data.For many retailers this is the only source ofsales information that isavailable for data mining.For example,automated analysis ofcheckout datamay uncover the fact that customers who buy beer also buy chips,a discoverythat could be signiﬁcant from the supermarket operator’s point ofview(although rather an obvious one that probably does not need a data miningexercise to discover).Or it may come up with the fact that on Thursdays,cus-tomers often purchase diapers and beer together,an initially surprising resultthat,on reﬂection,makes some sense as young parents stock up for a weekendat home.Such information could be used for many purposes:planning storelayouts,limiting special discounts to just one ofa set ofitems that tend to bepurchased together,offering coupons for a matching product when one ofthemis sold alone,and so on.There is enormous added value in being able to iden-tify individual customer’s sales histories.In fact,this value is leading to a pro-liferation ofdiscount cards or “loyalty”cards that allow retailers to identifyindividual customers whenever they make a purchase;the personal data thatresults will be far more valuable than the cash value ofthe discount.Identiﬁca-tion ofindividual customers not only allows historical analysis ofpurchasingpatterns but also permits precisely targeted special offers to be mailed out toprospective customers.This brings us to direct marketing,another popular domain for data mining.Promotional offers are expensive and have an extremely low—but highly proﬁtable—response rate.Any technique that allows a promotional mailout tobe more tightly focused,achieving the same or nearly the same response froma much smaller sample,is valuable.Commercially available databases contain-ing demographic information based on ZIP codes that characterize the associ-ated neighborhood can be correlated with information on existing customersto ﬁnd a socioeconomic model that predicts what kind ofpeople will turn outto be actual customers.This model can then be used on information gained inresponse to an initial mailout,where people send back a response card or callan 800 number for more information,to predict likely future customers.Directmail companies have the advantage over shopping-mall retailers ofhaving com-plete purchasing histories for each individual customer and can use data miningto determine those likely to respond to special offers.Targeted campaigns arecheaper than mass-marketed campaigns because companies save money by1.3FIELDED APPLICATIONS27P088407-Ch001.qxd  4/30/05  11:11 AM  Page 27sending offers only to those likely to want the product.Machine learning canhelp companies to ﬁnd the targets.Other applicationsThere are countless other applications ofmachine learning.We brieﬂy mentiona few more areas to illustrate the breadth ofwhat has been done.Sophisticated manufacturing processes often involve tweaking controlparameters.Separating crude oil from natural gas is an essential prerequisite tooil reﬁnement,and controlling the separation process is a tricky job.BritishPetroleum used machine learning to create rules for setting the parameters.Thisnow takes just 10 minutes,whereas previously human experts took more thana day.Westinghouse faced problems in their process for manufacturing nuclearfuel pellets and used machine learning to create rules to control the process.This was reported to save them more than $10 million per year (in 1984).TheTennessee printing company R.R.Donnelly applied the same idea to controlrotogravure printing presses to reduce artifacts caused by inappropriate parameter settings,reducing the number ofartifacts from more than 500 eachyear to less than 30.In the realm ofcustomer support and service,we have already described adju-dicating loans,and marketing and sales applications.Another example ariseswhen a customer reports a telephone problem and the company must decidewhat kind oftechnician to assign to the job.An expert system developed by BellAtlantic in 1991 to make this decision was replaced in 1999 by a set ofruleslearned using machine learning,which saved more than $10 million per year bymaking fewer incorrect decisions.There are many scientiﬁc applications.In biology,machine learning is usedto help identify the thousands ofgenes within each new genome.In biomedi-cine,it is used to predict drug activity by analyzing not just the chemical properties ofdrugs but also their three-dimensional structure.This acceleratesdrug discovery and reduces its cost.In astronomy,machine learning has been used to develop a fully automatic cataloguing system for celestial objectsthat are too faint to be seen by visual inspection.In chemistry,it has been usedto predict the structure ofcertain organic compounds from magnetic resonancespectra.In all these applications,machine learning techniques have attainedlevels ofperformance—or should we say skill?—that rival or surpass humanexperts.Automation is especially welcome in situations involving continuous moni-toring,a job that is time consuming and exceptionally tedious for humans.Eco-logical applications include the oil spill monitoring described earlier.Someother applications are rather less consequential—for example,machine learn-ing is being used to predict preferences for TV programs based on past choices28CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 28and advise viewers about the available channels.Still others may save lives.Intensive care patients may be monitored to detect changes in variables thatcannot be explained by circadian rhythm,medication,and so on,raising an alarm when appropriate.Finally,in a world that relies on vulnerable net-worked computer systems and is increasingly concerned about cybersecurity,machine learning is used to detect intrusion by recognizing unusual patterns ofoperation.1.4Machine learning and statisticsWhat’s the difference between machine learning and statistics? Cynics,lookingwryly at the explosion ofcommercial interest (and hype) in this area,equatedata mining to statistics plus marketing.In truth,you should not look for adividing line between machine learning and statistics because there is a contin-uum—and a multidimensional one at that—ofdata analysis techniques.Somederive from the skills taught in standard statistics courses,and others are moreclosely associated with the kind ofmachine learning that has arisen out ofcom-puter science.Historically,the two sides have had rather different traditions.Ifforced to point to a single difference ofemphasis,it might be that statistics hasbeen more concerned with testing hypotheses,whereas machine learning hasbeen more concerned with formulating the process ofgeneralization as a searchthrough possible hypotheses.But this is a gross oversimpliﬁcation:statistics isfar more than hypothesis testing,and many machine learning techniques do notinvolve any searching at all.In the past,very similar methods have developed in parallel in machine learn-ing and statistics.One is decision tree induction.Four statisticians (Breiman etal.1984) published a book on Classiﬁcation and regression treesin the mid-1980s,and throughout the 1970s and early 1980s a prominent machine learningresearcher,J.Ross Quinlan,was developing a system for inferring classiﬁcationtrees from examples.These two independent projects produced quite similarmethods for generating trees from examples,and the researchers only becameaware ofone another’s work much later.A second area in which similar methodshave arisen involves the use ofnearest-neighbor methods for classiﬁcation.These are standard statistical techniques that have been extensively adapted bymachine learning researchers,both to improve classiﬁcation performance andto make the procedure more efﬁcient computationally.We will examine bothdecision tree induction and nearest-neighbor methods in Chapter 4.But now the two perspectives have converged.The techniques we willexamine in this book incorporate a great deal ofstatistical thinking.From thebeginning,when constructing and reﬁning the initial example set,standard sta-tistical methods apply:visualization ofdata,selection ofattributes,discarding1.4MACHINE LEARNING AND STATISTICS29P088407-Ch001.qxd  4/30/05  11:11 AM  Page 29outliers,and so on.Most learning algorithms use statistical tests when con-structing rules or trees and for correcting models that are “overﬁtted”in thatthey depend too strongly on the details ofthe particular examples used toproduce them (we have already seen an example ofthis in the two decision treesofFigure 1.3 for the labor negotiations problem).Statistical tests are used tovalidate machine learning models and to evaluate machine learning algorithms.In our study ofpractical techniques for data mining,we will learn a great dealabout statistics.1.5Generalization as searchOne way ofvisualizing the problem oflearning—and one that distinguishes itfrom statistical approaches—is to imagine a search through a space ofpossibleconcept descriptions for one that ﬁts the data.Although the idea ofgeneraliza-tion as search is a powerful conceptual tool for thinking about machine learn-ing,it is not essential for understanding the practical methods described in thisbook.That is why this section is marked optional,as indicated by the gray barin the margin.Suppose,for deﬁniteness,that concepts—the result oflearning—areexpressed as rules such as the ones given for the weather problem in Section 1.2(although other concept description languages would do just as well).Supposethat we list all possible sets ofrules and then look for ones that satisfy a givenset ofexamples.A big job? Yes.An inﬁnitejob? At ﬁrst glance it seems so becausethere is no limit to the number ofrules there might be.But actually the numberofpossible rule sets is ﬁnite.Note ﬁrst that each individual rule is no greaterthan a ﬁxed maximum size,with at most one term for each attribute:for theweather data ofTable 1.2 this involves four terms in all.Because the number ofpossible rules is ﬁnite,the number ofpossible rule setsis ﬁnite,too,althoughextremely large.However,we’d hardly be interested in sets that contained a verylarge number ofrules.In fact,we’d hardly be interested in sets that had morerules than there are examples because it is difﬁcult to imagine needing morethan one rule for each example.So ifwe were to restrict consideration to rulesets smaller than that,the problem would be substantially reduced,althoughstill very large.The threat ofan inﬁnite number ofpossible concept descriptions seems moreserious for the second version ofthe weather problem in Table 1.3 because theserules contain numbers.Ifthey are real numbers,you can’t enumerate them,evenin principle.However,on reﬂection the problem again disappears because thenumbers really just represent breakpoints in the numeric values that appear inthe examples.For instance,consider the temperatureattribute in Table 1.3.Itinvolves the numbers 64,65,68,69,70,71,72,75,80,81,83,and 85—12 dif-30CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 30ferent numbers.There are 13 possible places in which we might want to put abreakpoint for a rule involving temperature.The problem isn’t inﬁnite after all.So the process ofgeneralization can be regarded as a search through an enor-mous,but ﬁnite,search space.In principle,the problem can be solved by enu-merating descriptions and striking out those that do not ﬁt the examplespresented.A positive example eliminates all descriptions that it does not match,and a negative one eliminates those it does match.With each example the setofremaining descriptions shrinks (or stays the same).Ifonly one is left,it is thetarget description—the target concept.Ifseveral descriptions are left,they may still be used to classify unknownobjects.An unknown object that matches all remaining descriptions should beclassiﬁed as matching the target;ifit fails to match any description it should beclassiﬁed as being outside the target concept.Only when it matches somedescriptions but not others is there ambiguity.In this case ifthe classiﬁcationofthe unknown object were revealed,it would cause the set ofremainingdescriptions to shrink because rule sets that classiﬁed the object the wrong waywould be rejected.Enumerating the concept spaceRegarding it as search is a good way oflooking at the learning process.However,the search space,although ﬁnite,is extremely big,and it is generally quiteimpractical to enumerate all possible descriptions and then see which ones ﬁt.In the weather problem there are 4 ¥4 ¥3 ¥3 ¥2 =288 possibilities for eachrule.There are four possibilities for the outlookattribute:sunny,overcast,rainy,or it may not participate in the rule at all.Similarly,there are four for tempera-ture,three for weatherand humidity,and two for the class.Ifwe restrict the ruleset to contain no more than 14 rules (because there are 14 examples in the train-ing set),there are around 2.7 ¥1034possible different rule sets.That’s a lot toenumerate,especially for such a patently trivial problem.Although there are ways ofmaking the enumeration procedure more feasi-ble,a serious problem remains:in practice,it is rare for the process to convergeon a unique acceptable description.Either many descriptions are still in therunning after the examples are processed or the descriptors are all eliminated.The ﬁrst case arises when the examples are not sufﬁciently comprehensive toeliminate all possible descriptions except for the “correct”one.In practice,people often want a single “best”description,and it is necessary to apply someother criteria to select the best one from the set ofremaining descriptions.Thesecond problem arises either because the description language is not expressiveenough to capture the actual concept or because ofnoise in the examples.Ifanexample comes in with the “wrong”classiﬁcation because ofan error in someofthe attribute values or in the class that is assigned to it,this will likely 1.5GENERALIZATION AS SEARCH31P088407-Ch001.qxd  4/30/05  11:11 AM  Page 31eliminate the correct description from the space.The result is that the set ofremaining descriptions becomes empty.This situation is very likely to happenifthe examples contain any noise at all,which inevitably they do except in artiﬁcial situations.Another way oflooking at generalization as search is to imagine it not as aprocess ofenumerating descriptions and striking out those that don’t apply butas a kind ofhill-climbing in description space to ﬁnd the description that bestmatches the set ofexamples according to some prespeciﬁed matching criterion.This is the way that most practical machine learning methods work.However,except in the most trivial cases,it is impractical to search the whole spaceexhaustively;most practical algorithms involve heuristic search and cannotguarantee to ﬁnd the optimal description.BiasViewing generalization as a search in a space ofpossible concepts makes it clearthat the most important decisions in a machine learning system are as follows:䊏The concept description language䊏The order in which the space is searched䊏The way that overﬁtting to the particular training data is avoidedThese three properties are generally referred to as the biasofthe search and arecalled languagebias,searchbias,and overﬁtting-avoidancebias.You bias thelearning scheme by choosing a language in which to express concepts,by search-ing in a particular way for an acceptable description,and by deciding when theconcept has become so complex that it needs to be simpliﬁed.Language biasThe most important question for language bias is whether the concept descrip-tion language is universal or whether it imposes constraints on what concepts canbe learned.Ifyou consider the set ofall possible examples,a concept is really justa division ofit into subsets.In the weather example,ifyou were to enumerate allpossible weather conditions,the playconcept is a subset ofpossible weather con-ditions.A “universal”language is one that is capable ofexpressing every possiblesubset ofexamples.In practice,the set ofpossible examples is generally huge,andin this respect our perspective is a theoretical,not a practical,one.Ifthe concept description language permits statements involving logical or,that is,disjunctions,then any subset can be represented.Ifthe description lan-guage is rule based,disjunction can be achieved by using separate rules.Forexample,one possible concept representation is just to enumerate the examples:If outlook =overcast and temperature =hot and humidity =highand windy =false then play =yes32CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 32If outlook =rainy and temperature =mild and humidity =highand windy =false then play =yesIf outlook =rainy and temperature =cool and humidity =normaland windy =false then play =yesIf outlook =overcast and temperature =cool and humidity =normaland windy =true then play =yes...If none of the above then play =noThis is not a particularly enlightening concept description:it simply records thepositive examples that have been observed and assumes that all the rest are neg-ative.Each positive example is given its own rule,and the concept is the dis-junction ofthe rules.Alternatively,you could imagine having individual rulesfor each ofthe negative examples,too—an equally uninteresting concept.Ineither case the concept description does not perform any generalization;itsimply records the original data.On the other hand,ifdisjunction is notallowed,some possible concepts—sets ofexamples—may not be able to be represented at all.In that case,amachine learning scheme may simply be unable to achieve good performance.Another kind oflanguage bias is that obtained from knowledge ofthe par-ticular domain being used.For example,it may be that some combinations ofattribute values can never happen.This would be the case ifone attributeimplied another.We saw an example ofthis when considering the rules for thesoybean problem described on page 20.Then,it would be pointless to even con-sider concepts that involved redundant or impossible combinations ofattributevalues.Domain knowledge can be used to cut down the search space.Knowl-edge is power:a little goes a long way,and even a small hint can reduce thesearch space dramatically.Search biasIn realistic data mining problems,there are many alternative concept descrip-tions that ﬁt the data,and the problem is to ﬁnd the “best”one according tosome criterion—usually simplicity.We use the term ﬁtin a statistical sense;weseek the best description that ﬁts the data reasonably well.Moreover,it is oftencomputationally infeasible to search the whole space and guarantee that thedescription found really is the best.Consequently,the search procedure isheuristic,and no guarantees can be made about the optimality ofthe ﬁnal result.This leaves plenty ofroom for bias:different search heuristics bias the search indifferent ways.For example,a learning algorithm might adopt a “greedy”search for rules bytrying to ﬁnd the best rule at each stage and adding it in to the rule set.However,it may be that the best pairofrules is not just the two rules that are individu-ally found to be the best.Or when building a decision tree,a commitment to1.5GENERALIZATION AS SEARCH33P088407-Ch001.qxd  4/30/05  11:11 AM  Page 33split early on using a particular attribute might turn out later to be ill consid-ered in light ofhow the tree develops below that node.To get around these prob-lems,a beamsearchcould be used in which irrevocable commitments are notmade but instead a set ofseveral active alternatives—whose number is the beamwidth—are pursued in parallel.This will complicate the learning algorithmquite considerably but has the potential to avoid the myopia associated with agreedy search.Ofcourse,ifthe beam width is not large enough,myopia maystill occur.There are more complex search strategies that help to overcome thisproblem.A more general and higher-level kind ofsearch bias concerns whether thesearch is done by starting with a general description and reﬁning it,or by starting with a speciﬁc example and generalizing it.The former is called ageneral-to-speciﬁcsearch bias;the latter a speciﬁc-to-generalone.Many learningalgorithms adopt the former policy,starting with an empty decision tree,or avery general rule,and specializing it to ﬁt the examples.However,it is perfectlypossible to work in the other direction.Instance-based methods start with a particular example and see how it can be generalized to cover nearby examplesin the same class.Overﬁtting-avoidance biasOverﬁtting-avoidance bias is often just another kind ofsearch bias.But becauseit addresses a rather special problem,we treat it separately.Recall the disjunc-tion problem described previously.The problem is that ifdisjunction is allowed,useless concept descriptions that merely summarize the data become possible,whereas ifit is prohibited,some concepts are unlearnable.To get around thisproblem,it is common to search the concept space starting with the simplestconcept descriptions and proceeding to more complex ones:simplest-ﬁrstordering.This biases the search toward simple concept descriptions.Using a simplest-ﬁrst search and stopping when a sufﬁciently complexconcept description is found is a good way ofavoiding overﬁtting.It is some-times called forwardpruningor prepruningbecause complex descriptions arepruned away before they are reached.The alternative,backwardpruningor post-pruning,is also viable.Here,we ﬁrst ﬁnd a description that ﬁts the data well andthen prune it back to a simpler description that also ﬁts the data.This is not asredundant as it sounds:often the only way to arrive at a simple theory is to ﬁnda complex one and then simplify it.Forward and backward pruning are both akind ofoverﬁtting-avoidance bias.In summary,although generalization as search is a nice way to think aboutthe learning problem,bias is the only way to make it feasible in practice.Dif-ferent learning algorithms correspond to different concept description spacessearched with different biases.This is what makes it interesting:different34CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 34description languages and biases serve some problems well and other problemsbadly.There is no universal “best”learning method—as every teacher knows!1.6Data mining and ethicsThe use ofdata—particularly data about people—for data mining has seriousethical implications,and practitioners ofdata mining techniques must actresponsibly by making themselves aware ofthe ethical issues that surround theirparticular application.When applied to people,data mining is frequently used to discriminate—who gets the loan,who gets the special offer,and so on.Certain kinds ofdiscrimination—racial,sexual,religious,and so on—are not only unethical but also illegal.However,the situation is complex:everything depends on theapplication.Using sexual and racial information for medical diagnosis is certainly ethical,but using the same information when mining loan paymentbehavior is not.Even when sensitive information is discarded,there is a risk that models will be built that rely on variables that can be shown to substitutefor racial or sexual characteristics.For example,people frequently live in areas that are associated with particular ethnic identities,so using an area code in a data mining study runs the risk ofbuilding models that are based onrace—even though racial information has been explicitly excluded from thedata.It is widely accepted that before people make a decision to provide personalinformation they need to know how it will be used and what it will be used for,what steps will be taken to protect its conﬁdentiality and integrity,what the con-sequences ofsupplying or withholding the information are,and any rights ofredress they may have.Whenever such information is collected,individualsshould be told these things—not in legalistic small print but straightforwardlyin plain language they can understand.The potential use ofdata mining techniques means that the ways in which arepository ofdata can be used may stretch far beyond what was conceived whenthe data was originally collected.This creates a serious problem:it is necessaryto determine the conditions under which the data was collected and for whatpurposes it may be used.Does the ownership ofdata bestow the right to use itin ways other than those purported when it was originally recorded? Clearly inthe case ofexplicitly collected personal data it does not.But in general the situation is complex.Surprising things emerge from data mining.For example,it has beenreported that one ofthe leading consumer groups in France has found thatpeople with red cars are more likely to default on their car loans.What is the1.6DATA MINING AND ETHICS35P088407-Ch001.qxd  4/30/05  11:11 AM  Page 35status ofsuch a “discovery”? What information is it based on? Under what con-ditions was that information collected? In what ways is it ethical to use it?Clearly,insurance companies are in the business ofdiscriminating amongpeople based on stereotypes—young males pay heavily for automobile insur-ance—but such stereotypes are not based solely on statistical correlations;theyalso involve common-sense knowledge about the world.Whether the precedingﬁnding says something about the kind ofperson who chooses a red car,orwhether it should be discarded as an irrelevancy,is a matter for human judgment based on knowledge ofthe world rather than on purely statistical criteria.When presented with data,you need to ask who is permitted to have accessto it,for what purpose it was collected,and what kind ofconclusions is it legit-imate to draw from it.The ethical dimension raises tough questions for thoseinvolved in practical data mining.It is necessary to consider the norms ofthecommunity that is used to dealing with the kind ofdata involved,standards thatmay have evolved over decades or centuries but ones that may not be known tothe information specialist.For example,did you know that in the library com-munity,it is taken for granted that the privacy ofreaders is a right that is jealously protected? Ifyou call your university library and ask who has such-and-such a textbook out on loan,they will not tell you.This prevents a studentfrom being subjected to pressure from an irate professor to yield access to a bookthat she desperately needs for her latest grant application.It also prohibitsenquiry into the dubious recreational reading tastes ofthe university ethicscommittee chairman.Those who build,say,digital libraries may not be awareofthese sensitivities and might incorporate data mining systems that analyzeand compare individuals’reading habits to recommend new books—perhapseven selling the results to publishers!In addition to community standards for the use ofdata,logical and scientiﬁcstandards must be adhered to when drawing conclusions from it.Ifyou do comeup with conclusions (such as red car owners being greater credit risks),you needto attach caveats to them and back them up with arguments other than purelystatistical ones.The point is that data mining is just a tool in the whole process:it is people who take the results,along with other knowledge,and decide whataction to apply.Data mining prompts another question,which is really a political one:towhat use are society’s resources being put? We mentioned previously the appli-cation ofdata mining to basket analysis,where supermarket checkout recordsare analyzed to detect associations among items that people purchase.What useshould be made ofthe resulting information? Should the supermarket managerplace the beer and chips together,to make it easier for shoppers,or farther apart,making it less convenient for them,maximizing their time in the store,andtherefore increasing their likelihood ofbeing drawn into unplanned further36CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 36purchases? Should the manager move the most expensive,most proﬁtablediapers near the beer,increasing sales to harried fathers ofa high-margin itemand add further luxury baby products nearby?Ofcourse,anyone who uses advanced technologies should consider thewisdom ofwhat they are doing.Ifdatais characterized as recorded facts,theninformationis the set ofpatterns,or expectations,that underlie the data.Youcould go on to deﬁne knowledgeas the accumulation ofyour set ofexpectationsand wisdomas the value attached to knowledge.Although we will not pursue itfurther here,this issue is worth pondering.As we saw at the very beginning ofthis chapter,the techniques described inthis book may be called upon to help make some ofthe most profound andintimate decisions that life presents.Data mining is a technology that we needto take seriously.1.7Further readingTo avoid breaking up the ﬂow ofthe main text,all references are collected in asection at the end ofeach chapter.This ﬁrst Furtherreadingsection describespapers,books,and other resources relevant to the material covered in Chapter1.The human invitrofertilization research mentioned in the opening to thischapter was undertaken by the Oxford University Computing Laboratory,and the research on cow culling was performed in the Computer ScienceDepartment at the University ofWaikato,New Zealand.The example ofthe weather problem is from Quinlan (1986) and has beenwidely used to explain machine learning schemes.The corpus ofexample prob-lems mentioned in the introduction to Section 1.2 is available from Blake et al.(1998).The contact lens example is from Cendrowska (1998),who introducedthe PRISM rule-learning algorithm that we will encounter in Chapter 4.The irisdataset was described in a classic early paper on statistical inference (Fisher1936).The labor negotiations data is from the Collectivebargainingreview,apublication ofLabour Canada issued by the Industrial Relations InformationService (BLI 1988),and the soybean problem was ﬁrst described by Michalskiand Chilausky (1980).Some ofthe applications in Section 1.3 are covered in an excellent paper thatgives plenty ofother applications ofmachine learning and rule induction(Langley and Simon 1995);another source ofﬁelded applications is a specialissue ofthe MachineLearningJournal(Kohavi and Provost 1998).The loancompany application is described in more detail by Michie (1989),the oil slickdetector is from Kubat et al.(1998),the electric load forecasting work is byJabbour et al.(1988),and the application to preventative maintenance ofelectromechanical devices is from Saitta and Neri (1998).Fuller descriptions 1.7FURTHER READING37P088407-Ch001.qxd  4/30/05  11:11 AM  Page 37ofsome ofthe other projects mentioned in Section 1.3 (including the ﬁguresofdollars saved and related literature references) appear at the Web sites oftheAlberta Ingenuity Centre for Machine Learning and MLnet,a Europeannetwork for machine learning.The book Classiﬁcationandregressiontreesmentioned in Section 1.4 is byBreiman et al.(1984),and the independently derived but similar scheme ofQuinlan was described in a series ofpapers that eventually led to a book(Quinlan 1993).The ﬁrst book on data mining appeared in 1991 (Piatetsky-Shapiro andFrawley 1991)—a collection ofpapers presented at a workshop on knowledgediscovery in databases in the late 1980s.Another book from the same stable hasappeared since (Fayyad et al.1996) from a 1994 workshop.There followed arash ofbusiness-oriented books on data mining,focusing mainly on practicalaspects ofhow it can be put into practice with only rather superﬁcial descrip-tions ofthe technology that underlies the methods used.They are valuablesources ofapplications and inspiration.For example,Adriaans and Zantige(1996) from Syllogic,a European systems and database consultancy,provide anearly introduction to data mining.Berry and Linoff(1997),from a Pennsylva-nia-based company specializing in data warehousing and data mining,give anexcellent and example-studded review ofdata mining techniques for market-ing,sales,and customer support.Cabena et al.(1998),written by people fromﬁve international IBM laboratories,overview the data mining process withmany examples ofreal-world applications.Dhar and Stein (1997) give a busi-ness perspective on data mining and include broad-brush,popularized reviewsofmany ofthe technologies involved.Groth (1998),working for a provider ofdata mining software,gives a briefintroduction to data mining and then a fairly extensive review ofdata mining software products;the book includes aCD-ROM containing a demo version ofhis company’s product.Weiss andIndurkhya (1998) look at a wide variety ofstatistical techniques for making predictions from what they call “big data.”Han and Kamber (2001) cover datamining from a database perspective,focusing on the discovery ofknowledge inlarge corporate databases.Finally,Hand et al.(2001) produced an interdiscipli-nary book on data mining from an international group ofauthors who are wellrespected in the ﬁeld.Books on machine learning,on the other hand,tend to be academic textssuited for use in university courses rather than practical guides.Mitchell (1997)wrote an excellent book that covers many techniques ofmachine learning,including some—notably genetic algorithms and reinforcement learning—thatare not covered here.Langley (1996) offers another good text.Although the pre-viously mentioned book by Quinlan (1993) concentrates on a particular learn-ing algorithm,C4.5,which we will cover in detail in Chapters 4 and 6,it is agood introduction to some ofthe problems and techniques ofmachine learn-38CHAPTER 1|WHAT’S IT ALL ABOUT?P088407-Ch001.qxd  4/30/05  11:11 AM  Page 38ing.An excellent book on machine learning from a statistical perspective is fromHastie et al.(2001).This is quite a theoretically oriented work,and is beauti-fully produced with apt and telling illustrations.Pattern recognition is a topic that is closely related to machine learning,andmany ofthe same techniques apply.Duda et al.(2001) offer the second editionofa classic and successful book on pattern recognition (Duda and Hart 1973).Ripley (1996) and Bishop (1995) describe the use ofneural networks for patternrecognition.Data mining with neural networks is the subject ofa book by Bigus(1996) ofIBM,which features the IBM Neural Network Utility Product that hedeveloped.There is a great deal ofcurrent interest in support vector machines,whichwe return to in Chapter 6.Cristianini and Shawe-Taylor (2000) give a nice intro-duction,and a follow-up work generalizes this to cover additional algorithms,kernels,and solutions with applications to pattern discovery problems in ﬁeldssuch as bioinformatics,text analysis,and image analysis (Shawe-Taylor and Cristianini 2004).Schölkopfand Smola (2002) provide a comprehensive intro-duction to support vector machines and related kernel methods by two youngresearchers who did their PhD research in this rapidly developing area.1.7FURTHER READING39P088407-Ch001.qxd  4/30/05  11:11 AM  Page 39P088407-Ch001.qxd  4/30/05  11:11 AM  Page 40Before delving into the question ofhow machine learning methods operate,webegin by looking at the different forms the input might take and,in the nextchapter,the different kinds ofoutput that might be produced.With any soft-ware system,understanding what the inputs and outputs are is far more impor-tant than knowing what goes on in between,and machine learning is noexception.The input takes the form ofconcepts,instances,and attributes.We call thething that is to be learned a conceptdescription.The idea ofa concept,like the very idea oflearning in the ﬁrst place,is hard to pin down precisely,and we won’t spend time philosophizing about just what it is and isn’t.In a sense,what we are trying to ﬁnd—the result ofthe learning process—is adescription ofthe concept that is intelligiblein that it can be understood,dis-cussed,and disputed,and operationalin that it can be applied to actual exam-ples.The next section explains some distinctions among different kinds oflearning problems,distinctions that are very concrete and very important inpractical data mining.chapter2Input:Concepts,Instances,and Attributes41P088407-Ch002.qxd  4/30/05  11:10 AM  Page 41The information that the learner is given takes the form ofa set ofinstances.In the illustrations in Chapter 1,each instance was an individual,independentexample ofthe concept to be learned.Ofcourse there are many things you mightlike to learn for which the raw data cannot be expressed as individual,inde-pendent instances.Perhaps background knowledge should be taken intoaccount as part ofthe input.Perhaps the raw data is an agglomerated mass thatcannot be fragmented into individual instances.Perhaps it is a single sequence,say,a time sequence,that cannot meaningfully be cut into pieces.However,thisbook is about simple,practical methods ofdata mining,and we focus on situations in which the information can be supplied in the form ofindividualexamples.Each instance is characterized by the values ofattributes that measure dif-ferent aspects ofthe instance.There are many different types ofattributes,although typical data mining methods deal only with numeric and nominal,orcategorical,ones.Finally,we examine the question ofpreparing input for data mining andintroduce a simple format—the one that is used by the Java code that accom-panies this book—for representing the input information as a text ﬁle.2.1What’s a concept?Four basically different styles oflearning appear in data mining applications.Inclassiﬁcationlearning,the learning scheme is presented with a set ofclassiﬁedexamples from which it is expected to learn a way ofclassifying unseen exam-ples.In associationlearning,any association among features is sought,not justones that predict a particular classvalue.In clustering,groups ofexamples thatbelong together are sought.In numericprediction,the outcome to be predictedis not a discrete class but a numeric quantity.Regardless ofthe type oflearninginvolved,we call the thing to be learned the conceptand the output producedby a learning scheme the conceptdescription.Most ofthe examples in Chapter 1 are classiﬁcation problems.The weatherdata (Tables 1.2 and 1.3) presents a set ofdays together with a decision for eachas to whether to play the game or not.The problem is to learn how to classifynew days as play or don’t play.Given the contact lens data (Table 1.1),theproblem is to learn how to decide on a lens recommendation for a new patient—or more precisely,since every possible combination ofattributes is present inthe data,the problem is to learn a way ofsummarizing the given data.For theirises (Table 1.4),the problem is to learn how to decide whether a new iris ﬂoweris setosa,versicolor,or virginica,given its sepal length and width and petal lengthand width.For the labor negotiations data (Table 1.6),the problem is to decidewhether a new contract is acceptable or not,on the basis ofits duration;wage42CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 42increase in the ﬁrst,second,and third years;cost ofliving adjustment;and soforth.Classiﬁcation learning is sometimes called supervisedbecause,in a sense,themethod operates under supervision by being provided with the actual outcomefor each ofthe training examples—the play or don’t play judgment,the lens rec-ommendation,the type ofiris,the acceptability ofthe labor contract.Thisoutcome is called the classofthe example.The success ofclassiﬁcation learningcan be judged by trying out the concept description that is learned on an inde-pendent set oftest data for which the true classiﬁcations are known but notmade available to the machine.The success rate on test data gives an objectivemeasure ofhow well the concept has been learned.In many practical datamining applications,success is measured more subjectively in terms ofhowacceptable the learned description—such as the rules or the decision tree—areto a human user.Most ofthe examples in Chapter 1 can be used equally well for associationlearning,in which there is no speciﬁed class.Here,the problem is to discoverany structure in the data that is “interesting.”Some association rules for theweather data were given in Section 1.2.Association rules differ from classiﬁca-tion rules in two ways:they can “predict”any attribute,not just the class,andthey can predict more than one attribute’s value at a time.Because ofthis thereare far more association rules than classiﬁcation rules,and the challenge is toavoid being swamped by them.For this reason,association rules are oftenlimited to those that apply to a certain minimum number ofexamples—say80% ofthe dataset—and have greater than a certain minimum accuracy level—say 95% accurate.Even then,there are usually lots ofthem,and they have to beexamined manually to determine whether they are meaningful or not.Associ-ation rules usually involve only nonnumeric attributes:thus you wouldn’t nor-mally look for association rules in the iris dataset.When there is no speciﬁed class,clustering is used to group items that seemto fall naturally together.Imagine a version ofthe iris data in which the type ofiris is omitted,such as in Table 2.1.Then it is likely that the 150 instances fallinto natural clusters corresponding to the three iris types.The challenge is toﬁnd these clusters and assign the instances to them—and to be able to assignnew instances to the clusters as well.It may be that one or more ofthe iris typessplits naturally into subtypes,in which case the data will exhibit more than threenatural clusters.The success ofclustering is often measured subjectively in termsofhow useful the result appears to be to a human user.It may be followed by asecond step ofclassiﬁcation learning in which rules are learned that give anintelligible description ofhow new instances should be placed into the clusters.Numeric prediction is a variant ofclassiﬁcation learning in which theoutcome is a numeric value rather than a category.The CPU performanceproblem is one example.Another,shown in Table 2.2,is a version ofthe weather2.1WHAT’S A CONCEPT?43P088407-Ch002.qxd  4/30/05  11:10 AM  Page 4344CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESTable 2.1Iris data as a clustering problem.Sepal lengthSepal widthPetal lengthPetal width(cm)(cm)(cm)(cm)15.13.51.40.224.93.01.40.234.73.21.30.244.63.11.50.255.03.61.40.2...517.03.24.71.4526.43.24.51.5536.93.14.91.5545.52.34.01.3556.52.84.61.5...1016.33.36.02.51025.82.75.11.91037.13.05.92.11046.32.95.61.81056.53.05.82.2...Table 2.2Weather data with a numeric class.OutlookTemperatureHumidityWindyPlay time (min.)sunny8585false5sunny8090true0overcast8386false55rainy7096false40rainy6880false65rainy6570true45overcast6465true60sunny7295false0sunny6970false70rainy7580false45sunny7570true50overcast7290true55overcast8175false75rainy7191true10P088407-Ch002.qxd  4/30/05  11:10 AM  Page 44data in which what is to be predicted is not play or don’t play but rather is thetime (in minutes) to play.With numeric prediction problems,as with othermachine learning situations,the predicted value for new instances is often ofless interest than the structure ofthe description that is learned,expressed interms ofwhat the important attributes are and how they relate to the numericoutcome.2.2What’s in an example?The input to a machine learning scheme is a set ofinstances.These instancesare the things that are to be classiﬁed,associated,or clustered.Although until now we have called them examples,henceforth we will use the more spe-ciﬁc term instancesto refer to the input.Each instance is an individual,inde-pendent example ofthe concept to be learned.In addition,each one ischaracterized by the values ofa set ofpredetermined attributes.This was thecase in all the sample datasets described in the last chapter (the weather,contactlens,iris,and labor negotiations problems).Each dataset is represented as amatrix ofinstances versus attributes,which in database terms is a single rela-tion,or a ﬂatﬁle.Expressing the input data as a set ofindependent instances is by far the mostcommon situation for practical data mining.However,it is a rather restrictiveway offormulating problems,and it is worth spending some time reviewingwhy.Problems often involve relationships between objects rather than separate,independent instances.Suppose,to take a speciﬁc situation,a family tree isgiven,and we want to learn the concept sister.Imagine your own family tree,with your relatives (and their genders) placed at the nodes.This tree is the inputto the learning process,along with a list ofpairs ofpeople and an indication ofwhether they are sisters or not.Figure 2.1 shows part ofa family tree,below which are two tables that eachdeﬁne sisterhood in a slightly different way.A yesin the third column ofthetables means that the person in the second column is a sister ofthe person inthe ﬁrst column (that’s just an arbitrary decision we’ve made in setting up thisexample).The ﬁrst thing to notice is that there are a lot ofnos in the third column ofthe table on the left—because there are 12 people and 12 ¥12 =144 pairs ofpeople in all,and most pairs ofpeople aren’t sisters.The table on the right,whichgives the same information,records only the positive instances and assumes thatall others are negative.The idea ofspecifying only positive examples and adopt-ing a standing assumption that the rest are negative is called the closedworldassumption.It is frequently assumed in theoretical studies;however,it is not of2.2WHAT’S IN AN EXAMPLE?45P088407-Ch002.qxd  4/30/05  11:10 AM  Page 45much practical use in real-life problems because they rarely involve “closed”worlds in which you can be certain that all cases are covered.Neither table in Figure 2.1 is ofany use without the family tree itself.Thistree can also be expressed in the form ofa table,part ofwhich is shown in Table2.3.Now the problem is expressed in terms oftwo relationships.But these tablesdo not contain independent sets ofinstances because values in the Name,Parent1,and Parent2 columns ofthe sister-ofrelation refer to rows ofthe familytree relation.We can make them into a single set ofinstances by collapsing thetwo tables into the single one ofTable 2.4.We have at last succeeded in transforming the original relational probleminto the form ofinstances,each ofwhich is an individual,independent example46CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESfirstpersonsecondpersonPeterMPeggyF=GraceFRayM=PamFIanM=StevenMGrahamMPippaFBrianMAnnaFNikkiFPeterPeter...StevenStevenStevenSteven...lan...Anna...NikkiPeggySteven......PeterGrahamPamGrace......Pippa......Nikki.....Annasisterof?nonononoyesnoyesyesyesfirstpersonsecondpersonStevenGrahamlanBrianAnnaNikkiPamPamPippaPippaNikkiAnnasisterof?yesyesyesyesyesyesnoAll the restFigure2.1A family tree and two ways ofexpressing the sister-ofrelation.P088407-Ch002.qxd  4/30/05  11:10 AM  Page 46ofthe concept that is to be learned.Ofcourse,the instances are not really inde-pendent—there are plenty ofrelationships among different rows ofthe table!—but they are independent as far as the concept ofsisterhood is concerned.Mostmachine learning schemes will still have trouble dealing with this kind ofdata,as we will see in Section 3.6,but at least the problem has been recast into theright form.A simple rule for the sister-ofrelation is as follows:If second person’s gender =femaleand ﬁrst person’s parent1 =second person’s parent1then sister-of =yesThis example shows how you can take a relationship between different nodesofa tree and recast it into a set ofindependent instances.In database terms,youtake two relations and join them together to make one,a process ofﬂatteningthat is technically called denormalization.It is always possible to do this withany (ﬁnite) set of(ﬁnite) relations.The structure ofTable 2.4 can be used to describe any relationship betweentwo people—grandparenthood,second cousins twice removed,and so on.Rela-2.2WHAT’S IN AN EXAMPLE?47Table 2.3Family tree represented as a table.NameGenderParent1Parent2Petermale??Peggyfemale??StevenmalePeterPeggyGrahammalePeterPeggyPamfemalePeterPeggyIanmaleGraceRay...Table 2.4The sister-of relation represented in a table.First personSecond personNameGenderParent1Parent2NameGenderParent1Parent2Sister of?StevenmalePeterPeggyPamfemalePeterPeggyyesGrahammalePeterPeggyPamfemalePeterPeggyyesIanmaleGraceRayPippafemaleGraceRayyesBrianmaleGraceRayPippafemaleGraceRayyesAnnafemalePamIanNikkifemalePamIanyesNikkifemalePamIanAnnafemalePamIanyesall the restnoP088407-Ch002.qxd  4/30/05  11:10 AM  Page 47tionships among more people would require a larger table.Relationships in whichthe maximum number ofpeople is not speciﬁed in advance pose a more seriousproblem.Ifwe want to learn the concept ofnuclearfamily(parents and their chil-dren),the number ofpeople involved depends on the size ofthe largest nuclearfamily,and although we could guess at a reasonable maximum (10? 20?),theactual number could only be found by scanning the tree itself.Nevertheless,givena ﬁnite set ofﬁnite relations we could,at least in principle,form a new “superre-lation”that contained one row for everycombination ofpeople,and this wouldbe enough to express any relationship between people no matter how many wereinvolved.The computational and storage costs would,however,be prohibitive.Another problem with denormalization is that it produces apparent regular-ities in the data that are completely spurious and are in fact merely reﬂectionsofthe original database structure.For example,imagine a supermarket data-base with a relation for customers and the products they buy,one for productsand their supplier,and one for suppliers and their address.Denormalizing thiswill produce a ﬂat ﬁle that contains,for each instance,customer,product,sup-plier,and supplier address.A database mining tool that seeks structure in thedatabase may come up with the fact that customers who buy beer also buy chips,a discovery that could be signiﬁcant from the supermarket manager’s point ofview.However,it may also come up with the fact that supplier address can bepredicted exactly from supplier—a “discovery”that will not impress the super-market manager at all.This fact masquerades as a signiﬁcant discovery from theﬂat ﬁle but is present explicitly in the original database structure.Many abstract computational problems involve relations that are not ﬁnite,although clearly any actual set ofinput instances must be ﬁnite.Concepts suchas ancestor-ofinvolve arbitrarily long paths through a tree,and although thehuman race,and hence its family tree,may be ﬁnite (although prodigiously large),many artiﬁcial problems generate data that truly is inﬁnite.Although it maysound abstruse,this situation is the norm in areas such as list processing and logicprogramming and is addressed in a subdiscipline ofmachine learning calledinductivelogicprogramming.Computer scientists usually use recursion to dealwith situations in which the number ofpossible instances is inﬁnite.For example,If person1 is a parent of person2then person1 is an ancestor of person2If person1 is a parent of person2and person2 is an ancestor of person3then person1 is an ancestor of person3is a simple recursive deﬁnition ofancestorthat works no matter how distantlytwo people are related.Techniques ofinductive logic programming can learnrecursive rules such as these from a ﬁnite set ofinstances such as those in Table2.5.48CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 48The real drawbacks ofsuch techniques,however,are that they do not copewell with noisy data,and they tend to be so slow as to be unusable on anythingbut small artiﬁcial datasets.They are not covered in this book;see Bergadanoand Gunetti (1996) for a comprehensive treatment.In summary,the input to a data mining scheme is generally expressed as atable ofindependent instances ofthe concept to be learned.Because ofthis,ithas been suggested,disparagingly,that we should really talk ofﬁleminingratherthan databasemining.Relational data is more complex than a ﬂat ﬁle.A ﬁniteset ofﬁnite relations can always be recast into a single table,although often atenormous cost in space.Moreover,denormalization can generate spurious regularities in the data,and it is essential to check the data for such artifactsbefore applying a learning method.Finally,potentially inﬁnite concepts can bedealt with by learning rules that are recursive,although that is beyond the scopeofthis book.2.3What’s in an attribute?Each individual,independent instance that provides the input to machine learning is characterized by its values on a ﬁxed,predeﬁned set offeatures orattributes.The instances are the rows ofthe tables that we have shown for theweather,contact lens,iris,and CPU performance problems,and the attributesare the columns.(The labor negotiations data was an exception:we presentedthis with instances in columns and attributes in rows for space reasons.)The use ofa ﬁxed set offeatures imposes another restriction on the kinds ofproblems generally considered in practical data mining.What ifdifferent2.3WHAT’S IN AN ATTRIBUTE?49Table 2.5Another relation represented as a table.First personSecond personAncestorNameGenderParent1Parent2NameGenderParent1Parent2of?Petermale??StevenmalePeterPeggyyesPetermale??PamfemalePeterPeggyyesPetermale??AnnafemalePamIanyesPetermale??NikkifemalePamIanyesPamfemalePeterPeggyNikkifemalePamIanyesGracefemale??IanmaleGraceRayyesGracefemale??NikkifemalePamIanyesother examples hereyesall the restnoP088407-Ch002.qxd  4/30/05  11:10 AM  Page 49instances have different features? Ifthe instances were transportation vehicles,then number ofwheels is a feature that applies to many vehicles but not to ships,for example,whereas number ofmasts might be a feature that applies to shipsbut not to land vehicles.The standard workaround is to make each possiblefeature an attribute and to use a special “irrelevant value”ﬂag to indicate that aparticular attribute is not available for a particular case.A similar situation ariseswhen the existence ofone feature (say,spouse’s name) depends on the value ofanother (married or single).The value ofan attribute for a particular instance is a measurement ofthequantity to which the attribute refers.There is a broad distinction between quan-tities that are numericand ones that are nominal.Numeric attributes,sometimescalled continuousattributes,measure numbers—either real or integer valued.Note that the term continuousis routinely abused in this context:integer-valuedattributes are certainly not continuous in the mathematical sense.Nominalattributes take on values in a prespeciﬁed,ﬁnite set ofpossibilities and are some-times called categorical.But there are other possibilities.Statistics texts oftenintroduce “levels ofmeasurement”such as nominal,ordinal,interval,and ratio.Nominal quantities have values that are distinct symbols.The values them-selves serve just as labels or names—hence the term nominal,which comes fromthe Latin word for name.For example,in the weather data the attribute outlookhas values sunny,overcast,and rainy.No relation is implied among thesethree—no ordering or distance measure.It certainly does not make sense to addthe values together,multiply them,or even compare their size.A rule using suchan attribute can only test for equality or inequality,as follows:outlook: sunnyÆnoovercast ÆyesrainyÆyesOrdinal quantities are ones that make it possible to rank order the categories.However,although there is a notion ofordering,there is no notion ofdistance.For example,in the weather data the attribute temperaturehas values hot,mild,and cool.These are ordered.Whether you sayhot>mild>coolor hot<mild<coolis a matter ofconvention—it does not matter which is used as long as consis-tency is maintained.What is important is that mild lies between the other two.Although it makes sense to compare two values,it does not make sense to addor subtract them—the difference between hotand mildcannot be comparedwith the difference between mildand cool.A rule using such an attribute mightinvolve a comparison,as follows:temperature =hot Ænotemperature <hot Æyes50CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 50Notice that the distinction between nominal and ordinal quantities is notalways straightforward and obvious.Indeed,the very example ofan ordinalquantity that we used previously,outlook,is not completely clear:you mightargue that the three values dohave an ordering—overcastbeing somehow inter-mediate between sunnyand rainyas weather turns from good to bad.Interval quantities have values that are not only ordered but also measuredin ﬁxed and equal units.A good example is temperature,expressed in degrees(say,degrees Fahrenheit) rather than on the nonnumeric scale implied by cool,mild,and hot.It makes perfect sense to talk about the difference between twotemperatures,say 46 and 48 degrees,and compare that with the differencebetween another two temperatures,say 22 and 24 degrees.Another example isdates.You can talk about the difference between the years 1939 and 1945 (6years) or even the average ofthe years 1939 and 1945 (1942),but it doesn’t makemuch sense to consider the sum ofthe years 1939 and 1945 (3884) or threetimes the year 1939 (5817),because the starting point,year 0,is completely arbitrary—indeed,it has changed many times throughout the course ofhis-tory.(Children sometimes wonder what the year 300 was called in 300 .)Ratio quantities are ones for which the measurement method inherentlydeﬁnes a zero point.For example,when measuring the distance from one objectto others,the distance between the object and itselfforms a natural zero.Ratioquantities are treated as real numbers:any mathematical operations are allowed.It certainly does make sense to talk about three times the distance and even tomultiply one distance by another to get an area.However,the question ofwhether there is an “inherently”deﬁned zero pointdepends on our scientiﬁc knowledge—it’s culture relative.For example,DanielFahrenheit knew no lower limit to temperature,and his scale is an interval one.Nowadays,however,we view temperature as a ratio scale based on absolute zero.Measurement oftime in years since some culturally deﬁned zero such as 0is not a ratio scale;years since the big bang is.Even the zero point ofmoney—where we are usually quite happy to say that something cost twice as much assomething else—may not be quite clearly deﬁned for those ofus who constantlymax out our credit cards.Most practical data mining systems accommodate just two ofthese four levelsofmeasurement:nominal and ordinal.Nominal attributes are sometimes calledcategorical,enumerated,or discrete.Enumeratedis the standard term used incomputer science to denote a categorical data type;however,the strict deﬁni-tion ofthe term—namely,to put into one-to-one correspondence with thenatural numbers—implies an ordering,which is speciﬁcally not implied in themachine learning context.Discretealso has connotations ofordering becauseyou often discretize a continuous,numeric quantity.Ordinal attributes are generally called numeric,or perhaps continuous,but without the implication ofmathematical continuity.A special case ofthe nominal scale is the dichotomy,2.3WHAT’S IN AN ATTRIBUTE?51P088407-Ch002.qxd  4/30/05  11:10 AM  Page 51which has only two members—often designated as trueand false,or yesand noin the weather data.Such attributes are sometimes called Boolean.Machine learning systems can use a wide variety ofother information aboutattributes.For instance,dimensional considerations could be used to restrict thesearch to expressions or comparisons that are dimensionally correct.Circularordering could affect the kinds oftests that are considered.For example,in atemporal context,tests on a day attribute could involve next day,previous day,next weekday,and same day next week.Partial orderings,that is,generalizationor specialization relations,frequently occur in practical situations.Informationofthis kind is often referred to as metadata,data about data.However,the kindsofpractical methods used for data mining are rarely capable oftaking metadatainto account,although it is likely that these capabilities will develop rapidly inthe future.(We return to this in Chapter 8.)2.4Preparing the inputPreparing input for a data mining investigation usually consumes the bulk ofthe effort invested in the entire data mining process.Although this book is notreally about the problems ofdata preparation,we want to give you a feeling forthe issues involved so that you can appreciate the complexities.Following that,we look at a particular input ﬁle format,the attribute-relation ﬁle format (ARFFformat),that is used in the Java package described in Part II.Then we considerissues that arise when converting datasets to such a format,because there aresome simple practical points to be aware of.Bitter experience shows that realdata is often ofdisappointingly low in quality,and careful checking—a processthat has become known as datacleaning—pays offmany times over.Gathering the data togetherWhen beginning work on a data mining problem,it is ﬁrst necessary to bringall the data together into a set ofinstances.We explained the need to denor-malize relational data when describing the family tree example.Although itillustrates the basic issue,this self-contained and rather artiﬁcial example doesnot really convey a feeling for what the process will be like in practice.In a realbusiness application,it will be necessary to bring data together from differentdepartments.For example,in a marketing study data will be needed from thesales department,the customer billing department,and the customer servicedepartment.Integrating data from different sources usually presents many challenges—not deep issues ofprinciple but nasty realities ofpractice.Different departmentswill use different styles ofrecord keeping,different conventions,different timeperiods,different degrees ofdata aggregation,different primary keys,and willhave different kinds oferror.The data must be assembled,integrated,and52CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 52cleaned up.The idea ofcompany wide database integration is known as datawarehousing.Data warehouses provide a single consistent point ofaccess to cor-porate or organizational data,transcending departmental divisions.They arethe place where old data is published in a way that can be used to inform busi-ness decisions.The movement toward data warehousing is a recognition ofthefact that the fragmented information that an organization uses to support day-to-day operations at a departmental level can have immense strategic valuewhen brought together.Clearly,the presence ofa data warehouse is a very usefulprecursor to data mining,and ifit is not available,many ofthe steps involvedin data warehousing will have to be undertaken to prepare the data for mining.Often even a data warehouse will not contain all the necessary data,and youmay have to reach outside the organization to bring in data relevant to theproblem at hand.For example,weather data had to be obtained in the load forecasting example in the last chapter,and demographic data is needed formarketing and sales applications.Sometimes called overlaydata,this is not nor-mally collected by an organization but is clearly relevant to the data miningproblem.It,too,must be cleaned up and integrated with the other data that hasbeen collected.Another practical question when assembling the data is the degree ofaggre-gation that is appropriate.When a dairy farmer decides which cows to sell,themilk production records—which an automatic milking machine records twicea day—must be aggregated.Similarly,raw telephone call data is oflittle use whentelecommunications companies study their clients’behavior:the data must beaggregated to the customer level.But do you want usage by month or by quarter,and for how many months or quarters in arrears? Selecting the right type andlevel ofaggregation is usually critical for success.Because so many different issues are involved,you can’t expect to get it rightthe ﬁrst time.This is why data assembly,integration,cleaning,aggregating,andgeneral preparation take so long.ARFF formatWe now look at a standard way ofrepresenting datasets that consist ofinde-pendent,unordered instances and do not involve relationships among instances,called an ARFFﬁle.Figure 2.2 shows an ARFF ﬁle for the weather data in Table 1.3,the versionwith some numeric features.Lines beginning with a %sign are comments.Following the comments at the beginning ofthe ﬁle are the name ofthe rela-tion (weather) and a block deﬁning the attributes (outlook,temperature, humid-ity,windy,play?).Nominal attributes are followed by the set ofvalues they cantake on,enclosed in curly braces.Values can include spaces;ifso,they must beplaced within quotation marks.Numeric values are followed by the keywordnumeric.2.4PREPARING THE INPUT53P088407-Ch002.qxd  4/30/05  11:10 AM  Page 5354CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTES% ARFF file for the weather data with some numeric features%@relation weather@attribute outlook { sunny, overcast, rainy }@attribute temperature numeric@attribute humidity numeric@attribute windy { true, false }@attribute play? { yes, no }@data%% 14 instances%sunny, 85, 85, false, nosunny, 80, 90, true, noovercast, 83, 86, false, yesrainy, 70, 96, false, yesrainy, 68, 80, false, yesrainy, 65, 70, true, noovercast, 64, 65, true, yessunny, 72, 95, false, nosunny, 69, 70, false, yesrainy, 75, 80, false, yessunny, 75, 70, true, yesovercast, 72, 90, true, yesovercast, 81, 75, false, yesrainy, 71, 91, true, noFigure2.2ARFF ﬁle for the weather data.Although the weather problem is to predict the class value play?from the values ofthe other attributes,the class attribute is not dis-tinguished in any way in the data ﬁle.The ARFF format merely givesa dataset;it does not specify which ofthe attributes is the one thatis supposed to be predicted.This means that the same ﬁle can be usedfor investigating how well each attribute can be predicted from theothers,or to ﬁnd association rules,or for clustering.Following the attribute deﬁnitions is an @dataline that signals thestart ofthe instances in the dataset.Instances are written one per line,with values for each attribute in turn,separated by commas.Ifa valueis missing it is represented by a single question mark (there are noP088407-Ch002.qxd  4/30/05  11:10 AM  Page 54missing values in this dataset).The attribute speciﬁcations in ARFF ﬁles allowthe dataset to be checked to ensure that it contains legal values for all attributes,and programs that read ARFF ﬁles do this checking automatically.In addition to nominal and numeric attributes,exempliﬁed by the weatherdata,the ARFF format has two further attribute types:string attributes and dateattributes.String attributes have values that are textual.Suppose you have astring attribute that you want to call description.In the block deﬁning the attrib-utes,it is speciﬁed as follows:@attribute description stringThen,in the instance data,include any character string in quotation marks (toinclude quotation marks in your string,use the standard convention ofpre-ceding each one by a backslash,\).Strings are stored internally in a string tableand represented by their address in that table.Thus two strings that contain thesame characters will have the same value.String attributes can have values that are very long—even a whole document.To be able to use string attributes for text mining,it is necessary to be able tomanipulate them.For example,a string attribute might be converted into manynumeric attributes,one for each word in the string,whose value is the numberoftimes that word appears.These transformations are described in Section 7.3.Date attributes are strings with a special format and are introduced like this:@attribute today date(for an attribute called today).Weka,the machine learning software discussedin Part II ofthis book,uses the ISO-8601 combined date and time format yyyy-MM-dd-THH:mm:sswith four digits for the year,two each for the month andday,then the letter Tfollowed by the time with two digits for each ofhours,minutes,and seconds.1In the data section ofthe ﬁle,dates are speciﬁed as thecorresponding string representation ofthe date and time,for example,2004-04-03T12:00:00.Although they are speciﬁed as strings,dates are converted tonumeric form when the input ﬁle is read.Dates can also be converted internallyto different formats,so you can have absolute timestamps in the data ﬁle anduse transformations to forms such as time ofday or day ofthe week to detectperiodic behavior.Sparse dataSometimes most attributes have a value of0 for most the instances.For example,market basket data records purchases made by supermarket customers.No2.4PREPARING THE INPUT551Weka contains a mechanism for deﬁning a date attribute to have a different format byincluding a special string in the attribute deﬁnition.P088407-Ch002.qxd  4/30/05  11:10 AM  Page 55matter how big the shopping expedition,customers never purchase more thana tiny portion ofthe items a store offers.The market basket data contains thequantity ofeach item that the customer purchases,and this is zero for almostall items in stock.The data ﬁle can be viewed as a matrix whose rows andcolumns represent customers and stock items,and the matrix is “sparse”—nearly all its elements are zero.Another example occurs in text mining,in whichthe instances are documents.Here,the columns and rows represent documentsand words,and the numbers indicate how many times a particular word appearsin a particular document.Most documents have a rather small vocabulary,somost entries are zero.It can be impractical to represent each element ofa sparse matrix explicitly,writing each value in order,as follows:0, 26, 0, 0, 0, 0, 63, 0, 0, 0, “class A”0, 0, 0, 42, 0, 0, 0, 0, 0, 0, “class B”Instead,the nonzero attributes can be explicitly identiﬁed by attribute numberand their value stated:{1 26, 6 63, 10 “class A”}{3 42, 10 “class B”}Each instance is enclosed in curly braces and contains the index number ofeachnonzero attribute (indexes start from 0) and its value.Sparse data ﬁles have thesame @relationand @attributetags,followed by an @dataline,but the datasection is different and contains speciﬁcations in braces such as those shownpreviously.Note that the omitted values have a value of0—they are not“missing”values! Ifa value is unknown,it must be explicitly represented witha question mark.Attribute typesARFF ﬁles accommodate the two basic data types,nominal and numeric.Stringattributes and date attributes are effectively nominal and numeric,respectively,although before they are used strings are often converted into a numeric formsuch as a word vector.But how the two basic types are interpreted depends onthe learning method being used.For example,most methods treat numericattributes as ordinal scales and only use less-than and greater-than comparisonsbetween the values.However,some treat them as ratio scales and use distancecalculations.You need to understand how machine learning methods workbefore using them for data mining.Ifa learning method treats numeric attributes as though they are measuredon ratio scales,the question ofnormalization arises.Attributes are often nor-malized to lie in a ﬁxed range,say,from zero to one,by dividing all values bythe maximum value encountered or by subtracting the minimum value and56CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 56dividing by the range between the maximum and the minimum values.Anothernormalization technique is to calculate the statistical mean and standard deviation ofthe attribute values,subtract the mean from each value,and dividethe result by the standard deviation.This process is called standardizinga sta-tistical variable and results in a set ofvalues whose mean is zero and standarddeviation is one.Some learning methods—for example,varieties ofinstance-based learningand regression methods—deal only with ratio scales because they calculate the “distance”between two instances based on the values oftheir attributes.Ifthe actual scale is ordinal,a numeric distance function must be deﬁned.Oneway ofdoing this is to use a two-level distance:one ifthe two values are differ-ent and zero ifthey are the same.Any nominal quantity can be treated as numericby using this distance function.However,it is rather a crude technique and con-ceals the true degree ofvariation between instances.Another possibility is to gen-erate several synthetic binary attributes for each nominal attribute:we return tothis in Section 6.5 when we look at the use oftrees for numeric prediction.Sometimes there is a genuine mapping between nominal quantities andnumeric scales.For example,postal ZIP codes indicate areas that could be rep-resented by geographic coordinates;the leading digits oftelephone numbersmay do so,too,depending on where you live.The ﬁrst two digits ofa student’sidentiﬁcation number may be the year in which she ﬁrst enrolled.It is very common for practical datasets to contain nominal values that arecoded as integers.For example,an integer identiﬁer may be used as a code foran attribute such as partnumber,yet such integers are not intended for use inless-than or greater-than comparisons.Ifthis is the case,it is important tospecify that the attribute is nominal rather than numeric.It is quite possible to treat an ordinal quantity as though it were nominal.Indeed,some machine learning methods only deal with nominal elements.Forexample,in the contact lens problem the age attribute is treated as nominal,andthe rules generated included the following:If age =young and astigmatic =no and tear production rate =normal then recommendation =softIf age =pre-presbyopic and astigmatic =no and tear production rate =normal then recommendation =softBut in fact age,speciﬁed in this way,is really an ordinal quantity for which thefollowing is true:young <pre-presbyopic <presbyopicIfit were treated as ordinal,the two rules could be collapsed into one:If age £pre-presbyopic and astigmatic =no and tear production rate =normal then recommendation =soft2.4PREPARING THE INPUT57P088407-Ch002.qxd  4/30/05  11:10 AM  Page 57which is a more compact,and hence more satisfactory,way ofsaying the samething.Missing valuesMost datasets encountered in practice,such as the labor negotiations data inTable 1.6,contain missing values.Missing values are frequently indicated by out-of-range entries,perhaps a negative number (e.g.,-1) in a numeric ﬁeld that isnormally only positive or a 0 in a numeric ﬁeld that can never normally be 0.For nominal attributes,missing values may be indicated by blanks or dashes.Sometimes different kinds ofmissing values are distinguished (e.g.,unknownvs.unrecorded vs.irrelevant values) and perhaps represented by different negative integers (-1,-2,etc.).You have to think carefully about the signiﬁcance ofmissing values.They mayoccur for several reasons,such as malfunctioning measurement equipment,changes in experimental design during data collection,and collation ofseveralsimilar but not identical datasets.Respondents in a survey may refuse to answercertain questions such as age or income.In an archaeological study,a specimensuch as a skull may be damaged so that some variables cannot be measured.In a biologic one,plants or animals may die before all variables have been measured.What do these things meanabout the example under consideration?Might the skull damage have some signiﬁcance in itself,or is it just because ofsome random event? Does the plants’early death have some bearing on the caseor not?Most machine learning methods make the implicit assumption that there isno particular signiﬁcance in the fact that a certain instance has an attribute valuemissing:the value is simply not known.However,there may be a good reasonwhy the attribute’s value is unknown—perhaps a decision was made,on the evi-dence available,not to perform some particular test—and that might conveysome information about the instance other than the fact that the value is simplymissing.Ifthis is the case,then it would be more appropriate to record nottestedas another possible value for this attribute or perhaps as another attribute in thedataset.As the preceding examples illustrate,only someone familiar with the datacan make an informed judgment about whether a particular value being missinghas some extra signiﬁcance or whether it should simply be coded as an ordinarymissing value.Ofcourse,ifthere seem to be several types ofmissing value,thatis prima facie evidence that something is going on that needs to be investigated.Ifmissing values mean that an operator has decided not to make a particu-lar measurement,that may convey a great deal more than the mere fact that thevalue is unknown.For example,people analyzing medical databases havenoticed that cases may,in some circumstances,be diagnosable simply from thetests that a doctor decides to make regardless ofthe outcome ofthe tests.Then58CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 58a record ofwhich values are “missing”is all that is needed for a complete diagnosis—the actual values can be ignored completely!Inaccurate valuesIt is important to check data mining ﬁles carefully for rogue attributes andattribute values.The data used for mining has almost certainly not been gath-ered expressly for that purpose.When originally collected,many ofthe ﬁeldsprobably didn’t matter and were left blank or unchecked.Provided that it doesnot affect the original purpose ofthe data,there is no incentive to correct it.However,when the same database is used for mining,the errors and omissionssuddenly start to assume great signiﬁcance.For example,banks do not really needto know the age oftheir customers,so their databases may contain many missingor incorrect values.But age may be a very signiﬁcant feature in mined rules.Typographic errors in a dataset will obviously lead to incorrect values.Oftenthe value ofa nominal attribute is misspelled,creating an extra possible valuefor that attribute.Or perhaps it is not a misspelling but different names for thesame thing,such as Pepsi and Pepsi Cola.Obviously the point ofa deﬁnedformat such as ARFF is to allow data ﬁles to be checked for internal consistency.However,errors that occur in the original data ﬁle are often preserved throughthe conversion process into the ﬁle that is used for data mining;thus the list ofpossible values that each attribute takes on should be examined carefully.Typographic or measurement errors in numeric values generally cause out-liers that can be detected by graphing one variable at a time.Erroneous valuesoften deviate signiﬁcantly from the pattern that is apparent in the remainingvalues.Sometimes,however,inaccurate values are hard to ﬁnd,particularlywithout specialist domain knowledge.Duplicate data presents another source oferror.Most machine learning toolswill produce different results ifsome ofthe instances in the data ﬁles are dupli-cated,because repetition gives them more inﬂuence on the result.People often make deliberate errors when entering personal data into data-bases.They might make minor changes in the spelling oftheir street to try toidentify whether the information they have provided was sold to advertisingagencies that burden them with junk mail.They might adjust the spelling oftheir name when applying for insurance ifthey have had insurance refused inthe past.Rigid computerized data entry systems often impose restrictions thatrequire imaginative workarounds.One story tells ofa foreigner renting a vehiclein the United States.Being from abroad,he had no ZIP code,yet the computerinsisted on one;in desperation the operator suggested that he use the ZIP codeofthe rental agency.Ifthis is common practice,future data mining projects maynotice a cluster ofcustomers who apparently live in the same district as the agency!Similarly,a supermarket checkout operator sometimes uses his own frequent2.4PREPARING THE INPUT59P088407-Ch002.qxd  4/30/05  11:10 AM  Page 59buyer card when the customer does not supply one,either so that the customercan get a discount that would otherwise be unavailable or simply to accumulatecredit points in the cashier’s account.Only a deep semantic knowledge ofwhat isgoing on will be able to explain systematic data errors such as these.Finally,data goes stale.Many items change as circumstances change.Forexample,items in mailing lists—names,addresses,telephone numbers,and soon—change frequently.You need to consider whether the data you are miningis still current.Getting to know your dataThere is no substitute for getting to know your data.Simple tools that show his-tograms ofthe distribution ofvalues ofnominal attributes,and graphs ofthevalues ofnumeric attributes (perhaps sorted or simply graphed against instancenumber),are very helpful.These graphical visualizations ofthe data make iteasy to identify outliers,which may well represent errors in the data ﬁle—orarcane conventions for coding unusual situations,such as a missing year as 9999or a missing weight as -1kg,that no one has thought to tell you about.Domainexperts need to be consulted to explain anomalies,missing values,the signiﬁ-cance ofintegers that represent categories rather than numeric quantities,andso on.Pairwise plots ofone attribute against another,or each attribute againstthe class value,can be extremely revealing.Data cleaning is a time-consuming and labor-intensive procedure but onethat is absolutely necessary for successful data mining.With a large dataset,people often give up—how can they possibly check it all? Instead,you shouldsample a few instances and examine them carefully.You’ll be surprised at whatyou ﬁnd.Time looking at your data is always well spent.2.5Further readingPyle (1999) provides an extensive guide to data preparation for data mining.There is also a great deal ofcurrent interest in data warehousing and the prob-lems it entails.Kimball (1996) offers the best introduction to these that we knowof.Cabena et al.(1998) estimate that data preparation accounts for 60% oftheeffort involved in a data mining application,and they write at some length aboutthe problems involved.The area ofinductive logic programming,which deals with ﬁnite and inﬁ-nite relations,is covered by Bergadano and Gunetti (1996).The different “levelsofmeasurement”for attributes were introduced by Stevens (1946) and are welldescribed in the manuals for statistical packages such as SPSS (Nie et al.1970).60CHAPTER 2|INPUT:CONCEPTS,INSTANCES,AND ATTRIBUTESP088407-Ch002.qxd  4/30/05  11:10 AM  Page 60Most ofthe techniques in this book produce easily comprehensible descriptionsofthe structural patterns in the data.Before looking at how these techniqueswork,we have to see how structural patterns can be expressed.There are manydifferent ways for representing the patterns that can be discovered by machinelearning,and each one dictates the kind oftechnique that can be used to inferthat output structure from data.Once you understand how the output is represented,you have come a long way toward understanding how it can be generated.We saw many examples ofdata mining in Chapter 1.In these cases the outputtook the form ofdecision trees and classiﬁcation rules,which are basic knowl-edge representation styles that many machine learning methods use.Knowledgeis really too imposing a word for a decision tree or a collection ofrules,and byusing it we don’t really mean to imply that these structures vie with the realkindofknowledge that we carry in our heads:it’s just that we need some word torefer to the structures that learning methods produce.There are more complexvarieties ofrules that allow exceptions to be speciﬁed,and ones that can expresschapter3Output:Knowledge Representation61P088407-Ch003.qxd  4/30/05  11:09 AM  Page 61relations among the values ofthe attributes ofdifferent instances.Special formsoftrees can be used for numeric prediction,too.Instance-based representationsfocus on the instances themselves rather than rules that govern their attributevalues.Finally,some learning methods generate clusters ofinstances.These dif-ferent knowledge representation methods parallel the different kinds oflearn-ing problems introduced in Chapter 2.3.1Decision tablesThe simplest,most rudimentary way ofrepresenting the output from machinelearning is to make it just the same as the input—a decision table.For example,Table 1.2 is a decision table for the weather data:you just look up the appro-priate conditions to decide whether or not to play.Less trivially,creating a deci-sion table might involve selecting some ofthe attributes.Iftemperatureisirrelevant to the decision,for example,a smaller,condensed table with thatattribute missing would be a better guide.The problem is,ofcourse,to decidewhich attributes to leave out without affecting the ﬁnal decision.3.2Decision treesA “divide-and-conquer”approach to the problem oflearning from a set ofinde-pendent instances leads naturally to a style ofrepresentation called a decisiontree.We have seen some examples ofdecision trees,for the contact lens (Figure1.2) and labor negotiations (Figure 1.3) datasets.Nodes in a decision tree involvetesting a particular attribute.Usually,the test at a node compares an attributevalue with a constant.However,some trees compare two attributes with eachother,or use some function ofone or more attributes.Leafnodes give a classi-ﬁcation that applies to all instances that reach the leaf,or a set ofclassiﬁcations,or a probability distribution over all possible classiﬁcations.To classify anunknown instance,it is routed down the tree according to the values oftheattributes tested in successive nodes,and when a leafis reached the instance isclassiﬁed according to the class assigned to the leaf.Ifthe attribute that is tested at a node is a nominal one,the number ofchil-dren is usually the number ofpossible values ofthe attribute.In this case,because there is one branch for each possible value,the same attribute will notbe retested further down the tree.Sometimes the attribute values are dividedinto two subsets,and the tree branches just two ways depending on which subsetthe value lies in the tree;in that case,the attribute might be tested more thanonce in a path.Ifthe attribute is numeric,the test at a node usually determines whether itsvalue is greater or less than a predetermined constant,giving a two-way split.62CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONP088407-Ch003.qxd  4/30/05  11:09 AM  Page 62Alternatively,a three-way split may be used,in which case there are several dif-ferent possibilities.Ifmissing valueis treated as an attribute value in its ownright,that will create a third branch.An alternative for an integer-valued attrib-ute would be a three-way split into less than,equal to,and greater than.An alter-native for a real-valued attribute,for which equal tois not such a meaningfuloption,would be to test against an interval rather than a single constant,againgiving a three-way split:below,within,and above.A numeric attribute is oftentested several times in any given path down the tree from root to leaf,each testinvolving a different constant.We return to this when describing the handlingofnumeric attributes in Section 6.1.Missing values pose an obvious problem.It is not clear which branch shouldbe taken when a node tests an attribute whose value is missing.Sometimes,asdescribed in Section 2.4,missing valueis treated as an attribute value in its ownright.Ifthis is not the case,missing values should be treated in a special wayrather than being considered as just another possible value that the attributemight take.A simple solution is to record the number ofelements in the train-ing set that go down each branch and to use the most popular branch ifthevalue for a test instance is missing.A more sophisticated solution is to notionally split the instance into piecesand send part ofit down each branch and from there right on down to the leavesofthe subtrees involved.The split is accomplished using a numeric weightbetween zero and one,and the weight for a branch is chosen to be proportionalto the number oftraining instances going down that branch,all weightssumming to one.A weighted instance may be further split at a lower node.Even-tually,the various parts ofthe instance will each reach a leafnode,and the deci-sions at these leafnodes must be recombined using the weights that havepercolated down to the leaves.We return to this in Section 6.1.It is instructive and can even be entertaining to build a decision tree for adataset manually.To do so effectively,you need a good way ofvisualizing thedata so that you can decide which are likely to be the best attributes to test andwhat an appropriate test might be.The Weka Explorer,described in Part II,hasa User Classiﬁer facility that allows users to construct a decision tree interac-tively.It presents you with a scatter plot ofthe data against two selected attrib-utes,which you choose.When you ﬁnd a pair ofattributes that discriminatesthe classes well,you can create a two-way split by drawing a polygon around theappropriate data points on the scatter plot.For example,in Figure 3.1(a) the user is operating on a dataset with threeclasses,the iris dataset,and has found two attributes,petallengthand petalwidth,that do a good job ofsplitting up the classes.A rectangle has been drawn,man-ually,to separate out one ofthe classes (Iris versicolor).Then the user switchesto the decision tree view in Figure 3.1(b) to see the tree so far.The left-handleafnode contains predominantly irises ofone type (Iris versicolor,contami-3.2DECISION TREES63P088407-Ch003.qxd  4/30/05  11:09 AM  Page 6364CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATION(a)(b)Figure 3.1Constructing a decision tree interactively:(a) creating a rectangular testinvolving petallengthand petalwidthand (b) the resulting (unﬁnished) decision tree.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 64nated by only two virginicas);the right-hand one contains predominantly twotypes (Iris setosa and virginica,contaminated by only two versicolors).The userwill probably select the right-hand leafand work on it next,splitting it furtherwith another rectangle—perhaps based on a different pair ofattributes(although,from Figure 3.1[a],these two look pretty good).Section 10.2 explains how to use Weka’s User Classiﬁer facility.Most peopleenjoy making the ﬁrst few decisions but rapidly lose interest thereafter,and onevery useful option is to select a machine learning method and let it take over atany point in the decision tree.Manual construction ofdecision trees is a goodway to get a feel for the tedious business ofevaluating different combinationsofattributes to split on.3.3Classiﬁcation rulesClassiﬁcation rules are a popular alternative to decision trees,and we havealready seen examples for the weather (page 10),contact lens (page 13),iris(page 15),and soybean (page 18) datasets.The antecedent,or precondition,ofa rule is a series oftests just like the tests at nodes in decision trees,and the con-sequent,or conclusion,gives the class or classes that apply to instances coveredby that rule,or perhaps gives a probability distribution over the classes.Gener-ally,the preconditions are logically ANDed together,and all the tests mustsucceed ifthe rule is to ﬁre.However,in some rule formulations the precondi-tions are general logical expressions rather than simple conjunctions.We oftenthink ofthe individual rules as being effectively logically ORed together:ifanyone applies,the class (or probability distribution) given in its conclusion isapplied to the instance.However,conﬂicts arise when several rules with differ-ent conclusions apply;we will return to this shortly.It is easy to read a set ofrules directly offa decision tree.One rule is gener-ated for each leaf.The antecedent ofthe rule includes a condition for every nodeon the path from the root to that leaf,and the consequent ofthe rule is the class assigned by the leaf.This procedure produces rules that are unambigu-ous in that the order in which they are executed is irrelevant.However,ingeneral,rules that are read directly offa decision tree are far more complex thannecessary,and rules derived from trees are usually pruned to remove redundanttests.Because decision trees cannot easily express the disjunction implied amongthe different rules in a set,transforming a general set ofrules into a tree is notquite so straightforward.A good illustration ofthis occurs when the rules havethe same structure but different attributes,like:If a and b then xIf c and d then x3.3CLASSIFICATION RULES65P088407-Ch003.qxd  4/30/05  11:09 AM  Page 65Then it is necessary to break the symmetry and choose a single test for the rootnode.If,for example,ais chosen,the second rule must,in effect,be repeatedtwice in the tree,as shown in Figure 3.2.This is known as the replicated subtreeproblem.The replicated subtree problem is sufﬁciently important that it is worthlooking at a couple more examples.The diagram on the left ofFigure 3.3 showsan exclusive-orfunction for which the output is aifx=1or y=1but not both.To make this into a tree,you have to split on one attribute ﬁrst,leading to astructure like the one shown in the center.In contrast,rules can faithfully reﬂectthe true symmetry ofthe problem with respect to the attributes,as shown onthe right.66CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONabycnxycndynxyndynxynFigure 3.2Decision tree for a simple disjunction.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 66In this example the rules are not notably more compact than the tree.In fact,they are just what you would get by reading rules offthe tree in the obviousway.But in other situations,rules are much more compact than trees,particu-larly ifit is possible to have a “default”rule that covers cases not speciﬁed by theother rules.For example,to capture the effect ofthe rules in Figure 3.4—inwhich there are four attributes,x,y,z,and w,that can each be 1,2,or 3—requiresthe tree shown on the right.Each ofthe three small gray triangles to the upperright should actually contain the whole three-level subtree that is displayed ingray,a rather extreme example ofthe replicated subtree problem.This is a dis-tressingly complex description ofa rather simple concept.One reason why rules are popular is that each rule seems to represent an inde-pendent “nugget”ofknowledge.New rules can be added to an existing rule setwithout disturbing ones already there,whereas to add to a tree structure mayrequire reshaping the whole tree.However,this independence is something ofan illusion,because it ignores the question ofhow the rule set is executed.Weexplained earlier (on page 11) the fact that ifrules are meant to be interpretedin orderas a “decision list,”some ofthem,taken individually and out ofcontext,may be incorrect.On the other hand,ifthe order ofinterpretation is supposedto be immaterial,then it is not clear what to do when different rules lead to dif-ferent conclusions for the same instance.This situation cannot arise for rulesthat are read directly offa decision tree because the redundancy included in thestructure ofthe rules prevents any ambiguity in interpretation.But it does arisewhen rules are generated in other ways.Ifa rule set gives multiple classiﬁcations for a particular example,one solu-tion is to give no conclusion at all.Another is to count how often each rule ﬁreson the training data and go with the most popular one.These strategies can lead3.3CLASSIFICATION RULES67ab10ba01x = 1 ?y = 1 ?noy = 1 ?yesbnoayesanobyesIf x=1 and y=0 then class = a If x=0 and y=1 then class = a If x=0 and y=0 then class = b If x=1 and y=1 then class = b Figure 3.3The exclusive-or problem.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 67to radically different results.A different problem occurs when an instance isencountered that the rules fail to classify at all.Again,this cannot occur withdecision trees,or with rules read directly offthem,but it can easily happen withgeneral rule sets.One way ofdealing with this situation is to fail to classify suchan example;another is to choose the most frequently occurring class as a default.Again,radically different results may be obtained for these strategies.Individ-ual rules are simple,and sets ofrules seem deceptively simple—but given justa set ofrules with no additional information,it is not clear how it should beinterpreted.A particularly straightforward situation occurs when rules lead to a class thatis Boolean (say,yesand no) and when only rules leading to one outcome (say,yes) are expressed.The assumption is that ifa particular instance is not in class68CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONxy123a1z23w1b2b3a1b2b3If x=1 and y=1 then class = a If z=1 and w=1 then class = a Otherwise class = b Figure 3.4Decision tree with a replicated subtree.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 68yes,then it must be in class no—a form ofclosed world assumption.Ifthis isthe case,then rules cannot conﬂict and there is no ambiguity in rule interpre-tation:any interpretation strategy will give the same result.Such a set ofrulescan be written as a logic expression in what is called disjunctive normal form:that is,as a disjunction (OR) ofconjunctive (AND) conditions.It is this simple special case that seduces people into assuming rules are veryeasy to deal with,because here each rule really does operate as a new,inde-pendent piece ofinformation that contributes in a straightforward way to thedisjunction.Unfortunately,it only applies to Boolean outcomes and requires theclosed world assumption,and both these constraints are unrealistic in mostpractical situations.Machine learning algorithms that generate rules invariablyproduce ordered rule sets in multiclass situations,and this sacriﬁces any possi-bility ofmodularity because the order ofexecution is critical.3.4Association rulesAssociation rules are really no different from classiﬁcation rules except that theycan predict any attribute,not just the class,and this gives them the freedom topredict combinations ofattributes too.Also,association rules are not intendedto be used together as a set,as classiﬁcation rules are.Different association rulesexpress different regularities that underlie the dataset,and they generally predictdifferent things.Because so many different association rules can be derived from even a tinydataset,interest is restricted to those that apply to a reasonably large number ofinstances and have a reasonably high accuracy on the instances to which theyapply to.The coverageofan association rule is the number ofinstances for whichit predicts correctly—this is often called its support.Its accuracy—often calledconﬁdence—is the number ofinstances that it predicts correctly,expressed as aproportion ofall instances to which it applies.For example,with the rule:If temperature =cool then humidity =normalthe coverage is the number ofdays that are both cool and have normal humid-ity (4 days in the data ofTable 1.2),and the accuracy is the proportion ofcooldays that have normal humidity (100% in this case).It is usual to specifyminimum coverage and accuracy values and to seek only those rules whose cov-erage and accuracy are both at least these speciﬁed minima.In the weather data,for example,there are 58 rules whose coverage and accuracy are at least 2 and95%,respectively.(It may also be convenient to specify coverage as a percent-age ofthe total number ofinstances instead.)Association rules that predict multiple consequences must be interpretedrather carefully.For example,with the weather data in Table 1.2 we saw this rule:3.4ASSOCIATION RULES69P088407-Ch003.qxd  4/30/05  11:09 AM  Page 69If windy =false and play =no then outlook =sunnyand humidity =highThis is notjust a shorthand expression for the two separate rules:If windy =false and play =no then outlook =sunnyIf windy =false and play =no then humidity =highIt indeed implies that these exceed the minimum coverage and accuracyﬁgures—but it also implies more.The original rule means that the number ofexamples that are nonwindy,nonplaying,with sunny outlook and high humidity,is at least as great as the speciﬁed minimum coverage ﬁgure.It also means thatthe number ofsuch days,expressed as a proportion ofnonwindy,nonplaying days,is at least the speciﬁed minimum accuracy ﬁgure.This implies that the ruleIf humidity =high and windy =false and play =no then outlook =sunnyalso holds,because it has the same coverage as the original rule,and its accu-racy must be at least as high as the original rule’s because the number ofhigh-humidity,nonwindy,nonplaying days is necessarily less than that ofnonwindy,nonplaying days—which makes the accuracy greater.As we have seen,there are relationships between particular association rules:some rules imply others.To reduce the number ofrules that are produced,in cases where several rules are related it makes sense to present only thestrongest one to the user.In the preceding example,only the ﬁrst rule shouldbe printed.3.5Rules with exceptionsReturning to classiﬁcation rules,a natural extension is to allow them to haveexceptions.Then incremental modiﬁcations can be made to a rule set by express-ing exceptions to existing rules rather than reengineering the entire set.Forexample,consider the iris problem described earlier.Suppose a new ﬂower wasfound with the dimensions given in Table 3.1,and an expert declared it to bean instance ofIris setosa.Ifthis ﬂower was classiﬁed by the rules given in Chapter1 (pages 15–16) for this problem,it would be misclassiﬁed by two ofthem:70CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONTable 3.1A new iris ﬂower.Sepal length (cm)Sepal width (cm)Petal length (cm)Petal width (cm)Type5.13.52.60.2?P088407-Ch003.qxd  4/30/05  11:09 AM  Page 70If petal length ≥2.45 and petal length <4.45 then Iris versicolorIf petal length ≥2.45 and petal length <4.95 and petal width <1.55 then Iris versicolorThese rules require modiﬁcation so that the new instance can be treated correctly.However,simply changing the bounds for the attribute-value tests in these rules may not sufﬁce because the instances used to create therule set may then be misclassiﬁed.Fixing up a rule set is not as simple as itsounds.Instead ofchanging the tests in the existing rules,an expert might be con-sulted to explain why the new ﬂower violates them,receiving explanations thatcould be used to extend the relevant rules only.For example,the ﬁrst ofthesetwo rules misclassiﬁes the new Iris setosaas an instance ofthe genus Iris versi-color.Instead ofaltering the bounds on any ofthe inequalities in the rule,anexception can be made based on some other attribute:If petal length ≥2.45 and petal length <4.45 then Iris versicolor EXCEPTif petal width <1.0 then Iris setosaThis rule says that a ﬂower is Iris versicolorifits petal length is between 2.45cmand 4.45cm exceptwhen its petal width is less than 1.0cm,in which case it isIris setosa.Ofcourse,we might have exceptions to the exceptions,exceptions to these,and so on,giving the rule set something ofthe character ofa tree.As well as being used to make incremental changes to existing rule sets,rules withexceptions can be used to represent the entire concept description in the ﬁrstplace.Figure 3.5 shows a set ofrules that correctly classify all examples in the Irisdataset given earlier (pages 15–16).These rules are quite difﬁcult to compre-hend at ﬁrst.Let’s follow them through.A default outcome has been chosen,Irissetosa,and is shown in the ﬁrst line.For this dataset,the choice ofdefault israther arbitrary because there are 50 examples ofeach type.Normally,the mostfrequent outcome is chosen as the default.Subsequent rules give exceptions to this default.The ﬁrst if...then,on lines2 through 4,gives a condition that leads to the classiﬁcation Iris versicolor.However,there are two exceptions to this rule (lines 5 through 8),which we willdeal with in a moment.Ifthe conditions on lines 2 and 3 fail,the elseclause online 9 is reached,which essentially speciﬁes a second exception to the originaldefault.Ifthe condition on line 9 holds,the classiﬁcation is Iris virginica(line10).Again,there is an exception to this rule (on lines 11 and 12).Now return to the exception on lines 5 through 8.This overrides the Iris ver-sicolorconclusion on line 4 ifeither ofthe tests on lines 5 and 7 holds.As ithappens,these two exceptions both lead to the same conclusion,Iris virginica3.5RULES WITH EXCEPTIONS71P088407-Ch003.qxd  4/30/05  11:09 AM  Page 71(lines 6 and 8).The ﬁnal exception is the one on lines 11 and 12,which over-rides the Iris virginicaconclusion on line 10 when the condition on line 11 ismet,and leads to the classiﬁcation Iris versicolor.You will probably need to ponder these rules for some minutes before itbecomes clear how they are intended to be read.Although it takes some time to get used to reading them,sorting out the excepts and if...then...elsesbecomes easier with familiarity.People often think ofreal problems interms ofrules,exceptions,and exceptions to the exceptions,so it is often a goodway to express a complex rule set.But the main point in favor ofthis way ofrepresenting rules is that it scales up well.Although the whole rule set is a littlehard to comprehend,each individual conclusion,each individual thenstate-ment,can be considered just in the context ofthe rules and exceptions that leadto it;whereas with decision lists,all prior rules need to be reviewed to deter-mine the precise effect ofan individual rule.This locality property is crucialwhen trying to understand large rule sets.Psychologically,people familiar withthe data think ofa particular set ofcases,or kind ofcase,when looking at anyone conclusion in the exception structure,and when one ofthese cases turnsout to be an exception to the conclusion,it is easy to add an exceptclause tocater for it.It is worth pointing out that the default...except if...then...structure islogically equivalent to if...then...else...,where the elseis unconditional andspeciﬁes exactly what the default did.An unconditional elseis,ofcourse,adefault.(Note that there are no unconditional elses in the preceding rules.) Log-72CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONDefault: Iris-setosa1except if petal-length ≥ 2.45 and petal-length < 5.355  2          and petal-width < 1.75 3       then Iris-versicolor 4            except if petal-length ≥ 4.95 and petal-width < 1.55 5                   then Iris-virginica 6                   else if sepal-length < 4.95 and sepal-width ≥ 2.45 7                        then Iris-virginica 8       else if petal-length ≥ 3.35 9            then Iris-virginica 10                 except if petal-length < 4.85 and sepal-length < 5.95 11                        then Iris-versicolor 12Figure 3.5Rules for the Iris data.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 72ically,the exception-based rules can very simply be rewritten in terms ofregularif...then...elseclauses.What is gained by the formulation in terms ofexcep-tions is not logicalbut psychological.We assume that the defaults and the teststhat occur early apply more widely than the exceptions further down.Ifthis isindeed true for the domain,and the user can see that it is plausible,the expres-sion in terms of(common) rules and (rare) exceptions will be easier to graspthan a different,but logically equivalent,structure.3.6Rules involving relationsWe have assumed implicitly that the conditions in rules involve testing anattribute value against a constant.Such rules are called propositionalbecause theattribute-value language used to deﬁne them has the same power as what logi-cians call the propositional calculus.In many classiﬁcation tasks,propositionalrules are sufﬁciently expressive for concise,accurate concept descriptions.Theweather,contact lens recommendation,iris type,and acceptability oflabor con-tract datasets mentioned previously,for example,are well described by propo-sitional rules.However,there are situations in which a more expressive form ofrule would provide a more intuitive and concise concept description,and theseare situations that involve relationships between examples such as those encoun-tered in Section 2.2.Suppose,to take a concrete example,we have the set ofeight building blocksofthe various shapes and sizes illustrated in Figure 3.6,and we wish to learnthe concept ofstanding.This is a classic two-class problem with classes stand-ingand lying.The four shaded blocks are positive (standing)examples oftheconcept,and the unshaded blocks are negative (lying)examples.The only infor-3.6RULES INVOLVING RELATIONS73Shaded:Unshaded: standinglyingFigure 3.6The shapes problem.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 73mation the learning algorithm will be given is the width,height,and number ofsidesofeach block.The training data is shown in Table 3.2.A propositional rule set that might be produced for this data is:if width ≥3.5 and height <7.0 then lyingif height ≥3.5 then standingIn case you’re wondering,3.5 is chosen as the breakpoint for widthbecause it ishalfway between the width ofthe thinnest lying block,namely 4,and the widthofthe fattest standing block whose height is less than 7,namely 3.Also,7.0 ischosen as the breakpoint for heightbecause it is halfway between the height ofthe tallest lying block,namely 6,and the shortest standing block whose widthis greater than 3.5,namely 8.It is common to place numeric thresholds halfwaybetween the values that delimit the boundaries ofa concept.Although these two rules work well on the examples given,they are not verygood.Many new blocks would not be classiﬁed by either rule (e.g.,one withwidth 1 and height 2),and it is easy to devise many legitimate blocks that therules would not ﬁt.A person classifying the eight blocks would probably notice that “standing blocks are those that are taller than they are wide.”This rule does not compare attribute values with constants,it compares attributes with eachother:if width >height then lyingif height >width then standingThe actual values ofthe heightand widthattributes are not important;just theresult ofcomparing the two.Rules ofthis form are called relational,becausethey express relationships between attributes,rather than propositional,whichdenotes a fact about just one attribute.74CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONTable 3.2Training data for the shapes problem.WidthHeightSidesClass244standing364standing434lying783standing763lying294standing914lying1023lyingP088407-Ch003.qxd  4/30/05  11:09 AM  Page 74Standard relations include equality (and inequality) for nominal attributesand less than and greater than for numeric ones.Although relational nodescould be put into decision trees just as relational conditions can be put intorules,schemes that accommodate relations generally use the rule rather than thetree representation.However,most machine learning methods do not considerrelational rules because there is a considerable cost in doing so.One way ofallowing a propositional method to make use ofrelations is to add extra,sec-ondary attributes that say whether two primary attributes are equal or not,orgive the difference between them ifthey are numeric.For example,we mightadd a binary attribute is width <height?to Table 3.2.Such attributes are oftenadded as part ofthe data engineering process.With a seemingly rather small further enhancement,the expressive power ofthe relational knowledge representation can be extended very greatly.The trick is to express rules in a way that makes the role ofthe instanceexplicit:if width(block) >height(block) then lying(block)if height(block) >width(block) then standing(block)Although this does not seem like much ofan extension,it is ifinstances canbe decomposed into parts.For example,ifa toweris a pile ofblocks,one on topofthe other,then the fact that the topmost block ofthe tower is standing canbe expressed by:if height(tower.top) >width(tower.top) then standing(tower.top)Here,tower.topis used to refer to the topmost block.So far,nothing has beengained.But iftower.restrefers to the rest ofthe tower,then the fact that the toweris composed entirelyofstanding blocks can be expressed by the rules:if height(tower.top) >width(tower.top) and standing(tower.rest)then standing(tower)The apparently minor addition ofthe condition standing(tower.rest)is a recur-sive expression that will turn out to be true only ifthe rest ofthe tower is com-posed ofstanding blocks.A recursive application ofthe same rule will test this.Ofcourse,it is necessary to ensure that the recursion “bottoms out”properlyby adding a further rule,such as:if tower =empty then standing(tower.top)With this addition,relational rules can express concepts that cannot possibly beexpressed propositionally,because the recursion can take place over arbitrarilylong lists ofobjects.Sets ofrules such as this are called logic programs,and thisarea ofmachine learning is called inductive logic programming.We will not betreating it further in this book.3.6RULES INVOLVING RELATIONS75P088407-Ch003.qxd  4/30/05  11:09 AM  Page 7576CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATION3.7Trees for numeric predictionThe kind ofdecision trees and rules that we have been looking at are designedfor predicting categories rather than numeric quantities.When it comes to pre-dicting numeric quantities,as with the CPU performance data in Table 1.5,thesame kind oftree or rule representation can be used,but the leafnodes ofthetree,or the right-hand side ofthe rules,would contain a numeric value that isthe average ofall the training set values to which the leaf,or rule,applies.Because statisticians use the term regressionfor the process ofcomputing anexpression that predicts a numeric quantity,decision trees with averagednumeric values at the leaves are called regression trees.Figure 3.7(a) shows a regression equation for the CPU performance data,andFigure 3.7(b) shows a regression tree.The leaves ofthe tree are numbers thatrepresent the average outcome for instances that reach the leaf.The tree is muchlarger and more complex than the regression equation,and ifwe calculate theaverage ofthe absolute values ofthe errors between the predicted and the actualCPU performance measures,it turns out to be signiﬁcantly less for the tree thanfor the regression equation.The regression tree is more accurate because asimple linear model poorly represents the data in this problem.However,thetree is cumbersome and difﬁcult to interpret because ofits large size.It is possible to combine regression equations with regression trees.Figure3.7(c) is a tree whose leaves contain linear expressions—that is,regression equa-tions—rather than single predicted values.This is (slightly confusingly) calleda model tree.Figure 3.7(c) contains the six linear models that belong at the sixleaves,labeled LM1 through LM6.The model tree approximates continuousfunctions by linear “patches,”a more sophisticated representation than eitherlinear regression or regression trees.Although the model tree is smaller andmore comprehensible than the regression tree,the average error values on thetraining data are lower.(However,we will see in Chapter 5 that calculating theaverage error on the training set is not in general a good way ofassessing the performance ofmodels.)3.8Instance-based representationThe simplest form oflearning is plain memorization,or rote learning.Once aset oftraining instances has been memorized,on encountering a new instancethe memory is searched for the training instance that most strongly resemblesthe new one.The only problem is how to interpret “resembles”:we will explainthat shortly.First,however,note that this is a completely different way ofrep-resenting the “knowledge”extracted from a set ofinstances:just store theinstances themselves and operate by relating new instances whose class isP088407-Ch003.qxd  4/30/05  11:09 AM  Page 7664.6 (24/19.2%)CHMIN≤ 7.5 > 7.5 MMAX≤ 8.5 (8.5,28] MMAX> 28 19.3 (28/8.7%)≤ 2500 29.8(37/8.18%)(2500,4250] CACH> 4250 MYCT≤ 0.5 59.3(24/16.9%)(0.5,8.5] ≤ 550 37.3(19/11.3%)18.3(7/3.83%)> 550 75.7(10/24.6%)≤ 10000 133(16/28.8%)> 10000 157(21/73.7%)≤ 28000 > 28000 MMIN≤ 58 783 (5/359%)> 58 281(11/56%)≤12000 492(7/53.9%)> 12000 CACHMMAXCHMAX(b)CHMIN≤ 7.5 > 7.5 ≤ 8.5 LM4(50/22.1%)> 8.5 LM1(65/7.32%)≤ 4250 > 4250 ≤ 0.5 LM2(26/6.37%)LM3(24/14.5%)(0.5,8.5] LM5(21/45.5%)≤ 28000 LM6(23/63.5%)> 28000 MMAXCACHMMAXCACH(c)LM1 PRP=8.29+0.004 MMAX+2.77 CHMINLM2 PRP=20.3+0.004 MMIN-3.99 CHMIN        +0.946 CHMAXLM3 PRP=38.1+0.012 MMINLM4 PRP=19.5+0.002 MMAX+0.698 CACH        +0.969 CHMAXLM5 PRP=285-1.46 MYCT+1.02 CACH        -9.39 CHMINLM6 PRP=-65.8+0.03 MMIN-2.94 CHMIN        +4.98 CHMAXPRP =-56.1+0.049 MYCT+0.015 MMIN+0.006 MMAX+0.630 CACH-0.270 CHMIN+1.46 CHMAX(a)Figure 3.7Models for the CPU performance data:(a) linear regression,(b) regressiontree,and (c) model tree.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 77unknown to existing ones whose class is known.Instead oftrying to create rules,work directly from the examples themselves.This is known as instance-basedlearning.In a sense all the other learning methods are “instance-based,”too,because we always start with a set ofinstances as the initial training informa-tion.But the instance-based knowledge representation uses the instances them-selves to represent what is learned,rather than inferring a rule set or decisiontree and storing it instead.In instance-based learning,all the real work is done when the time comes toclassify a new instance rather than when the training set is processed.In a sense,then,the difference between this method and the others we have seen is the timeat which the “learning”takes place.Instance-based learning is lazy,deferring thereal work as long as possible,whereas other methods are eager,producing a gen-eralization as soon as the data has been seen.In instance-based learning,eachnew instance is compared with existing ones using a distance metric,and theclosest existing instance is used to assign the class to the new one.This is calledthe nearest-neighborclassiﬁcation method.Sometimes more than one nearestneighbor is used,and the majority class ofthe closest kneighbors (or the dis-tance-weighted average,ifthe class is numeric) is assigned to the new instance.This is termed the k-nearest-neighbormethod.Computing the distance between two examples is trivial when examples havejust one numeric attribute:it is just the difference between the two attributevalues.It is almost as straightforward when there are several numeric attributes:generally,the standard Euclidean distance is used.However,this assumes thatthe attributes are normalized and are ofequal importance,and one ofthe mainproblems in learning is to determine which are the important features.When nominal attributes are present,it is necessary to come up with a “dis-tance”between different values ofthat attribute.What are the distances between,say,the values red,green,and blue?Usually a distance ofzero is assigned ifthevalues are identical;otherwise,the distance is one.Thus the distance betweenredand redis zero but that between redand greenis one.However,it may bedesirable to use a more sophisticated representation ofthe attributes.Forexample,with more colors one could use a numeric measure ofhue in colorspace,making yellow closer to orangethan it is to greenand ochercloser still.Some attributes will be more important than others,and this is usuallyreﬂected in the distance metric by some kind ofattribute weighting.Derivingsuitable attribute weights from the training set is a key problem in instance-based learning.It may not be necessary,or desirable,to store allthe training instances.Forone thing,this may make the nearest-neighbor calculation unbearably slow.Foranother,it may consume unrealistic amounts ofstorage.Generally,some regionsofattribute space are more stable than others with regard to class,and just a78CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONP088407-Ch003.qxd  4/30/05  11:09 AM  Page 78few exemplars are needed inside stable regions.For example,you might expectthe required density ofexemplars that lie well inside class boundaries to bemuch less than the density that is needed near class boundaries.Deciding whichinstances to save and which to discard is another key problem in instance-basedlearning.An apparent drawback to instance-based representations is that they do notmake explicit the structures that are learned.In a sense this violates the notionof“learning”that we presented at the beginning ofthis book;instances do notreally “describe”the patterns in data.However,the instances combine with thedistance metric to carve out boundaries in instance space that distinguish oneclass from another,and this is a kind ofexplicit representation ofknowledge.For example,given a single instance ofeach oftwo classes,the nearest-neigh-bor rule effectively splits the instance space along the perpendicular bisector ofthe line joining the instances.Given several instances ofeach class,the space isdivided by a set oflines that represent the perpendicular bisectors ofselectedlines joining an instance ofone class to one ofanother class.Figure 3.8(a) illus-trates a nine-sided polygon that separates the ﬁlled-circle class from the open-circle class.This polygon is implicit in the operation ofthe nearest-neighborrule.When training instances are discarded,the result is to save just a few proto-typical examples ofeach class.Figure 3.8(b) shows as dark circles only theexamples that actually get used in nearest-neighbor decisions:the others (thelight gray ones) can be discarded without affecting the result.These prototypi-cal examples serve as a kind ofexplicit knowledge representation.Some instance-based representations go further and explicitly generalize theinstances.Typically,this is accomplished by creating rectangular regions thatenclose examples ofthe same class.Figure 3.8(c) shows the rectangular regionsthat might be produced.Unknown examples that fall within one ofthe rectan-gles will be assigned the corresponding class;ones that fall outside all rectan-gles will be subject to the usual nearest-neighbor rule.Ofcourse this produces3.8INSTANCE-BASED REPRESENTATION79(a)(b)(c)(d)Figure 3.8Different ways ofpartitioning the instance space.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 79different decision boundaries from the straightforward nearest-neighbor rule,as can be seen by superimposing the polygon in Figure 3.8(a) onto the rectan-gles.Any part ofthe polygon that lies within a rectangle will be chopped offandreplaced by the rectangle’s boundary.Rectangular generalizations in instance space are just like rules with a specialform ofcondition,one that tests a numeric variable against an upper and lowerbound and selects the region in between.Different dimensions ofthe rectanglecorrespond to tests on different attributes being ANDed together.Choosingsnugly ﬁtting rectangular regions as tests leads to much more conservative rulesthan those generally produced by rule-based machine learning methods,because for each boundary ofthe region,there is an actual instance that lies on(or just inside) that boundary.Tests such as x<a(where xis an attribute valueand ais a constant) encompass an entire half-space—they apply no matter howsmall xis as long as it is less than a.When doing rectangular generalization ininstance space you can afford to be conservative because ifa new example isencountered that lies outside all regions,you can fall back on the nearest-neigh-bor metric.With rule-based methods the example cannot be classiﬁed,orreceives just a default classiﬁcation,ifno rules apply to it.The advantage ofmoreconservative rules is that,although incomplete,they may be more perspicuousthan a complete set ofrules that covers all cases.Finally,ensuring that theregions do not overlap is tantamount to ensuring that at most one rule can applyto an example,eliminating another ofthe difﬁculties ofrule-based systems—what to do when several rules apply.A more complex kind ofgeneralization is to permit rectangular regions tonest one within another.Then a region that is basically all one class can containan inner region ofa different class,as illustrated in Figure 3.8(d).It is possibleto allow nesting within nesting so that the inner region can itselfcontain its owninner region ofa different class—perhaps the original class ofthe outer region.This is analogous to allowing rules to have exceptions and exceptions to theexceptions,as in Section 3.5.It is worth pointing out a slight danger to the technique ofvisualizinginstance-based learning in terms ofboundaries in example space:it makes theimplicit assumption that attributes are numeric rather than nominal.Ifthevarious values that a nominal attribute can take on were laid out along a line,generalizations involving a segment ofthat line would make no sense:eachtest involves either one value for the attribute or all values for it (or perhaps anarbitrary subset ofvalues).Although you can more or less easily imagine extend-ing the examples in Figure 3.8 to several dimensions,it is much harder toimagine how rules involving nominal attributes will look in multidimensionalinstance space.Many machine learning situations involve numerous attributes,and our intuitions tend to lead us astray when extended to high-dimensionalspaces.80CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONP088407-Ch003.qxd  4/30/05  11:09 AM  Page 803.9ClustersWhen clusters rather than a classiﬁer is learned,the output takes the form ofadiagram that shows how the instances fall into clusters.In the simplest case thisinvolves associating a cluster number with each instance,which might bedepicted by laying the instances out in two dimensions and partitioning thespace to show each cluster,as illustrated in Figure 3.9(a).Some clustering algorithms allow one instance to belong to more than onecluster,so the diagram might lay the instances out in two dimensions and drawoverlapping subsets representing each cluster—a Venn diagram.Some algo-rithms associate instances with clusters probabilistically rather than categori-cally.In this case,for every instance there is a probability or degree ofmembership with which it belongs to each ofthe clusters.This is shown inFigure 3.9(c).This particular association is meant to be a probabilistic one,sothe numbers for each example sum to one—although that is not always the case.Other algorithms produce a hierarchical structure ofclusters so that at the top level the instance space divides into just a few clusters,each ofwhichdivides into its own subclusters at the next level down,and so on.In this case adiagram such as the one in Figure 3.9(d) is used,in which elements joinedtogether at lower levels are more tightly clustered than ones joined together at3.9CLUSTERS81abkdhgjfcie(a)abkdhgjfcie(b)abcdefgh0.10.80.30.10.20.40.20.410.50.10.40.80.40.50.10.10.40.10.30.10.40.10.70.523(c)gaciedkbjfh(d)Figure 3.9Different ways ofrepresenting clusters.P088407-Ch003.qxd  4/30/05  11:09 AM  Page 81higher levels.Diagrams such as this are called dendrograms.This term meansjust the same thing as tree diagrams(the Greek word dendronmeans “a tree”),but in clustering the more exotic version seems to be preferred—perhapsbecause biologic species are a prime application area for clustering techniques,and ancient languages are often used for naming in biology.Clustering is often followed by a stage in which a decision tree or rule set isinferred that allocates each instance to the cluster in which it belongs.Then,theclustering operation is just one step on the way to a structural description.3.10Further readingKnowledge representation is a key topic in classical artiﬁcial intelligence and iswell represented by a comprehensive series ofpapers edited by Brachman andLevesque (1985).However,these are about ways ofrepresenting handcrafted,not learned knowledge,and the kind ofrepresentations that can be learned fromexamples are quite rudimentary in comparison.In particular,the shortcomingsofpropositional rules,which in logic are referred to as the propositional calcu-lus,and the extra expressive power ofrelational rules,or the predicate calculus,are well described in introductions to logic such as that in Chapter 2 ofthe bookby Genesereth and Nilsson (1987).We mentioned the problem ofdealing with conﬂict among different rules.Various ways ofdoing this,called conﬂict resolution strategies,have been devel-oped for use with rule-based programming systems.These are described inbooks on rule-based programming,such as that by Brownstown et al.(1985).Again,however,they are designed for use with handcrafted rule sets rather thanones that have been learned.The use ofhand-crafted rules with exceptions fora large dataset has been studied by Gaines and Compton (1995),and Richardsand Compton (1998) describe their role as an alternative to classic knowledgeengineering.Further information on the various styles ofconcept representation can befound in the papers that describe machine learning methods ofinferring con-cepts from examples,and these are covered in the Further readingsection ofChapter 4 and the Discussionsections ofChapter 6.82CHAPTER 3|OUTPUT:KNOWLEDGE REPRESENTATIONP088407-Ch003.qxd  4/30/05  11:09 AM  Page 82Now that we’ve seen how the inputs and outputs can be represented,it’s timeto look at the learning algorithms themselves.This chapter explains the basicideas behind the techniques that are used in practical data mining.We will notdelve too deeply into the trickier issues—advanced versions ofthe algorithms,optimizations that are possible,complications that arise in practice.These topicsare deferred to Chapter 6,where we come to grips with real implementationsofmachine learning methods such as the ones included in data mining toolkitsand used for real-world applications.It is important to understand these moreadvanced issues so that you know what is really going on when you analyze aparticular dataset.In this chapter we look at the basic ideas.One ofthe most instructive lessonsis that simple ideas often work very well,and we strongly recommend the adop-tion ofa “simplicity-ﬁrst”methodology when analyzing practical datasets.Thereare many different kinds ofsimple structure that datasets can exhibit.In onedataset,there might be a single attribute that does all the work and the othersmay be irrelevant or redundant.In another dataset,the attributes might chapter4Algorithms:The Basic Methods83P088407-Ch004.qxd  4/30/05  11:13 AM  Page 83contribute independently and equally to the ﬁnal outcome.A third might havea simple logical structure,involving just a few attributes that can be capturedby a decision tree.In a fourth,there may be a few independent rules that governthe assignment ofinstances to different classes.A ﬁfth might exhibit depend-encies among different subsets ofattributes.A sixth might involve lineardependence among numeric attributes,where what matters is a weighted sumofattribute values with appropriately chosen weights.In a seventh,classiﬁca-tions appropriate to particular regions ofinstance space might be governed bythe distances between the instances themselves.And in an eighth,it might bethat no class values are provided:the learning is unsupervised.In the inﬁnite variety ofpossible datasets there are many different kinds ofstructure that can occur,and a data mining tool—no matter how capable—thatis looking for one class ofstructure may completely miss regularities ofa dif-ferent kind,regardless ofhow rudimentary those may be.The result is a baroqueand opaque classiﬁcation structure ofone kind instead ofa simple,elegant,immediately comprehensible structure ofanother.Each ofthe eight examples ofdifferent kinds ofdatasets sketched previouslyleads to a different machine learning method well suited to discovering it.Thesections ofthis chapter look at each ofthese structures in turn.4.1Inferring rudimentary rulesHere’s an easy way to ﬁnd very simple classiﬁcation rules from a set ofinstances.Called 1Rfor 1-rule,it generates a one-level decision tree expressed in the formofa set ofrules that all test one particular attribute.1R is a simple,cheap methodthat often comes up with quite good rules for characterizing the structure indata.It turns out that simple rules frequently achieve surprisingly high accu-racy.Perhaps this is because the structure underlying many real-world datasetsis quite rudimentary,and just one attribute is sufﬁcient to determine the classofan instance quite accurately.In any event,it is always a good plan to try thesimplest things ﬁrst.The idea is this:we make rules that test a single attribute and branch accord-ingly.Each branch corresponds to a different value ofthe attribute.It is obviouswhat is the best classiﬁcation to give each branch:use the class that occurs mostoften in the training data.Then the error rate ofthe rules can easily be deter-mined.Just count the errors that occur on the training data,that is,the numberofinstances that do not have the majority class.Each attribute generates a different set ofrules,one rule for every value ofthe attribute.Evaluate the error rate for each attribute’s rule set and choosethe best.It’s that simple! Figure 4.1 shows the algorithm in the form ofpseudocode.84CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 84To see the 1R method at work,consider the weather data ofTable 1.2 (we willencounter it many times again when looking at how learning algorithms work).To classify on the ﬁnal column,play,1R considers four sets ofrules,one for eachattribute.These rules are shown in Table 4.1.An asterisk indicates that a randomchoice has been made between two equally likely outcomes.The number oferrors is given for each rule,along with the total number oferrors for the ruleset as a whole.1R chooses the attribute that produces rules with the smallestnumber oferrors—that is,the ﬁrst and third rule sets.Arbitrarily breaking thetie between these two rule sets gives:outlook: sunnyÆnoovercast ÆyesrainyÆyes4.1INFERRING RUDIMENTARY RULES85For each attribute,  For each value of that attribute, make a rule as follows:    count how often each class appears    find the most frequent class    make the rule assign that class to this attribute-value.  Calculate the error rate of the rules.Choose the rules with the smallest error rate.Figure 4.1Pseudocode for 1R.Table 4.1Evaluating the attributes in the weather data.AttributeRulesErrorsTotal errors1outlooksunny Æno2/54/14overcast Æyes0/4rainy Æyes2/52temperaturehot Æno*2/45/14mild Æyes2/6cool Æyes1/43humidityhigh Æno3/74/14normal Æyes1/74windyfalse Æyes2/85/14true Æno*3/6*A random choice was made between two equally likely outcomes.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 85We noted at the outset that the game for the weather data is unspeciﬁed.Oddly enough,it is apparently played when it is overcast or rainy but not whenit is sunny.Perhaps it’s an indoor pursuit.Missing values and numeric attributesAlthough a very rudimentary learning method,1R does accommodate bothmissing values and numeric attributes.It deals with these in simple but effec-tive ways.Missingis treated as just another attribute value so that,for example,ifthe weather data had contained missing values for the outlookattribute,a ruleset formed on outlookwould specify four possible class values,one each forsunny,overcast,and rainyand a fourth for missing.We can convert numeric attributes into nominal ones using a simple dis-cretization method.First,sort the training examples according to the values ofthe numeric attribute.This produces a sequence ofclass values.For example,sorting the numeric version ofthe weather data (Table 1.3) according to thevalues oftemperatureproduces the sequence6465686970717272757580818385yesno yes yesyes no  no yes yes yes no  yes yes noDiscretization involves partitioning this sequence by placing breakpoints init.One possibility is to place breakpoints wherever the class changes,producingeight categories:yes |no |yes yes yes |no no |yes yes yes |no |yes yes |noChoosing breakpoints halfway between the examples on either side placesthem at 64.5,66.5,70.5,72,77.5,80.5,and 84.However,the two instances withvalue 72 cause a problem because they have the same value oftemperaturebutfall into different classes.The simplest ﬁx is to move the breakpoint at 72 upone example,to 73.5,producing a mixed partition in which nois the majorityclass.A more serious problem is that this procedure tends to form a large numberofcategories.The 1R method will naturally gravitate toward choosing an attri-bute that splits into many categories,because this will partition the dataset intomany classes,making it more likely that instances will have the same class as themajority in their partition.In fact,the limiting case is an attribute that has adifferent value for each instance—that is,an identiﬁcation codeattribute thatpinpoints instances uniquely—and this will yield a zero error rate on the train-ing set because each partition contains just one instance.Ofcourse,highlybranching attributes do not usually perform well on test examples;indeed,theidentiﬁcation codeattribute will never predict any examples outside the trainingset correctly.This phenomenon is known as overﬁtting;we have already86CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 86described overﬁtting-avoidance bias in Chapter 1 (page 35),and we willencounter this problem repeatedly in subsequent chapters.For 1R,overﬁtting is likely to occur whenever an attribute has a large number ofpossible values.Consequently,when discretizing a numeric attrib-ute a rule is adopted that dictates a minimum number ofexamples ofthe majority class in each partition.Suppose that minimum is set at three.Thiseliminates all but two ofthe preceding partitions.Instead,the partitioningprocess beginsyes no yes yes |yes...ensuring that there are three occurrences ofyes,the majority class,in the ﬁrstpartition.However,because the next example is also yes,we lose nothing byincluding that in the ﬁrst partition,too.This leads to a new division:yes no yes yes yes |no no yes yes yes |no yes yes nowhere each partition contains at least three instances ofthe majority class,exceptthe last one,which will usually have less.Partition boundaries always fallbetween examples ofdifferent classes.Whenever adjacent partitions have the same majority class,as do the ﬁrst twopartitions above,they can be merged together without affecting the meaning ofthe rule sets.Thus the ﬁnal discretization isyes no yes yes yes no no yes yes yes |no yes yes nowhich leads to the rule settemperature: £77.5 Æyes>77.5 ÆnoThe second rule involved an arbitrary choice;as it happens,nowas chosen.Ifwe had chosen yesinstead,there would be no need for any breakpoint at all—and as this example illustrates,it might be better to use the adjacent categoriesto help to break ties.In fact this rule generates ﬁve errors on the training setand so is less effective than the preceding rule for outlook.However,the sameprocedure leads to this rule for humidity:humidity: £82.5 Æyes>82.5 and £95.5 Æno>95.5 ÆyesThis generates only three errors on the training set and is the best “1-rule”forthe data in Table 1.3.Finally,ifa numeric attribute has missing values,an additional category iscreated for them,and the preceding discretization procedure is applied just tothe instances for which the attribute’s value is deﬁned.4.1INFERRING RUDIMENTARY RULES87P088407-Ch004.qxd  4/30/05  11:13 AM  Page 87DiscussionIn a seminal paper titled “Very simple classiﬁcation rules perform well on mostcommonly used datasets”(Holte 1993),a comprehensive study ofthe perform-ance ofthe 1R procedure was reported on 16 datasets frequently used bymachine learning researchers to evaluate their algorithms.Throughout,thestudy used cross-validation,an evaluation technique that we will explain inChapter 5,to ensure that the results were representative ofwhat independenttest sets would yield.After some experimentation,the minimum number ofexamples in each partition ofa numeric attribute was set at six,not three asused for the preceding illustration.Surprisingly,despite its simplicity 1R did astonishingly—even embarrass-ingly—well in comparison with state-of-the-art learning methods,and the rulesit produced turned out to be just a few percentage points less accurate,on almostall ofthe datasets,than the decision trees produced by a state-of-the-art deci-sion tree induction scheme.These trees were,in general,considerably largerthan 1R’s rules.Rules that test a single attribute are often a viable alternative tomore complex structures,and this strongly encourages a simplicity-ﬁrst meth-odology in which the baseline performance is established using simple,rudi-mentary techniques before progressing to more sophisticated learning methods,which inevitably generate output that is harder for people to interpret.The 1R procedure learns a one-level decision tree whose leaves represent thevarious different classes.A slightly more expressive technique is to use a differ-ent rule for each class.Each rule is a conjunction oftests,one for each attribute.For numeric attributes the test checks whether the value lies within a given inter-val;for nominal ones it checks whether it is in a certain subset ofthat attribute’svalues.These two types oftests—intervals and subset—are learned from thetraining data pertaining to each class.For a numeric attribute,the endpoints ofthe interval are the minimum and maximum values that occur in the trainingdata for that class.For a nominal one,the subset contains just those values thatoccur for that attribute in the training data for the class.Rules representing dif-ferent classes usually overlap,and at prediction time the one with the mostmatching tests is predicted.This simple technique often gives a useful ﬁrstimpression ofa dataset.It is extremely fast and can be applied to very largequantities ofdata.4.2Statistical modelingThe 1R method uses a single attribute as the basis for its decisions and choosesthe one that works best.Another simple technique is to use all attributes andallow them to make contributions to the decision that are equally importantandindependentofone another,given the class.This is unrealistic,ofcourse:what88CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 88makes real-life datasets interesting is that the attributes are certainly not equallyimportant or independent.But it leads to a simple scheme that again works sur-prisingly well in practice.Table 4.2 shows a summary ofthe weather data obtained by counting howmany times each attribute–value pair occurs with each value (yesand no) forplay.For example,you can see from Table 1.2 that outlookis sunnyfor ﬁve exam-ples,two ofwhich have play=yesand three ofwhich have play=no.The cellsin the ﬁrst row ofthe new table simply count these occurrences for all possiblevalues ofeach attribute,and the playﬁgure in the ﬁnal column counts the totalnumber ofoccurrences ofyesand no.In the lower part ofthe table,we rewrotethe same information in the form offractions,or observed probabilities.Forexample,ofthe nine days that playis yes,outlookis sunnyfor two,yielding afraction of2/9.For playthe fractions are different:they are the proportion ofdays that playis yesand no,respectively.Now suppose we encounter a new example with the values that are shown inTable 4.3.We treat the ﬁve features in Table 4.2—outlook,temperature,humid-ity,windy,and the overall likelihood that playis yesor no—as equally impor-tant,independent pieces ofevidence and multiply the corresponding fractions.Looking at the outcome yesgives:The fractions are taken from the yesentries in the table according to the valuesofthe attributes for the new day,and the ﬁnal 9/14 is the overall fraction likelihood of yes=¥¥¥¥=2939393991400053..4.2STATISTICAL MODELING89Table 4.2The weather data with counts and probabilities.OutlookTemperatureHumidityWindyPlayyesnoyesnoyesnoyesnoyesnosunny23hot22high34false6295overcast40mild42normal61true33rainy32cool31sunny2/93/5hot2/92/5high3/94/5false6/92/59/145/14overcast4/90/5mild4/92/5normal6/91/5true3/93/5rainy3/92/5cool3/91/5Table 4.3A new day.OutlookTemperatureHumidityWindyPlaysunnycoolhightrue?P088407-Ch004.qxd  4/30/05  11:13 AM  Page 89representing the proportion ofdays on which playis yes.A similar calculationfor the outcome noleads toThis indicates that for the new day,nois more likely than yes—four times morelikely.The numbers can be turned into probabilities by normalizing them sothat they sum to 1:This simple and intuitive method is based on Bayes’s rule ofconditional prob-ability.Bayes’s rule says that ifyou have a hypothesis Hand evidence Ethat bearson that hypothesis,thenWe use the notation that Pr[A] denotes the probability ofan event Aand thatPr[A|B] denotes the probability ofAconditional on another event B.Thehypothesis His that playwill be,say,yes,and Pr[H|E] is going to turn out to be20.5%,just as determined previously.The evidence Eis the particular combi-nation ofattribute values for the new day,outlook=sunny,temperature=cool,humidity=high,and windy=true.Let’s call these four pieces ofevidence E1,E2,E3,and E4,respectively.Assuming that these pieces ofevidence are independent(given the class),their combined probability is obtained by multiplying theprobabilities:Don’t worry about the denominator:we will ignore it and eliminate it in theﬁnal normalizing step when we make the probabilities ofyesand nosum to 1,just as we did previously.The Pr[yes] at the end is the probability ofa yesoutcome without knowing any ofthe evidence E,that is,without knowing any-thing about the particular day referenced—it’s called the prior probabilityofthehypothesis H.In this case,it’s just 9/14,because 9 ofthe 14 training exampleshad a yesvalue for play.Substituting the fractions in Table 4.2 for the appro-priate evidence probabilities leads toPrPryesEE[]=¥¥¥¥[]29393939914,Pr PrPrPrPrPrPryesEEyesEyesEyesEyesyesE[]=[]¥[]¥[]¥[]¥[][]1234.PrPrPrPrHEEHHE[]=[][][].Probability of no=+=002060005300206795....%.Probability of yes=+=000530005300206205....%,likelihood of no=¥¥¥¥=3515453551400206..90CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 90just as we calculated previously.Again,the Pr[E] in the denominator will dis-appear when we normalize.This method goes by the name ofNaïve Bayes,because it’s based on Bayes’srule and “naïvely”assumes independence—it is only valid to multiply proba-bilities when the events are independent.The assumption that attributes areindependent (given the class) in real life certainly is a simplistic one.But despitethe disparaging name,Naïve Bayes works very well when tested on actualdatasets,particularly when combined with some ofthe attribute selection pro-cedures introduced in Chapter 7 that eliminate redundant,and hence nonin-dependent,attributes.One thing that can go wrong with Naïve Bayes is that ifa particular attributevalue does not occur in the training set in conjunction with everyclass value,things go badly awry.Suppose in the example that the training data was differ-ent and the attribute value outlook =sunnyhad always been associated with the outcome no.Then the probability ofoutlook =sunnygiven a yes,that is,Pr[outlook=sunny|yes],would be zero,and because the other probabilities aremultiplied by this the ﬁnal probability ofyeswould be zero no matter how largethey were.Probabilities that are zero hold a veto over the other ones.This is nota good idea.But the bug is easily ﬁxed by minor adjustments to the method ofcalculating probabilities from frequencies.For example,the upper part ofTable 4.2 shows that for play=yes,outlook issunnyfor two examples,overcastfor four,and rainyfor three,and the lower partgives these events probabilities of2/9,4/9,and 3/9,respectively.Instead,wecould add 1 to each numerator and compensate by adding 3 to the denomina-tor,giving probabilities of3/12,5/12,and 4/12,respectively.This will ensure thatan attribute value that occurs zero times receives a probability which is nonzero,albeit small.The strategy ofadding 1 to each count is a standard technique calledthe Laplace estimatorafter the great eighteenth-century French mathematicianPierre Laplace.Although it works well in practice,there is no particular reasonfor adding 1 to the counts:we could instead choose a small constant mand useThe value ofm,which was set to 3,effectively provides a weight that determineshow inﬂuential the a priori values of1/3,1/3,and 1/3 are for each ofthe threepossible attribute values.A large msays that these priors are very important com-pared with the new evidence coming in from the training set,whereas a smallone gives them less inﬂuence.Finally,there is no particular reason for dividingminto three equalparts in the numerators:we could use294939123++++++mmmmmmppp,,and 239439339++++++mmmmmm,,.and 4.2STATISTICAL MODELING91P088407-Ch004.qxd  4/30/05  11:13 AM  Page 91instead,where p1,p2,and p3sum to 1.Effectively,these three numbers are a prioriprobabilities ofthe values ofthe outlookattribute being sunny,overcast,andrainy,respectively.This is now a fully Bayesian formulation where prior probabilities have beenassigned to everything in sight.It has the advantage ofbeing completely rigor-ous,but the disadvantage that it is not usually clear just how these prior prob-abilities should be assigned.In practice,the prior probabilities make littledifference provided that there are a reasonable number oftraining instances,and people generally just estimate frequencies using the Laplace estimator byinitializing all counts to one instead ofto zero.Missing values and numeric attributesOne ofthe really nice things about the Bayesian formulation is that missingvalues are no problem at all.For example,ifthe value ofoutlookwere missingin the example ofTable 4.3,the calculation would simply omit this attribute,yieldingThese two numbers are individually a lot higher than they were before,becauseone ofthe fractions is missing.But that’s not a problem because a fraction ismissing in both cases,and these likelihoods are subject to a further normal-ization process.This yields probabilities for yesand noof41% and 59%,respectively.Ifa value is missing in a training instance,it is simply not included in thefrequency counts,and the probability ratios are based on the number ofvaluesthat actually occur rather than on the total number ofinstances.Numeric values are usually handled by assuming that they have a “normal”or “Gaussian”probability distribution.Table 4.4 gives a summary ofthe weatherdata with numeric features from Table 1.3.For nominal attributes,we calcu-lated counts as before,and for numeric ones we simply listed the values thatoccur.Then,whereas we normalized the counts for the nominal attributes intoprobabilities,we calculated the mean and standard deviation for each class and each numeric attribute.Thus the mean value oftemperatureover the yesinstances is 73,and its standard deviation is 6.2.The mean is simply the averageofthe preceding values,that is,the sum divided by the number ofvalues.Thestandard deviation is the square root ofthe sample variance,which we can cal-culate as follows:subtract the mean from each value,square the result,sum themtogether,and then divide by one less thanthe number ofvalues.After we havefound this sample variance,ﬁnd its square root to determine the standard devi-ation.This is the standard way ofcalculating mean and standard deviation ofalikelihood of likelihood of yesno=¥¥¥==¥¥¥=3939399140023815453551400343...92CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 92set ofnumbers (the “one less than”is to do with the number ofdegrees offreedom in the sample,a statistical notion that we don’t want to get into here).The probability density function for a normal distribution with mean mandstandard deviation sis given by the rather formidable expression:But fear not! All this means is that ifwe are considering a yesoutcome whentemperaturehas a value,say,of66,we just need to plug x=66,m=73,and s=6.2 into the formula.So the value ofthe probability density function isBy the same token,the probability density ofa yesoutcome when humidityhasvalue,say,of90 is calculated in the same way:The probability density function for an event is very closely related to its prob-ability.However,it is not quite the same thing.Iftemperature is a continuousscale,the probability ofthe temperature being exactly66—or exactlyany othervalue,such as 63.14159262—is zero.The real meaning ofthe density functionf(x) is that the probability that the quantity lies within a small region around x,say,between x-e/2 and x+e/2,is ef(x).What we have written above is correctfhumidityyes=()=9000221..ftemperatureyese=()=◊=-()◊66126200340667326222p....fxex()=-()12222psms.4.2STATISTICAL MODELING93Table 4.4The numeric weather data with summary statistics.OutlookTemperatureHumidityWindyPlayyesnoyesnoyesnoyesnoyesnosunny2383858685false6295overcast4070809690true33rainy326865807064726595697170917580757072908175sunny2/93/5mean7374.6mean79.186.2false6/92/59/145/14overcast4/90/5std. dev.6.27.9std. dev.10.29.7true3/93/5rainy3/92/5P088407-Ch004.qxd  4/30/05  11:13 AM  Page 93iftemperature is measured to the nearest degree and humidity is measured tothe nearest percentage point.You might think we ought to factor in the accu-racy ﬁgure ewhen using these probabilities,but that’s not necessary.The sameewould appear in both the yesand nolikelihoods that follow and cancel outwhen the probabilities were calculated.Using these probabilities for the new day in Table 4.5 yieldswhich leads to probabilitiesThese ﬁgures are very close to the probabilities calculated earlier for the newday in Table 4.3,because the temperatureand humidityvalues of66 and 90 yieldsimilar probabilities to the cooland highvalues used before.The normal-distribution assumption makes it easy to extend the Naïve Bayesclassiﬁer to deal with numeric attributes.Ifthe values ofany numeric attributesare missing,the mean and standard deviation calculations are based only on theones that are present.Bayesian models for document classiﬁcationOne important domain for machine learning is document classiﬁcation,inwhich each instance represents a document and the instance’s class is the doc-ument’s topic.Documents might be news items and the classes might be domes-tic news,overseas news,ﬁnancial news,and sport.Documents are characterizedby the words that appear in them,and one way to apply machine learning todocument classiﬁcation is to treat the presence or absence ofeach word as a Boolean attribute.Naïve Bayes is a popular technique for this applicationbecause it is very fast and quite accurate.However,this does not take into account the number ofoccurrences ofeachword,which is potentially useful information when determining the categoryProbability of no=+=000010800000360000108750....%.Probability of yes=+=000003600000360000108250....%,likelihood of likelihood of yesno=¥¥¥¥==¥¥¥¥=290034000221399140000036350022100381355140000108...,...;94CHAPTER 4|ALGORITHMS:THE BASIC METHODSTable 4.5Another new day.OutlookTemperatureHumidityWindyPlaysunny6690true?P088407-Ch004.qxd  4/30/05  11:13 AM  Page 94ofa document.Instead,a document can be viewed as a bag ofwords—a set thatcontains all the words in the document,with multiple occurrences ofa wordappearing multiple times (technically,a setincludes each ofits members justonce,whereas a bagcan have repeated elements).Word frequencies can beaccommodated by applying a modiﬁed form ofNaïve Bayes that is sometimesdescribed as multinominal Naïve Bayes.Suppose n1,n2,...,nkis the number oftimes word ioccurs in the document,and P1,P2,...,Pkis the probability ofobtaining word iwhen sampling fromall the documents in category H.Assume that the probability is independent ofthe word’s context and position in the document.These assumptions lead to amultinomial distributionfor document probabilities.For this distribution,theprobability ofa document Egiven its class H—in other words,the formula forcomputing the probability Pr[E|H] in Bayes’s rule—iswhere N =n1+n2+...+nkis the number ofwords in the document.The reasonfor the factorials is to account for the fact that the ordering ofthe occurrencesofeach word is immaterial according to the bag-of-words model.Piis estimatedby computing the relative frequency ofword iin the text ofall training docu-ments pertaining to category H.In reality there should be a further term thatgives the probability that the model for category Hgenerates a document whoselength is the same as the length ofE(that is why we use the symbol ªinsteadof=),but it is common to assume that this is the same for all classes and hencecan be dropped.For example,suppose there are only the two words,yellowand blue,in thevocabulary,and a particular document class Hhas Pr[yellow|H] =75% andPr[blue|H] =25% (you might call Hthe class ofyellowish green documents).Suppose Eis the document blue yellow bluewith a length ofN=3 words.Thereare four possible bags ofthree words.One is {yellow yellow yellow},and its prob-ability according to the preceding formula isThe other three,with their probabilities,arePrPrPrblueblueblueHyellowyellowblueHyellowblueblueH{}[]={}[]={}[]=1642764964PryellowyellowyellowH{}[]ª¥¥=307530250276430!.!.!PrEHNPniniiki[]ª¥=’!!14.2STATISTICAL MODELING95P088407-Ch004.qxd  4/30/05  11:13 AM  Page 95Here,Ecorresponds to the last case (recall that in a bag ofwords the order isimmaterial);thus its probability ofbeing generated by the yellowish green doc-ument model is 9/64,or 14%.Suppose another class,very bluish green docu-ments (call it H¢),has Pr[yellow |H¢] =10%,Pr[blue |H¢] =90%.The probabilitythat Eis generated by this model is 24%.Ifthese are the only two classes,does that mean that Eis in the very bluishgreen document class? Not necessarily.Bayes’s rule,given earlier,says that youhave to take into account the prior probability ofeach hypothesis.Ifyou knowthat in fact very bluish green documents are twice as rare as yellowish greenones,this would be just sufﬁcient to outweigh the preceding 14% to 24% disparityand tip the balance in favor ofthe yellowish greenclass.The factorials in the preceding probability formula don’t actually need to becomputed because—being the same for every class—they drop out in the nor-malization process anyway.However,the formula still involves multiplyingtogether many small probabilities,which soon yields extremely small numbersthat cause underﬂow on large documents.The problem can be avoided by usinglogarithms ofthe probabilities instead ofthe probabilities themselves.In the multinomial Naïve Bayes formulation a document’s class is determinednot just by the words that occur in it but also by the number oftimes they occur.In general it performs better than the ordinary Naïve Bayes model for docu-ment classiﬁcation,particularly for large dictionary sizes.DiscussionNaïve Bayes gives a simple approach,with clear semantics,to representing,using,and learning probabilistic knowledge.Impressive results can be achievedusing it.It has often been shown that Naïve Bayes rivals,and indeed outper-forms,more sophisticated classiﬁers on many datasets.The moral is,always trythe simple things ﬁrst.Repeatedly in machine learning people have eventually,after an extended struggle,obtained good results using sophisticated learningmethods only to discover years later that simple methods such as 1R and NaïveBayes do just as well—or even better.There are many datasets for which Naïve Bayes does not do so well,however,and it is easy to see why.Because attributes are treated as though they were com-pletely independent,the addition ofredundant ones skews the learning process.As an extreme example,ifyou were to include a new attribute with the samevalues as temperatureto the weather data,the effect ofthe temperature attri-bute would be multiplied:all ofits probabilities would be squared,giving it agreat deal more inﬂuence in the decision.Ifyou were to add 10 such attributes,then the decisions would effectively be made on temperaturealone.Dependen-cies between attributes inevitably reduce the power ofNaïve Bayes to discernwhat is going on.They can,however,be ameliorated by using a subset ofthe96CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 964.3DIVIDE-AND-CONQUER:CONSTRUCTING DECISION TREES97attributes in the decision procedure,making a careful selection ofwhich onesto use.Chapter 7 shows how.The normal-distribution assumption for numeric attributes is anotherrestriction on Naïve Bayes as we have formulated it here.Many features simplyaren’t normally distributed.However,there is nothing to prevent us from usingother distributions for the numeric attributes:there is nothing magic about thenormal distribution.Ifyou know that a particular attribute is likely to followsome other distribution,standard estimation procedures for that distributioncan be used instead.Ifyou suspect it isn’t normal but don’t know the actual distribution,there are procedures for “kernel density estimation”that do notassume any particular distribution for the attribute values.Another possibilityis simply to discretize the data ﬁrst.4.3Divide-and-conquer: Constructing decision treesThe problem ofconstructing a decision tree can be expressed recursively.First,select an attribute to place at the root node and make one branch for each pos-sible value.This splits up the example set into subsets,one for every value ofthe attribute.Now the process can be repeated recursively for each branch,usingonly those instances that actually reach the branch.Ifat any time all instancesat a node have the same classiﬁcation,stop developing that part ofthe tree.The only thing left to decide is how to determine which attribute to split on,given a set ofexamples with different classes.Consider (again!) the weather data.There are four possibilities for each split,and at the top level they produce treessuch as those in Figure 4.2.Which is the best choice? The number ofyesand noclasses are shown at the leaves.Any leafwith only one class—yesor no—willnot have to be split further,and the recursive process down that branch will ter-minate.Because we seek small trees,we would like this to happen as soon aspossible.Ifwe had a measure ofthe purity ofeach node,we could choose theattribute that produces the purest daughter nodes.Take a moment to look atFigure 4.2 and ponder which attribute you think is the best choice.The measure ofpurity that we will use is called the informationand is meas-ured in units called bits.Associated with a node ofthe tree,it represents theexpected amount ofinformation that would be needed to specify whether a newinstance should be classiﬁed yesor no,given that the example reached that node.Unlike the bits in computer memory,the expected amount ofinformationusually involves fractions ofa bit—and is often less than one! We calculate itbased on the number ofyesand noclasses at the node;we will look at the detailsofthe calculation shortly.But ﬁrst let’s see how it’s used.When evaluating theﬁrst tree in Figure 4.2,the numbers ofyesand noclasses at the leafnodes are[2,3],[4,0],and [3,2],respectively,and the information values ofthese nodes are:P088407-Ch004.qxd  4/30/05  11:13 AM  Page 97We can calculate the average information value ofthese,taking into account thenumber ofinstances that go down each branch—ﬁve down the ﬁrst and thirdand four down the second:This average represents the amount ofinformation that we expect would be nec-essary to specify the class ofa new instance,given the tree structure in Figure4.2(a).infobits.2340325140971414051409710693,,,,,...[][][]()=()¥+()¥+()¥=infobitsinfobitsinfobits2309714000320971,.,.,.[]()=[]()=[]()=98CHAPTER 4|ALGORITHMS:THE BASIC METHODSyesyesnononosunnyyesyesyesyesovercastyesyesyesnonorainyoutlook(a)yesyesyesnonohotyesyesnonoyesyesyesnomildcooltemperatureyes(b)yesyesyesnonononoyesyesyesyesyesyesnohighnormalhumidity(c)yesyesyesyesyesyesnonoyesyesyesnononofalsetruewindy(d)Figure 4.2Tree stumps for the weather data.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 984.3DIVIDE-AND-CONQUER:CONSTRUCTING DECISION TREES99Before we created any ofthe nascent tree structures in Figure 4.2,the train-ing examples at the root comprised nine yesand ﬁve nonodes,correspondingto an information value ofThus the tree in Figure 4.2(a) is responsible for an information gain ofwhich can be interpreted as the informational value ofcreating a branch on theoutlook attribute.The way forward is clear.We calculate the information gain for each attri-bute and choose the one that gains the most information to split on.In the sit-uation ofFigure 4.2,so we select outlookas the splitting attribute at the root ofthe tree.Hopefullythis accords with your intuition as the best one to select.It is the only choicefor which one daughter node is completely pure,and this gives it a considerableadvantage over the other attributes.Humidityis the next best choice because itproduces a larger daughter node that is almost completely pure.Then we continue,recursively.Figure 4.3 shows the possibilities for a furtherbranch at the node reached when outlookis sunny.Clearly,a further split onoutlookwill produce nothing new,so we only consider the other three attributes.The information gain for each turns out to beso we select humidityas the splitting attribute at this point.There is no need tosplit these nodes any further,so this branch is ﬁnished.Continued application ofthe same idea leads to the decision tree ofFigure4.4 for the weather data.Ideally,the process terminates when all leafnodes arepure,that is,when they contain instances that all have the same classiﬁcation.However,it might not be possible to reach this happy situation because there isnothing to stop the training set containing two examples with identical sets ofattributes but different classes.Consequently,we stop when the data cannot besplit any further.gainbitsgainbitsgainbits,temperaturehumiditywindy()=()=()=057109710020...gainbitsgainbitsgainbitsgainbits,outlooktemperaturehumiditywindy()=()=()=()=0247002901520048....gaininfoinfobits,outlook()=[]()-[][][]()=-=95234032094006930247,,,,,,...infobits.950940,.[]()=P088407-Ch004.qxd  4/30/05  11:13 AM  Page 99Calculating informationNow it is time to explain how to calculate the information measure that is usedas a basis for evaluating different splits.We describe the basic idea in this section,then in the next we examine a correction that is usually made to counter a biastoward selecting splits on attributes with large numbers ofpossible values.Before examining the detailed formula for calculating the amount ofinfor-mation required to specify the class ofan example given that it reaches a treenode with a certain number ofyes’s and no’s,consider ﬁrst the kind ofproper-ties we would expect this quantity to have:100CHAPTER 4|ALGORITHMS:THE BASIC METHODS......nonoyessunnyhotmildcooloutlooktemperatureyesno(a)......nononoyesyessunnyhighnormaloutlookhumidity(b)......yesyesnonoyesnosunnyfalsetrueoutlookwindy(c)Figure 4.3Expanded tree stumps for the weather data.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1004.3DIVIDE-AND-CONQUER:CONSTRUCTING DECISION TREES1011.When the number ofeither yes’s or no’s is zero,the information is zero.2.When the number ofyes’s and no’s is equal,the information reaches amaximum.Moreover,the measure should be applicable to multiclass situations,not just totwo-class ones.The information measure relates to the amount ofinformation obtained bymaking a decision,and a more subtle property ofinformation can be derivedby considering the nature ofdecisions.Decisions can be made in a single stage,or they can be made in several stages,and the amount ofinformation involvedis the same in both cases.For example,the decision involved incan be made in two stages.First decide whether it’s the ﬁrst case or one oftheother two cases:and then decide which ofthe other two cases it is:In some cases the second decision will not need to be made,namely,when the decision turns out to be the ﬁrst one.Taking this into account leads to theequationinfo2,3,4info2,7info3,4[]()=[]()+()¥[]()79.info3,4[]()info2,7[]()info2,3,4[]()falsetrueyesyesnosunnyovercastrainyoutlookhumiditywindyhighnormalnoyesFigure 4.4Decision tree for the weather data.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 101Ofcourse,there is nothing special about these particular numbers,and a similarrelationship must hold regardless ofthe actual values.Thus we can add a furthercriterion to the preceding list:3.The information must obey the multistage property illustrated previously.Remarkably,it turns out that there is only one function that satisﬁes all theseproperties,and it is known as the information valueor entropy:The reason for the minus signs is that logarithms ofthe fractions p1,p2,...,pnare negative,so the entropy is actually positive.Usually the logarithms areexpressed in base 2,then the entropy is in units called bits—just the usual kindofbits used with computers.The arguments p1,p2,...ofthe entropy formula are expressed as fractionsthat add up to one,so that,for example,Thus the multistage decision property can be written in general aswhere p+q+r=1.Because ofthe way the log function works,you can calculate the informationmeasure without having to work out the individual fractions:This is the way that the information measure is usually calculated in practice.So the information value for the ﬁrst leafnode ofthe ﬁrst tree in Figure4.2 isas stated on page 98.Highly branching attributesWhen some attributes have a large number ofpossible values,giving rise to amultiway branch with many child nodes,a problem arises with the informationgain calculation.The problem can best be appreciated in the extreme case whenan attribute has a different value for each instance in the dataset—as,forexample,an identiﬁcation codeattribute might.info2,3bits,[]()=-¥-¥=252535350971loglog.info2,3,4[]()=-¥-¥-¥=---+[]292939394949223344999logloglogloglogloglog.entropyentropyentropypqrpqrqrqqrrqr,,,,()=+()++()◊++ÊËˆ¯info2,3,4entropy29[]()=(),,.3949entropypppppppppnnn121122,,...,loglog...log()=---102CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1024.3DIVIDE-AND-CONQUER:CONSTRUCTING DECISION TREES103Table 4.6 gives the weather data with this extra attribute.Branching on IDcodeproduces the tree stump in Figure 4.5.The information required to specifythe class given the value ofthis attribute iswhich is zero because each ofthe 14 terms is zero.This is not surprising:the IDcodeattribute identiﬁes the instance,which determines the class without anyambiguity—just as Table 4.6 shows.Consequently,the information gain ofthisattribute is just the information at the root,info([9,5]) =0.940 bits.This isgreater than the information gain ofany other attribute,and so ID codewillinevitably be chosen as the splitting attribute.But branching on the identiﬁca-tion code is no good for predicting the class ofunknown instances and tellsnothing about the structure ofthe decision,which after all are the twin goals ofmachine learning.info0,1info0,1info1,0info1,0info0,1[]()+[]()+[]()++[]()+[]()...,Table 4.6The weather data with identiﬁcation codes.ID codeOutlookTemperatureHumidityWindyPlayasunnyhothighfalsenobsunnyhothightruenocovercasthothighfalseyesdrainymildhighfalseyeserainycoolnormalfalseyesfrainycoolnormaltruenogovercastcoolnormaltrueyeshsunnymildhighfalsenoisunnycoolnormalfalseyesjrainymildnormalfalseyesksunnymildnormaltrueyeslovercastmildhightrueyesmovercasthotnormalfalseyesnrainymildhightruenonoyesnoyesnoID codeabc ...mnFigure 4.5Tree stump for the ID codeattribute.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 103The overall effect is that the information gain measure tends to prefer attri-butes with large numbers ofpossible values.To compensate for this,a modiﬁ-cation ofthe measure called the gain ratiois widely used.The gain ratio isderived by taking into account the number and size ofdaughter nodes intowhich an attribute splits the dataset,disregarding any information about theclass.In the situation shown in Figure 4.5,all counts have a value of1,so theinformation value ofthe split isbecause the same fraction,1/14,appears 14 times.This amounts to log 14,or3.807 bits,which is a very high value.This is because the information value ofa split is the number ofbits needed to determine to which branch each instanceis assigned,and the more branches there are,the greater this value is.The gainratio is calculated by dividing the original information gain,0.940 in this case,by the information value ofthe attribute,3.807—yielding a gain ratio value of0.247 for the ID codeattribute.Returning to the tree stumps for the weather data in Figure 4.2,outlooksplitsthe dataset into three subsets ofsize 5,4,and 5 and thus has an intrinsic infor-mation value ofwithout paying any attention to the classes involved in the subsets.As we haveseen,this intrinsic information value is higher for a more highly branchingattribute such as the hypothesizedID code.Again we can correct the informa-tion gain by dividing by the intrinsic information value to get the gain ratio.The results ofthese calculations for the tree stumps ofFigure 4.2 are sum-marized in Table 4.7.Outlookstill comes out on top,but humidityis now a muchcloser contender because it splits the data into two subsets instead ofthree.Inthis particular example,the hypothetical ID codeattribute,with a gain ratio of0.247,would still be preferred to any ofthese four.However,its advantage isinfo5,4,5[]()=1577.info1,1,...,1[]()=-¥¥11411414log,104CHAPTER 4|ALGORITHMS:THE BASIC METHODSTable 4.7Gain ratio calculations for the tree stumps of Figure 4.2.OutlookTemperatureHumidityWindyinfo:0.693info:0.911info:0.788info:0.892gain: 0.940–0.247gain: 0.940–0.029gain: 0.940–0.152gain: 0.940–0.0480.6930.9110.7880.892split info: 1.577split info: 1.557split info: 1.000split info: 0.985info([5,4,5])info([4,6,4])info ([7,7])info([8,6])gain ratio: 0.157gain ratio: 0.019gain ratio: 0.152gain ratio: 0.0490.247/1.5770.029/1.5570.152/10.048/0.985P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1044.4COVERING ALGORITHMS:CONSTRUCTING RULES105greatly reduced.In practical implementations,we can use an ad hoc test to guardagainst splitting on such a useless attribute.Unfortunately,in some situations the gain ratio modiﬁcation overcompen-sates and can lead to preferring an attribute just because its intrinsic informa-tion is much lower than that for the other attributes.A standard ﬁx is to choosethe attribute that maximizes the gain ratio,provided that the information gainfor that attribute is at least as great as the average information gain for all theattributes examined.DiscussionThe divide-and-conquer approach to decision tree induction,sometimes calledtop-down induction ofdecision trees,was developed and reﬁned over many yearsby J.Ross Quinlan ofthe University ofSydney,Australia.Although others haveworked on similar methods,Quinlan’s research has always been at the very fore-front ofdecision tree induction.The method that has been described using theinformation gain criterion is essentially the same as one known as ID3.The useofthe gain ratio was one ofmany improvements that were made to ID3 overseveral years;Quinlan described it as robust under a wide variety ofcircum-stances.Although a robust and practical solution,it sacriﬁces some ofthe ele-gance and clean theoretical motivation ofthe information gain criterion.A series ofimprovements to ID3 culminated in a practical and inﬂuentialsystem for decision tree induction called C4.5.These improvements includemethods for dealing with numeric attributes,missing values,noisy data,andgenerating rules from trees,and they are described in Section 6.1.4.4Covering algorithms: Constructing rulesAs we have seen,decision tree algorithms are based on a divide-and-conquerapproach to the classiﬁcation problem.They work from the top down,seekingat each stage an attribute to split on that best separates the classes;then recur-sively processing the subproblems that result from the split.This strategy generates a decision tree,which can ifnecessary be converted into a set ofclas-siﬁcation rules—although ifit is to produce effective rules,the conversion is nottrivial.An alternative approach is to take each class in turn and seek a way ofcov-ering all instances in it,at the same time excluding instances not in the class.This is called a coveringapproach because at each stage you identify a rule that“covers”some ofthe instances.By its very nature,this covering approach leadsto a set ofrules rather than to a decision tree.The covering method can readily be visualized in a two-dimensional spaceofinstances as shown in Figure 4.6(a).We ﬁrst make a rule covering the a’s.ForP088407-Ch004.qxd  4/30/05  11:13 AM  Page 105the ﬁrst test in the rule,split the space vertically as shown in the center picture.This gives the beginnings ofa rule:If x >1.2 then class =aHowever,the rule covers many b’s as well as a’s,so a new test is added to therule by further splitting the space horizontally as shown in the third diagram:If x >1.2 and y >2.6 then class =aThis gives a rule covering all but one ofthe a’s.It’s probably appropriate to leaveit at that,but ifit were felt necessary to cover the ﬁnal a,another rule would benecessary—perhapsIf x >1.4 and y <2.4 then class =aThe same procedure leads to two rules covering the b’s:If x £1.2 then class =bIf x >1.2 and y £2.6 then class =b106CHAPTER 4|ALGORITHMS:THE BASIC METHODSFigure 4.6Covering algorithm:(a) covering the instances and (b) the decision tree forthe same problem.x > 1.2 ?bnoy > 2.6 ?yesbnoayes(b)P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1064.4COVERING ALGORITHMS:CONSTRUCTING RULES107Again,one ais erroneously covered by these rules.Ifit were necessary to excludeit,more tests would have to be added to the second rule,and additional ruleswould need to be introduced to cover the b’s that these new tests exclude.Rules versus treesA top-down divide-and-conquer algorithm operates on the same data in amanner that is,at least superﬁcially,quite similar to a covering algorithm.Itmight ﬁrst split the dataset using the xattribute and would probably end upsplitting it at the same place,x=1.2.However,whereas the covering algorithmis concerned only with covering a single class,the division would take bothclasses into account,because divide-and-conquer algorithms create a singleconcept description that applies to all classes.The second split might also be atthe same place,y=2.6,leading to the decision tree in Figure 4.6(b).This treecorresponds exactly to the set ofrules,and in this case there is no difference ineffect between the covering and the divide-and-conquer algorithms.But in many situations there isa difference between rules and trees in termsofthe perspicuity ofthe representation.For example,when we described thereplicated subtree problem in Section 3.3,we noted that rules can be symmet-ric whereas trees must select one attribute to split on ﬁrst,and this can lead totrees that are much larger than an equivalent set ofrules.Another difference isthat,in the multiclass case,a decision tree split takes all classes into account,trying to maximize the purity ofthe split,whereas the rule-generating methodconcentrates on one class at a time,disregarding what happens to the otherclasses.A simple covering algorithmCovering algorithms operate by adding tests to the rule that is under construc-tion,always striving to create a rule with maximum accuracy.In contrast,divide-and-conquer algorithms operate by adding tests to the tree that is underconstruction,always striving to maximize the separation among the classes.Each ofthese involves ﬁnding an attribute to split on.But the criterion for thebest attribute is different in each case.Whereas divide-and-conquer algorithmssuch as ID3 choose an attribute to maximize the information gain,the cover-ing algorithm we will describe chooses an attribute–value pair to maximize theprobability ofthe desired classiﬁcation.Figure 4.7 gives a picture ofthe situation,showing the space containing allthe instances,a partially constructed rule,and the same rule after a new termhas been added.The new term restricts the coverage ofthe rule:the idea is toinclude as many instances ofthe desired class as possible and exclude as manyinstances ofother classes as possible.Suppose the new rule will cover a total oftinstances,ofwhich pare positive examples ofthe class and t-pare in otherP088407-Ch004.qxd  4/30/05  11:13 AM  Page 107classes—that is,they are errors made by the rule.Then choose the new term tomaximize the ratio p/t.An example will help.For a change,we use the contact lens problem ofTable1.1.We will form rules that cover each ofthe three classes,hard,soft,and none,in turn.To begin,we seek a rule:If ? then recommendation =hardFor the unknown term ?,we have nine choices:age =young2/8age =pre-presbyopic1/8age =presbyopic1/8spectacle prescription =myope3/12spectacle prescription =hypermetrope1/12astigmatism =no0/12astigmatism =yes4/12tear production rate =reduced0/12tear production rate =normal4/12The numbers on the right show the fraction of“correct”instances in the setsingled out by that choice.In this case,correctmeans that the recommendation ishard.For instance,age=youngselects eight instances,two ofwhich recommendhard contact lenses,so the ﬁrst fraction is 2/8.(To follow this,you will need tolook back at the contact lens data in Table 1.1 on page 6 and count up the entriesin the table.) We select the largest fraction,4/12,arbitrarily choosing betweenthe seventh and the last choice in the preceding list,and create the rule:If astigmatism =yes then recommendation =hardThis rule is an inaccurate one,getting only 4 instances correct out ofthe 12that it covers,shown in Table 4.8.So we reﬁne it further:If astigmatism =yes and ? then recommendation =hard108CHAPTER 4|ALGORITHMS:THE BASIC METHODSFigure 4.7The instance space during operation ofa covering algorithm.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1084.4COVERING ALGORITHMS:CONSTRUCTING RULES109Considering the possibilities for the unknown term ? yields the seven choices:age =young2/4age =pre-presbyopic1/4age =presbyopic1/4spectacle prescription =myope3/6spectacle prescription =hypermetrope1/6tear production rate =reduced0/6tear production rate =normal4/6(Again,count the entries in Table 4.8.) The last is a clear winner,getting fourinstances correct out ofthe six that it covers,and corresponds to the ruleIf astigmatism =yes and tear production rate =normalthen recommendation =hardShould we stop here? Perhaps.But let’s say we are going for exact rules,nomatter how complex they become.Table 4.9 shows the cases that are covered bythe rule so far.The possibilities for the next term are nowage =young2/2age =pre-presbyopic1/2age =presbyopic1/2spectacle prescription =myope3/3spectacle prescription =hypermetrope1/3We need to choose between the ﬁrst and fourth.So far we have treated the frac-tions numerically,but although these two are equal (both evaluate to 1),theyhave different coverage:one selects just two correct instances and the otherTable 4.8Part of the contact lens data for which astigmatism=yes.AgeSpectacle AstigmatismTear production Recommended prescriptionratelensesyoungmyopeyesreducednoneyoungmyopeyesnormalhardyounghypermetropeyesreducednoneyounghypermetropeyesnormalhardpre-presbyopicmyopeyesreducednonepre-presbyopicmyopeyesnormalhardpre-presbyopichypermetropeyesreducednonepre-presbyopichypermetropeyesnormalnonepresbyopicmyopeyesreducednonepresbyopicmyopeyesnormalhardpresbyopichypermetropeyesreducednonepresbyopichypermetropeyesnormalnoneP088407-Ch004.qxd  4/30/05  11:13 AM  Page 109selects three.In the event ofa tie,we choose the rule with the greater coverage,giving the ﬁnal rule:If astigmatism =yes and tear production rate =normaland spectacle prescription =myope then recommendation =hardThis is indeed one ofthe rules given for the contact lens problem.But it onlycovers three ofthe four hardrecommendations.So we delete these three fromthe set ofinstances and start again,looking for another rule ofthe form:If ? then recommendation =hardFollowing the same process,we will eventually ﬁnd that age=youngis the bestchoice for the ﬁrst term.Its coverage is seven;the reason for the seven is that 3instances have been removed from the original set,leaving 21 instances alto-gether.The best choice for the second term is astigmatism=yes,selecting 1/3(actually,this is a tie);tear production rate=normalis the best for the third,selecting 1/1.If age =young and astigmatism =yes and tear production rate =normal then recommendation =hardThis rule actually covers three ofthe original set ofinstances,two ofwhich arecovered by the previous rule—but that’s all right because the recommendationis the same for each rule.Now that all the hard-lens cases are covered,the next step is to proceed withthe soft-lens ones in just the same way.Finally,rules are generated for the nonecase—unless we are seeking a rule set with a default rule,in which case explicitrules for the ﬁnal outcome are unnecessary.What we have just described is the PRISM method for constructing rules.Itgenerates only correct or “perfect”rules.It measures the success ofa rule by theaccuracy formula p/t.Any rule with accuracy less than 100% is “incorrect”in110CHAPTER 4|ALGORITHMS:THE BASIC METHODSTable 4.9Part of the contact lens data for which astigmatism=yesand tear production rate=normal.AgeSpectacle AstigmatismTear productionRecommendedprescriptionratelensesyoungmyopeyesnormalhardyounghypermetropeyesnormalhardpre-presbyopicmyopeyesnormalhardpre-presbyopichypermetropeyesnormalnonepresbyopicmyopeyesnormalhardpresbyopichypermetropeyesnormalnoneP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1104.4COVERING ALGORITHMS:CONSTRUCTING RULES111that it assigns cases to the class in question that actually do not have that class.PRISM continues adding clauses to each rule until it is perfect:its accuracy is100%.Figure 4.8 gives a summary ofthe algorithm.The outer loop iterates overthe classes,generating rules for each class in turn.Note that we reinitialize tothe full set ofexamples each time round.Then we create rules for that class andremove the examples from the set until there are none ofthat class left.When-ever we create a rule,start with an empty rule (which covers all the examples),and then restrict it by adding tests until it covers only examples ofthe desiredclass.At each stage choose the most promising test,that is,the one that maxi-mizes the accuracy ofthe rule.Finally,break ties by selecting the test with great-est coverage.Rules versus decision listsConsider the rules produced for a particular class,that is,the algorithm in Figure4.8 with the outer loop removed.It seems clear from the way that these rulesare produced that they are intended to be interpreted in order,that is,as a deci-sion list,testing the rules in turn until one applies and then using that.This isbecause the instances covered by a new rule are removed from the instance setas soon as the rule is completed (in the third line from the end ofthe code inFigure 4.8):thus subsequent rules are designed for instances that are notcoveredby the rule.However,although it appears that we are supposed to check the rulesin turn,we do not have to do so.Consider that any subsequent rules generatedfor this class will have the same effect—they all predict the same class.Thismeans that it does not matter what order they are executed in:either a rule willFor each class C   Initialize E to the instance set  While E contains instances in class C     Create a rule R with an empty left-hand side that predicts class C     Until R is perfect (or there are no more attributes to use) do      For each attribute A not mentioned in R, and each value v,        Consider adding the condition A=v to the LHS of R         Select A and v to maximize the accuracy p/t          (break ties by choosing the condition with the largest p)      Add A=v to R     Remove the instances covered by R from E Figure 4.8Pseudocode for a basic rule learner.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 111be found that covers this instance,in which case the class in question is pre-dicted,or no such rule is found,in which case the class is not predicted.Now return to the overall algorithm.Each class is considered in turn,andrules are generated that distinguish instances in that class from the others.Noordering is implied between the rules for one class and those for another.Con-sequently,the rules that are produced can be executed independent oforder.As described in Section 3.3,order-independent rules seem to provide moremodularity by each acting as independent nuggets of“knowledge,”but theysuffer from the disadvantage that it is not clear what to do when conﬂictingrules apply.With rules generated in this way,a test example may receive multi-ple classiﬁcations,that is,rules that apply to different classes may accept it.Othertest examples may receive no classiﬁcation at all.A simple strategy to force adecision in these ambiguous cases is to choose,from the classiﬁcations that arepredicted,the one with the most training examples or,ifno classiﬁcation is pre-dicted,to choose the category with the most training examples overall.Thesedifﬁculties do not occur with decision lists because they are meant to be inter-preted in order and execution stops as soon as one rule applies:the addition ofa default rule at the end ensures that any test instance receives a classiﬁcation.It is possible to generate good decision lists for the multiclass case using a slightlydifferent method,as we shall see in Section 6.2.Methods such as PRISM can be described as separate-and-conqueralgo-rithms:you identify a rule that covers many instances in the class (and excludesones not in the class),separate out the covered instances because they are alreadytaken care ofby the rule,and continue the process on those that are left.Thiscontrasts nicely with the divide-and-conquer approach ofdecision trees.Theseparate step greatly increases the efﬁciency ofthe method because the instanceset continually shrinks as the operation proceeds.4.5Mining association rulesAssociation rules are like classiﬁcation rules.You could ﬁnd them in the sameway,by executing a divide-and-conquer rule-induction procedure for each pos-sible expression that could occur on the right-hand side ofthe rule.But not onlymight any attribute occur on the right-hand side with any possible value;asingle association rule often predicts the value ofmore than one attribute.Toﬁnd such rules,you would have to execute the rule-induction procedure oncefor every possible combinationofattributes,with every possible combination ofvalues,on the right-hand side.That would result in an enormous number ofassociation rules,which would then have to be pruned down on the basis oftheir coverage(the number ofinstances that they predict correctly) and their112CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1124.5MINING ASSOCIATION RULES113accuracy(the same number expressed as a proportion ofthe number ofinstances to which the rule applies).This approach is quite infeasible.(Note that,as we mentioned in Section 3.4,what we are calling coverageis often calledsupportand what we are calling accuracy is often called conﬁdence.)Instead,we capitalize on the fact that we are only interested in associationrules with high coverage.We ignore,for the moment,the distinction betweenthe left- and right-hand sides ofa rule and seek combinations ofattribute–valuepairs that have a prespeciﬁed minimum coverage.These are called item sets:anattribute–value pair is an item.The terminology derives from market basketanalysis,in which the items are articles in your shopping cart and the super-market manager is looking for associations among these purchases.Item setsThe ﬁrst column ofTable 4.10 shows the individual items for the weather dataofTable 1.2,with the number oftimes each item appears in the dataset givenat the right.These are the one-item sets.The next step is to generate the two-item sets by making pairs ofone-item ones.Ofcourse,there is no point in generating a set containing two different values ofthe same attribute (such asoutlook =sunnyand outlook =overcast),because that cannot occur in any actualinstance.Assume that we seek association rules with minimum coverage 2:thus wediscard any item sets that cover fewer than two instances.This leaves 47 two-item sets,some ofwhich are shown in the second column along with thenumber oftimes they appear.The next step is to generate the three-item sets,ofwhich 39 have a coverage of2 or greater.There are 6 four-item sets,and noﬁve-item sets—for this data,a ﬁve-item set with coverage 2 or greater could onlycorrespond to a repeated instance.The ﬁrst row ofthe table,for example,showsthat there are ﬁve days when outlook=sunny,two ofwhich have temperature=mild,and,in fact,on both ofthose days humidity=highand play=noas well.Association rulesShortly we will explain how to generate these item sets efﬁciently.But ﬁrst letus ﬁnish the story.Once all item sets with the required coverage have been gen-erated,the next step is to turn each into a rule,or set ofrules,with at least thespeciﬁed minimum accuracy.Some item sets will produce more than one rule;others will produce none.For example,there is one three-item set with a cov-erage of4 (row 38 ofTable 4.10):humidity =normal, windy =false, play =yesThis set leads to seven potential rules:P088407-Ch004.qxd  4/30/05  11:13 AM  Page 113114CHAPTER 4|ALGORITHMS:THE BASIC METHODSTable 4.10Item sets for the weather data with coverage 2 or greater.One-item setsTwo-item setsThree-item setsFour-item sets1outlook =sunny (5)outlook =sunnyoutlook =sunnyoutlook =sunnytemperature =mild (2)temperature =hottemperature =hothumidity =high (2)humidity =highplay =no (2)2outlook =overcast (4)outlook =sunnyoutlook =sunnyoutlook =sunnytemperature =hot (2)temperature =hothumidity =highplay =no (2)windy =falseplay =no (2)3outlook =rainy (5)outlook =sunnyoutlook =sunnyoutlook =overcasthumidity =normal (2)humidity =normaltemperature =hotplay =yes (2)windy =falseplay =yes (2)4temperature =cool (4)outlook =sunnyoutlook =sunnyoutlook =rainyhumidity =high (3)humidity =hightemperature =mildwindy =false (2)windy =falseplay =yes (2)5temperature =mild (6)outlook =sunnyoutlook =sunnyoutlook =rainywindy =true (2)humidity =highhumidity =normalplay =no (3)windy =falseplay =yes (2)6temperature =hot (4)outlook =sunnyoutlook =sunnytemperature =coolwindy =false (3)windy =falsehumidity =normalplay =no (2)windy =falseplay =yes (2)7humidity =normal (7)outlook =sunnyoutlook =overcastplay =yes (2)temperature =hotwindy =false (2)8humidity =high (7)outlook =sunnyoutlook =overcastplay =no (3)temperature =hotplay =yes (2)9windy =true (6)outlook =overcastoutlook =overcasttemperature =hot (2)humidity =normalplay =yes (2)10windy =false (8)outlook =overcastoutlook =overcasthumidity =normal (2)humidity =highplay =yes (2)11play =yes (9)outlook =overcastoutlook =overcasthumidity =high (2)windy =trueplay =yes (2)12play =no (5)outlook =overcastoutlook =overcastwindy =true (2)windy =falseplay =yes (2)13outlook =overcastoutlook =rainywindy =false (2)temperature =coolhumidity =normal (2)P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1144.5MINING ASSOCIATION RULES115If humidity =normal and windy =false then play =yes4/4If humidity =normal and play =yes then windy =false4/6If windy =false and play =yes then humidity =normal4/6If humidity =normal then windy =false and play =yes4/7If windy =false then humidity =normal and play =yes4/8If play =yes then humidity =normal and windy =false4/9If – then humidity =normal and windy =false and play =yes4/12The ﬁgures at the right show the number ofinstances for which all three con-ditions are true—that is,the coverage—divided by the number ofinstances forwhich the conditions in the antecedent are true.Interpreted as a fraction,theyrepresent the proportion ofinstances on which the rule is correct—that is,itsaccuracy.Assuming that the minimum speciﬁed accuracy is 100%,only the ﬁrstofthese rules will make it into the ﬁnal rule set.The denominators ofthe frac-tions are readily obtained by looking up the antecedent expression in Table 4.10(though some are not shown in the Table).The ﬁnal rule above has no condi-tions in the antecedent,and its denominator is the total number ofinstances inthe dataset.Table 4.11 shows the ﬁnal rule set for the weather data,with minimum cov-erage 2 and minimum accuracy 100%,sorted by coverage.There are 58 rules,3with coverage 4,5 with coverage 3,and 50 with coverage 2.Only 7 have twoconditions in the consequent,and none has more than two.The ﬁrst rule comesfrom the item set described previously.Sometimes several rules arise from thesame item set.For example,rules 9,10,and 11 all arise from the four-item setin row 6 ofTable 4.10:temperature =cool, humidity =normal, windy =false, play =yesTable 4.10(continued)One-item setsTwo-item setsThree-item setsFour-item sets.........38humidity =normalhumidity =normalwindy =false (4)windy =falseplay =yes (4)39humidity =normalhumidity =highplay =yes (6)windy =falseplay =no (2)40humidity =highwindy =true (3)......47windy =falseplay =no (2)P088407-Ch004.qxd  4/30/05  11:13 AM  Page 115116CHAPTER 4|ALGORITHMS:THE BASIC METHODSTable 4.11Association rules for the weather data.Association ruleCoverageAccuracy1humidity =normal windy =falseﬁplay =yes4100%2temperature =coolﬁhumidity =normal4100%3outlook =overcastﬁplay =yes4100%4temperature =cool play =yesﬁhumidity =normal3100%5outlook =rainy windy =falseﬁplay =yes3100%6outlook =rainy play =yesﬁwindy =false3100%7outlook =sunny humidity =highﬁplay =no3100%8outlook =sunny play =noﬁhumidity =high3100%9temperature =cool windy =falseﬁhumidity =normal2100%play =yes10temperature =cool humidity =normal windyﬁplay =yes2100%=false11temperature =cool windy =false play =yesﬁhumidity =normal2100%12outlook =rainy humidity =normal windy ﬁplay =yes2100%=false13outlook =rainy humidity =normal play =yesﬁwindy =false2100%14outlook =rainy temperature =mild windy ﬁplay =yes2100%=false15outlook =rainy temperature =mild play =yesﬁwindy =false2100%16temperature =mild windy =false play =yesﬁoutlook =rainy2100%17outlook =overcast temperature =hotﬁwindy =false2100%play =yes18outlook =overcast windy =falseﬁtemperature =hot2100%play =yes19temperature =hot play =yesﬁoutlook =overcast2100%windy =false20outlook =overcast temperature =hot windyﬁplay =yes2100%=false21outlook =overcast temperature =hot play ﬁwindy =false2100%=yes22outlook =overcast windy =false play =yesﬁtemperature =hot2100%23temperature =hot windy =false play =yesﬁoutlook =overcast2100%24windy =false play =noﬁoutlook =sunny2100%humidity =high25outlook =sunny humidity =high windy =falseﬁplay =no2100%26outlook =sunny windy =false play =noﬁhumidity =high2100%27humidity =high windy =false play =noﬁoutlook =sunny2100%28outlook =sunny temperature =hotﬁhumidity =high2100%play =no29temperature =hot play =noﬁoutlook =sunny2100%humidity =high30outlook =sunny temperature =hot humidity ﬁplay =no2100%=high31outlook =sunny temperature =hot play =noﬁhumidity =high2100%............58outlook =sunny temperature =hotﬁhumidity =high2100%P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1164.5MINING ASSOCIATION RULES117which has coverage 2.Three subsets ofthis item set also have coverage 2:temperature =cool, windy =falsetemperature =cool, humidity =normal, windy =falsetemperature =cool, windy =false, play =yesand these lead to rules 9,10,and 11,all ofwhich are 100% accurate (on thetraining data).Generating rules efﬁcientlyWe now consider in more detail an algorithm for producing association ruleswith speciﬁed minimum coverage and accuracy.There are two stages:generat-ing item sets with the speciﬁed minimum coverage,and from each item setdetermining the rules that have the speciﬁed minimum accuracy.The ﬁrst stage proceeds by generating all one-item sets with the givenminimum coverage (the ﬁrst column ofTable 4.10) and then using this to gen-erate the two-item sets (second column),three-item sets (third column),and soon.Each operation involves a pass through the dataset to count the items ineach set,and after the pass the surviving item sets are stored in a hash table—a standard data structure that allows elements stored in it to be found veryquickly.From the one-item sets,candidate two-item sets are generated,and thena pass is made through the dataset,counting the coverage ofeach two-item set;at the end the candidate sets with less than minimum coverage are removedfrom the table.The candidate two-item sets are simply all ofthe one-item setstaken in pairs,because a two-item set cannot have the minimum coverage unlessboth its constituent one-item sets have minimum coverage,too.This applies ingeneral:a three-item set can only have the minimum coverage ifall three ofitstwo-item subsets have minimum coverage as well,and similarly for four-itemsets.An example will help to explain how candidate item sets are generated.Suppose there are ﬁve three-item sets—(A B C),(A B D),(A C D),(A C E),and(B C D)—where,for example,A is a feature such as outlook=sunny.The unionofthe ﬁrst two,(A B C D),is a candidate four-item set because its other three-item subsets (A C D) and (B C D) have greater than minimum coverage.Ifthethree-item sets are sorted into lexical order,as they are in this list,then we needonly consider pairs whose ﬁrst two members are the same.For example,we donot consider (A C D) and (B C D) because (A B C D) can also be generatedfrom (A B C) and (A B D),and ifthese two are not candidate three-item setsthen (A B C D) cannot be a candidate four-item set.This leaves the pairs (A BC) and (A B D),which we have already explained,and (A C D) and (A C E).This second pair leads to the set (A C D E) whose three-item subsets do not allhave the minimum coverage,so it is discarded.The hash table assists with thischeck:we simply remove each item from the set in turn and check that theP088407-Ch004.qxd  4/30/05  11:13 AM  Page 117remaining three-item set is indeed present in the hash table.Thus in thisexample there is only one candidate four-item set,(A B C D).Whether or notit actually has minimum coverage can only be determined by checking theinstances in the dataset.The second stage ofthe procedure takes each item set and generates rulesfrom it,checking that they have the speciﬁed minimum accuracy.Ifonly ruleswith a single test on the right-hand side were sought,it would be simply a matterofconsidering each condition in turn as the consequent ofthe rule,deleting itfrom the item set,and dividing the coverage ofthe entire item set by the cov-erage ofthe resulting subset—obtained from the hash table—to yield the accu-racy ofthe corresponding rule.Given that we are also interested in associationrules with multiple tests in the consequent,it looks like we have to evaluate theeffect ofplacing each subsetofthe item set on the right-hand side,leaving theremainder ofthe set as the antecedent.This brute-force method will be excessively computation intensive unlessitem sets are small,because the number ofpossible subsets grows exponentiallywith the size ofthe item set.However,there is a better way.We observed whendescribing association rules in Section 3.4 that ifthe double-consequent ruleIf windy =false and play =no then outlook =sunnyand humidity =highholds with a given minimum coverage and accuracy,then both single-consequent rules formed from the same item set must also hold:If humidity =high and windy =false and play =nothen outlook =sunnyIf outlook =sunny and windy =false and play =nothen humidity =highConversely,ifone or other ofthe single-consequent rules does not hold,thereis no point in considering the double-consequent one.This gives a way ofbuild-ing up from single-consequent rules to candidate double-consequent ones,fromdouble-consequent rules to candidate triple-consequent ones,and so on.Ofcourse,each candidate rule must be checked against the hash table to see ifitreally does have more than the speciﬁed minimum accuracy.But this generallyinvolves checking far fewer rules than the brute force method.It is interestingthat this way ofbuilding up candidate (n+1)-consequent rules from actual n-consequent ones is really just the same as building up candidate (n+1)-itemsets from actual n-item sets,described earlier.DiscussionAssociation rules are often sought for very large datasets,and efﬁcient algo-rithms are highly valued.The method described previously makes one pass118CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1184.6LINEAR MODELS119through the dataset for each different size ofitem set.Sometimes the dataset istoo large to read in to main memory and must be kept on disk;then it may beworth reducing the number ofpasses by checking item sets oftwo consecutivesizes in one go.For example,once sets with two items have been generated,allsets ofthree items could be generated from them before going through theinstance set to count the actual number ofitems in the sets.More three-itemsets than necessary would be considered,but the number ofpasses through theentire dataset would be reduced.In practice,the amount ofcomputation needed to generate association rulesdepends critically on the minimum coverage speciﬁed.The accuracy has lessinﬂuence because it does not affect the number ofpasses that we must makethrough the dataset.In many situations we will want to obtain a certain num-ber ofrules—say 50—with the greatest possible coverage at a prespeciﬁedminimum accuracy level.One way to do this is to begin by specifying the cov-erage to be rather high and to then successively reduce it,reexecuting the entirerule-ﬁnding algorithm for each coverage value and repeating this until thedesired number ofrules has been generated.The tabular input format that we use throughout this book,and in particu-lar a standard ARFF ﬁle based on it,is very inefﬁcient for many association-ruleproblems.Association rules are often used when attributes are binary—eitherpresent or absent—and most ofthe attribute values associated with a giveninstance are absent.This is a case for the sparse data representation describedin Section 2.4;the same algorithm for ﬁnding association rules applies.4.6Linear modelsThe methods we have been looking at for decision trees and rules work mostnaturally with nominal attributes.They can be extended to numeric attributeseither by incorporating numeric-value tests directly into the decision tree or ruleinduction scheme,or by prediscretizing numeric attributes into nominal ones.We will see how in Chapters 6 and 7,respectively.However,there are methodsthat work most naturally with numeric attributes.We look at simple ones here,ones that form components ofmore complex learning methods,which we willexamine later.Numeric prediction: Linear regressionWhen the outcome,or class,is numeric,and all the attributes are numeric,linearregression is a natural technique to consider.This is a staple method in statis-tics.The idea is to express the class as a linear combination ofthe attributes,with predetermined weights:xwwawawakk=++++01122...P088407-Ch004.qxd  4/30/05  11:13 AM  Page 119where xis the class;a1,a2,...,akare the attribute values;and w0,w1,...,wkareweights.The weights are calculated from the training data.Here the notation gets alittle heavy,because we need a way ofexpressing the attribute values for eachtraining instance.The ﬁrst instance will have a class,say x(1),and attribute valuesa1(1),a2(1),...,ak(1),where the superscript denotes that it is the ﬁrst example.Moreover,it is notationally convenient to assume an extra attribute a0whosevalue is always 1.The predicted value for the ﬁrst instance’s class can be written asThis is the predicted,not the actual,value for the ﬁrst instance’s class.Ofinter-est is the difference between the predicted and the actual values.The method oflinear regression is to choose the coefﬁcients wj—there are k+1 ofthem—tominimize the sum ofthe squares ofthese differences over all the traininginstances.Suppose there are ntraining instances;denote the ith one with asuperscript (i).Then the sum ofthe squares ofthe differences iswhere the expression inside the parentheses is the difference between the ithinstance’s actual class and its predicted class.This sum ofsquares is what wehave to minimize by choosing the coefﬁcients appropriately.This is all starting to look rather formidable.However,the minimizationtechnique is straightforward ifyou have the appropriate math background.Sufﬁce it to say that given enough examples—roughly speaking,more examplesthan attributes—choosing weights to minimize the sum ofthe squared differ-ences is really not difﬁcult.It does involve a matrix inversion operation,but thisis readily available as prepackaged software.Once the math has been accomplished,the result is a set ofnumeric weights,based on the training data,which we can use to predict the class ofnewinstances.We saw an example ofthis when looking at the CPU performancedata,and the actual numeric weights are given in Figure 3.7(a).This formulacan be used to predict the CPU performance ofnew test instances.Linear regression is an excellent,simple method for numeric prediction,andit has been widely used in statistical applications for decades.Ofcourse,linearmodels suffer from the disadvantage of,well,linearity.Ifthe data exhibits a non-linear dependency,the best-ﬁtting straight line will be found,where “best”isinterpreted as the least mean-squared difference.This line may not ﬁt very well.xwaijjijkin()()==-ÊËÁˆ¯˜ÂÂ012wawawawawakkjjjk001111221110()()()()()=++++=Â....120CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1204.6LINEAR MODELS121However,linear models serve well as building blocks for more complex learn-ing methods.Linear classiﬁcation: Logistic regressionLinear regression can easily be used for classiﬁcation in domains with numericattributes.Indeed,we can use anyregression technique,whether linear or non-linear,for classiﬁcation.The trick is to perform a regression for each class,setting the output equal to one for training instances that belong to the classand zero for those that do not.The result is a linear expression for the class.Then,given a test example ofunknown class,calculate the value ofeachlinear expression and choose the one that is largest.This method is sometimescalled multiresponse linear regression.One way oflooking at multiresponse linear regression is to imagine that itapproximates a numeric membership functionfor each class.The membershipfunction is 1 for instances that belong to that class and 0 for other instances.Given a new instance we calculate its membership for each class and select thebiggest.Multiresponse linear regression often yields good results in practice.However,it has two drawbacks.First,the membership values it produces are notproper probabilities because they can fall outside the range 0 to 1.Second,least-squares regression assumes that the errors are not only statistically independ-ent,but are also normally distributed with the same standard deviation,anassumption that is blatantly violated when the method is applied to classiﬁca-tion problems because the observations only ever take on the values 0 and 1.A related statistical technique called logistic regressiondoes not suffer fromthese problems.Instead ofapproximating the 0 and 1 values directly,therebyrisking illegitimate probability values when the target is overshot,logistic regres-sion builds a linear model based on a transformed target variable.Suppose ﬁrst that there are only two classes.Logistic regression replaces theoriginal target variablewhich cannot be approximated accurately using a linear function,withThe resulting values are no longer constrained to the interval from 0 to 1 butcan lie anywhere between negative inﬁnity and positive inﬁnity.Figure 4.9(a)plots the transformation function,which is often called the logit transformation.The transformed variable is approximated using a linear function just likethe ones generated by linear regression.The resulting model isPr11112011aaawwawakkk,,...,exp...,[]=+----()()logPrPr1111212aaaaaakk,,...,,,...,.[]()-[]()Pr112aaak,,...,,[]P088407-Ch004.qxd  4/30/05  11:13 AM  Page 121with weights w.Figure 4.9(b) shows an example ofthis function in one dimen-sion,with two weights w0=0.5 and w1=1.Just as in linear regression,weights must be found that ﬁt the training datawell.Linear regression measures the goodness ofﬁt using the squared error.Inlogistic regression the log-likelihoodofthe model is used instead.This is givenby122CHAPTER 4|ALGORITHMS:THE BASIC METHODS–5–4–3–2–101234500.20.40.60.81(a)00.20.40.60.81–10–50510(b)Figure 4.9Logistic regression:(a) the logit transform and (b) an example logistic regres-sion function.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1224.6LINEAR MODELS123where the x(i)are either zero or one.The weights wineed to be chosen to maximize the log-likelihood.There areseveral methods for solving this maximization problem.A simple one is to iteratively solve a sequence ofweighted least-squares regression problems untilthe log-likelihood converges to a maximum,which usually happens in a few iterations.To generalize logistic regression to several classes,one possibility is to proceedin the way described previously for multiresponse linear regression by per-forming logistic regression independently for each class.Unfortunately,theresulting probability estimates will not sum to one.To obtain proper probabil-ities it is necessary to couple the individual models for each class.This yields ajoint optimization problem,and there are efﬁcient solution methods for this.A conceptually simpler,and very general,way to address multiclass problemsis known as pairwise classiﬁcation.Here a classiﬁer is built for every pair ofclasses,using only the instances from these two classes.The output on anunknown test example is based on which class receives the most votes.Thismethod generally yields accurate results in terms ofclassiﬁcation error.It canalso be used to produce probability estimates by applying a method called pair-wise coupling,which calibrates the individual probability estimates from the dif-ferent classiﬁers.Ifthere are kclasses,pairwise classiﬁcation builds a total ofk(k-1)/2 clas-siﬁers.Although this sounds unnecessarily computation intensive,it is not.Infact,ifthe classes are evenly populated pairwise classiﬁcation is at least as fastas any other multiclass method.The reason is that each ofthe pairwise learn-ing problem only involves instances pertaining to the two classes under consid-eration.Ifninstances are divided evenly among kclasses,this amounts to 2n/kinstances per problem.Suppose the learning algorithm for a two-class problemwith ninstances takes time proportional to nseconds to execute.Then the runtime for pairwise classiﬁcation is proportional to k(k-1)/2 ¥2n/kseconds,which is (k-1)n.In other words,the method scales linearly with the numberofclasses.Ifthe learning algorithm takes more time—say proportional to n2—the advantage ofthe pairwise approach becomes even more pronounced.The use oflinear functions for classiﬁcation can easily be visualized ininstance space.The decision boundary for two-class logistic regression lieswhere the prediction probability is 0.5,that is:This occurs whenPr1110512011aaawwawakkk,,...,exp.....[]=+----()()=111111212-()-[]()+[]()()=()()()()()()()Âxaaaxaaaiiniikiiiikilog,,...,log,,...,PrPrP088407-Ch004.qxd  4/30/05  11:13 AM  Page 123Because this is a linear equality in the attribute values,the boundary is a linearplane,or hyperplane,in instance space.It is easy to visualize sets ofpoints thatcannot be separated by a single hyperplane,and these cannot be discriminatedcorrectly by logistic regression.Multiresponse linear regression suffers from the same problem.Each classreceives a weight vector calculated from the training data.Focus for the momenton a particular pair ofclasses.Suppose the weight vector for class 1 isand the same for class 2 with appropriate superscripts.Then,an instance willbe assigned to class 1 rather than class 2 ifIn other words,it will be assigned to class 1 ifThis is a linear inequality in the attribute values,so the boundary between eachpair ofclasses is a hyperplane.The same holds true when performing pairwiseclassiﬁcation.The only difference is that the boundary between two classes isgoverned by the training instances in those classes and is not inﬂuenced by theother classes.Linear classiﬁcation using the perceptronLogistic regression attempts to produce accurate probability estimates by max-imizing the probability ofthe training data.Ofcourse,accurate probability esti-mates lead to accurate classiﬁcations.However,it is not necessary to performprobability estimation ifthe sole purpose ofthe model is to predict class labels.A different approach is to learn a hyperplane that separates the instances per-taining to the different classes—let’s assume that there are only two ofthem.Ifthe data can be separated perfectly into two groups using a hyperplane,it is saidto be linearly separable.It turns out that ifthe data is linearly separable,thereis a very simple algorithm for ﬁnding a separating hyperplane.The algorithm is called the perceptron learning rule.Before looking at it indetail,let’s examine the equation for a hyperplane again:Here,a1,a2,...,akare the attribute values,and w0,w1,...,wkare the weightsthat deﬁne the hyperplane.We will assume that each training instance a1,a2,...is extended by an additional attribute a0that always has the value 1 (as wedid in the case oflinear regression).This extension,which is called the bias,justwawawawakk0011220++++=....wwwwawwakkk010211121120()()()()()()-()+-()++-()>....wwawawwawakkkk011111021212()()()()()()+++>+++......wwawawakk011112121()()()()++++...----=wwawakk0110....124CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1244.6LINEAR MODELS125means that we don’t have to include an additional constant element in the sum.Ifthe sum is greater than zero,we will predict the ﬁrst class;otherwise,we willpredict the second class.We want to ﬁnd values for the weights so that the train-ing data is correctly classiﬁed by the hyperplane.Figure 4.10(a) gives the perceptron learning rule for ﬁnding a separatinghyperplane.The algorithm iterates until a perfect solution has been found,butit will only work properly ifa separating hyperplane exists,that is,ifthe data islinearly separable.Each iteration goes through all the training instances.Ifamisclassiﬁed instance is encountered,the parameters ofthe hyperplane arechanged so that the misclassiﬁed instance moves closer to the hyperplane ormaybe even across the hyperplane onto the correct side.Ifthe instance belongsto the ﬁrst class,this is done by adding its attribute values to the weight vector;otherwise,they are subtracted from it.Set all weights to zeroUntil all instances in the training data are classified correctly  For each instance I in the training data    If I is classified incorrectly by the perceptron      If I belongs to the first class add it to the weight vector      else subtract it from the weight vector(a) 1(“bias”)attributea1attributea2attributea3w0w2w1wk(b)Figure 4.10The perceptron:(a) learning rule and (b) representation as a neural network.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 125To see why this works,consider the situation after an instance apertainingto the ﬁrst class has been added:This means the output for ahas increased byThis number is always positive.Thus the hyperplane has moved in the correctdirection for classifying instance aas positive.Conversely,ifan instance belong-ing to the second class is misclassiﬁed,the output for that instance decreasesafter the modiﬁcation,again moving the hyperplane to the correct direction.These corrections are incremental and can interfere with earlier updates.However,it can be shown that the algorithm converges in a ﬁnite number ofiterations ifthe data is linearly separable.Ofcourse,ifthe data is not linearlyseparable,the algorithm will not terminate,so an upper bound needs to beimposed on the number ofiterations when this method is applied in practice.The resulting hyperplane is called a perceptron,and it’s the grandfather ofneural networks (we return to neural networks in Section 6.3).Figure 4.10(b)represents the perceptron as a graph with nodes and weighted edges,imagina-tively termed a “network”of“neurons.”There are two layers ofnodes:input andoutput.The input layer has one node for every attribute,plus an extra node thatis always set to one.The output layer consists ofjust one node.Every node inthe input layer is connected to the output layer.The connections are weighted,and the weights are those numbers found by the perceptron learning rule.When an instance is presented to the perceptron,its attribute values serve to“activate”the input layer.They are multiplied by the weights and summed upat the output node.Ifthe weighted sum is greater than 0 the output signal is 1,representing the ﬁrst class;otherwise,it is -1,representing the second.Linear classiﬁcation using WinnowThe perceptron algorithm is not the only method that is guaranteed to ﬁnd aseparating hyperplane for a linearly separable problem.For datasets with binaryattributes there is an alternative known as Winnow,shown in Figure 4.11(a).The structure ofthe two algorithms is very similar.Like the perceptron,Winnowonly updates the weight vector when a misclassiﬁed instance is encountered—it is mistake driven.The two methods differ in how the weights are updated.The perceptron ruleemploys an additive mechanism that alters the weight vector by adding (or sub-tracting) the instance’s attribute vector.Winnow employs multiplicative updatesand alters weights individually by multiplying them by the user-speciﬁedparameter a(or its inverse).The attribute values aiare either 0 or 1 because weaaaaaaaakk001122¥+¥+¥++¥....waawaawaawaakkk000111222+()++()++()+++()....126CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1264.6LINEAR MODELS127While some instances are misclassified      for every instance a        classify a using the current weights        if the predicted class is incorrect          if a belongs to the first class            for each ai that is 1, multiply wi by a            (if ai is 0, leave wi unchanged)          otherwise            for each ai that is 1, divide wi by a            (if ai is 0, leave wi unchanged)(a)    While some instances are misclassified      for every instance a        classify a using the current weights        if the predicted class is incorrect          if a belongs to the first class            for each ai that is 1,              multiply wi+ by a              divide wi– by a            (if ai is 0, leave wi+ and wi- unchanged)          otherwise for             for each ai that is 1,              multiply wi– by a              divide wi+ by a            (if ai is 0, leave wi+ and wi- unchanged)(b)Figure 4.11The Winnow algorithm:(a) the unbalanced version and (b) the balancedversion.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 127are working with binary data.Weights are unchanged ifthe attribute value is 0,because then they do not participate in the decision.Otherwise,the multiplieris aifthat attribute helps to make a correct decision and 1/aifit does not.Another difference is that the threshold in the linear function is also a user-speciﬁed parameter.We call this threshold qand classify an instance as belong-ing to class 1 ifand only ifThe multiplier aneeds to be greater than one.The wiare set to a constant atthe start.The algorithm we have described doesn’t allow negative weights,which—depending on the domain—can be a drawback.However,there is a version,called Balanced Winnow,which does allow them.This version maintains twoweight vectors,one for each class.An instance is classiﬁed as belonging to class1 if:Figure 4.11(b) shows the balanced algorithm.Winnow is very effective in homing in on the relevant features in a dataset—therefore it is called an attribute-efﬁcientlearner.That means that it may be agood candidate algorithm ifa dataset has many (binary) features and most ofthem are irrelevant.Both winnow and the perceptron algorithm can be used inan online setting in which new instances arrive continuously,because they canincrementally update their hypotheses as new instances arrive.4.7Instance-based learningIn instance-based learning the training examples are stored verbatim,and a dis-tance function is used to determine which member ofthe training set is closestto an unknown test instance.Once the nearest training instance has beenlocated,its class is predicted for the test instance.The only remaining problemis deﬁning the distance function,and that is not very difﬁcult to do,particularlyifthe attributes are numeric.The distance functionAlthough there are other possible choices,most instance-based learners useEuclidean distance.The distance between an instance with attribute values a1(1),a2(1),...,ak(1)(where kis the number ofattributes) and one with values a1(2),a2(2),...,ak(2)is deﬁned asaaaaaakk1112221222122()()()()()()-()+-()++-()....wwawwawwakkk000111+-+-+--()+-()++-()>...qwawawawakk001122++++>....q128CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1284.7INSTANCE-BASED LEARNING129When comparing distances it is not necessary to perform the square root oper-ation;the sums ofsquares can be compared directly.One alternative to theEuclidean distance is the Manhattan or city-block metric,where the differencebetween attribute values is not squared but just added up (after taking theabsolute value).Others are obtained by taking powers higher than the square.Higher powers increase the inﬂuence oflarge differences at the expense ofsmalldifferences.Generally,the Euclidean distance represents a good compromise.Other distance metrics may be more appropriate in special circumstances.Thekey is to think ofactual instances and what it means for them to be separatedby a certain distance—what would twice that distance mean,for example?Different attributes are measured on different scales,so ifthe Euclidean distance formula were used directly,the effects ofsome attributes might be completely dwarfed by others that had larger scales ofmeasurement.Conse-quently,it is usual to normalize all attribute values to lie between 0 and 1,bycalculatingwhere viis the actual value ofattribute i,and the maximum and minimum aretaken over all instances in the training set.These formulae implicitly assume numeric attributes.Here,the differencebetween two values is just the numerical difference between them,and it is thisdifference that is squared and added to yield the distance function.For nominalattributes that take on values that are symbolic rather than numeric,the differ-ence between two values that are not the same is often taken to be one,whereasifthe values are the same the difference is zero.No scaling is required in thiscase because only the values 0 and 1 are used.A common policy for handling missing values is as follows.For nominalattributes,assume that a missing feature is maximally different from any otherfeature value.Thus ifeither or both values are missing,or ifthe values are dif-ferent,the difference between them is taken as one;the difference is zero onlyifthey are not missing and both are the same.For numeric attributes,the dif-ference between two missing values is also taken as one.However,ifjust onevalue is missing,the difference is often taken as either the (normalized) size ofthe other value or one minus that size,whichever is larger.This means that ifvalues are missing,the difference is as large as it can possibly be.Finding nearest neighbors efﬁcientlyAlthough instance-based learning is simple and effective,it is often slow.Theobvious way to ﬁnd which member ofthe training set is closest to an unknowntest instance is to calculate the distance from every member ofthe training setavvvviiiii=--minmaxminP088407-Ch004.qxd  4/30/05  11:13 AM  Page 129and select the smallest.This procedure is linear in the number oftraininginstances:in other words,the time it takes to make a single prediction is pro-portional to the number oftraining instances.Processing an entire test set takestime proportional to the product ofthe number ofinstances in the training andtest sets.Nearest neighbors can be found more efﬁciently by representing the trainingset as a tree,although it is not quite obvious how.One suitable structure is akD-tree.This is a binary tree that divides the input space with a hyperplane and then splits each partition again,recursively.All splits are made parallel toone ofthe axes,either vertically or horizontally,in the two-dimensional case.The data structure is called a kD-treebecause it stores a set ofpoints in k-dimensional space,kbeing the number ofattributes.Figure 4.12(a) gives a small example with k=2,and Figure 4.12(b) shows thefour training instances it represents,along with the hyperplanes that constitutethe tree.Note that these hyperplanes are notdecision boundaries:decisions aremade on a nearest-neighbor basis as explained later.The ﬁrst split is horizon-tal (h),through the point (7,4)—this is the tree’s root.The left branch is notsplit further:it contains the single point (2,2),which is a leafofthe tree.Theright branch is split vertically (v) at the point (6,7).Its left child is empty,andits right child contains the point (3,8).As this example illustrates,each regioncontains just one point—or,perhaps,no points.Sibling branches ofthe tree—for example,the two daughters ofthe root in Figure 4.12(a)—are not neces-sarily developed to the same depth.Every point in the training set correspondsto a single node,and up to halfare leafnodes.130CHAPTER 4|ALGORITHMS:THE BASIC METHODS(2,2)(7,4); h(6,7); v(3,8)(a)aa(2,2)(7,4)(6,7)(3,8)21(b)Figure 4.12A kD-tree for four training instances:(a) the tree and (b) instances andsplits.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1304.7INSTANCE-BASED LEARNING131How do you build a kD-tree from a dataset? Can it be updated efﬁciently asnew training examples are added? And how does it speed up nearest-neighborcalculations? We tackle the last question ﬁrst.To locate the nearest neighbor ofa given target point,follow the tree downfrom its root to locate the region containing the target.Figure 4.13 shows a spacelike that ofFigure 4.12(b) but with a few more instances and an extra bound-ary.The target,which is not one ofthe instances in the tree,is marked by a star.The leafnode ofthe region containing the target is colored black.This is notnecessarily the target’s closest neighbor,as this example illustrates,but it is agood ﬁrst approximation.In particular,any nearer neighbor must lie closer—within the dashed circle in Figure 4.13.To determine whether one exists,ﬁrstcheck whether it is possible for a closer neighbor to lie within the node’s sibling.The black node’s sibling is shaded in Figure 4.13,and the circle does not inter-sect it,so the sibling cannot contain a closer neighbor.Then back up to theparent node and check itssibling—which here covers everything above the hor-izontal line.In this case it must be explored,because the area it covers intersectswith the best circle so far.To explore it,ﬁnd its daughters (the original point’stwo aunts),check whether they intersect the circle (the left one does not,butthe right one does),and descend to see whether it contains a closer point (itdoes).Figure 4.13Using a kD-tree to ﬁnd the nearest neighbor ofthe star.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 131In a typical case,this algorithm is far faster than examining all points to ﬁndthe nearest neighbor.The work involved in ﬁnding the initial approximatenearest neighbor—the black point in Figure 4.13—depends on the depth ofthetree,given by the logarithm ofthe number ofnodes,log2n.The amount ofworkinvolved in backtracking to check whether this really is the nearest neighbordepends a bit on the tree,and on how good the initial approximation is.But fora well-constructed tree whose nodes are approximately square,rather than longskinny rectangles,it can also be shown to be logarithmic in the number ofnodes.How do you build a good tree for a set oftraining examples? The problemboils down to selecting the ﬁrst training instance to split at and the direction ofthe split.Once you can do that,apply the same method recursively to each childofthe initial split to construct the entire tree.To ﬁnd a good direction for the split,calculate the variance ofthe data pointsalong each axis individually,select the axis with the greatest variance,and createa splitting hyperplane perpendicular to it.To ﬁnd a good place for the hyper-plane,locate the median value along that axis and select the correspondingpoint.This makes the split perpendicular to the direction ofgreatest spread,with halfthe points lying on either side.This produces a well-balanced tree.Toavoid long skinny regions it is best for successive splits to be along different axes,which is likely because the dimension ofgreatest variance is chosen at each stage.However,ifthe distribution ofpoints is badly skewed,choosing the medianvalue may generate several successive splits in the same direction,yielding long,skinny hyperrectangles.A better strategy is to calculate the mean rather than themedian and use the point closest to that.The tree will not be perfectly balanced,but its regions will tend to be squarish because there is a greater chance that dif-ferent directions will be chosen for successive splits.An advantage ofinstance-based learning over most other machine learningmethods is that new examples can be added to the training set at any time.Toretain this advantage when using a kD-tree,we need to be able to update it incre-mentally with new data points.To do this,determine which leafnode containsthe new point and ﬁnd its hyperrectangle.Ifit is empty,simply place the newpoint there.Otherwise split the hyperrectangle,splitting it along its longestdimension to preserve squareness.This simple heuristic does not guarantee thatadding a series ofpoints will preserve the tree’s balance,nor that the hyperrec-tangles will be well shaped for nearest-neighbor search.It is a good idea torebuild the tree from scratch occasionally—for example,when its depth growsto twice the best possible depth.As we have seen,kD-trees are good data structures for ﬁnding nearest neigh-bors efﬁciently.However,they are not perfect.Skewed datasets present a basicconﬂict between the desire for the tree to be perfectly balanced and the desirefor regions to be squarish.More importantly,rectangles—even squares—are notthe best shape to use anyway,because oftheir corners.Ifthe dashed circle in132CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 1324.7INSTANCE-BASED LEARNING133Figure 4.13 were any bigger,which it would be ifthe black instance were a littlefurther from the target,it would intersect the lower right-hand corner oftherectangle at the top left and then that rectangle would have to be investigated,too—despite the fact that the training instances that deﬁne it are a long wayfrom the corner in question.The corners ofrectangular regions are awkward.The solution? Use hyperspheres,not hyperrectangles.Neighboring spheresmay overlap whereas rectangles can abut,but this is not a problem because thenearest-neighbor algorithm for kD-trees described previously does not dependon the regions being disjoint.A data structure called a ball treedeﬁnes k-dimensional hyperspheres (“balls”) that cover the data points,and arrangesthem into a tree.Figure 4.14(a) shows 16 training instances in two-dimensional space,over-laid by a pattern ofoverlapping circles,and Figure 4.14(b) shows a tree formedfrom these circles.Circles at different levels ofthe tree are indicated by differ-ent styles ofdash,and the smaller circles are drawn in shades ofgray.Each nodeofthe tree represents a ball,and the node is dashed or shaded according to thesame convention so that you can identify which level the balls are at.To helpyou understand the tree,numbers are placed on the nodes to show how manydata points are deemed to be inside that ball.But be careful:this is not neces-sarily the same as the number ofpoints falling within the spatial region that theball represents.The regions at each level sometimes overlap,but points that fallinto the overlap area are assigned to only one ofthe overlapping balls (thediagram does not show which one).Instead ofthe occupancy counts in Figure4.14(b) the nodes ofactual ball trees store the center and radius oftheir ball;leafnodes record the points they contain as well.To use a ball tree to ﬁnd the nearest neighbor to a given target,start by tra-versing the tree from the top down to locate the leafthat contains the target andﬁnd the closest point to the target in that ball.This gives an upper bound forthe target’s distance from its nearest neighbor.Then,just as for the kD-tree,examine the sibling node.Ifthe distance from the target to the sibling’s centerexceeds its radius plus the current upper bound,it cannot possibly contain acloser point;otherwise the sibling must be examined by descending the treefurther.In Figure 4.15 the target is marked with a star and the black dot is itsclosest currently known neighbor.The entire contents ofthe gray ball can beruled out:it cannot contain a closer point because its center is too far away.Proceed recursively back up the tree to its root,examining any ball that maypossibly contain a point nearer than the current upper bound.Ball trees are built from the top down,and as with kD-trees the basic problemis to ﬁnd a good way ofsplitting a ball containing a set ofdata points into two.In practice you do not have to continue until the leafballs contain just two points:you can stop earlier,once a predetermined minimum number isreached—and the same goes for kD-trees.Here is one possible splitting method.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 133134CHAPTER 4|ALGORITHMS:THE BASIC METHODS(a)16610462422422222(b)Figure 4.14Ball tree for 16 training instances:(a) instances and balls and (b) the tree.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 1344.7INSTANCE-BASED LEARNING135Choose the point in the ball that is farthest from its center,and then a secondpoint that is farthest from the ﬁrst one.Assign all data points in the ball to theclosest one ofthese two cluster centers,then compute the centroid ofeachcluster and the minimum radius required for it to enclose all the data points itrepresents.This method has the merit that the cost ofsplitting a ball contain-ing npoints is only linear in n.There are more elaborate algorithms thatproduce tighter balls,but they require more computation.We will not describesophisticated algorithms for constructing ball trees or updating them incre-mentally as new training instances are encountered.DiscussionNearest-neighbor instance-based learning is simple and often works very well.In the method described previously each attribute has exactly the sameinﬂuence on the decision,just as it does in the Naïve Bayes method.Anotherproblem is that the database can easily become corrupted by noisy exemplars.One solution is to adopt the k-nearest-neighbor strategy,where some ﬁxed,small,number kofnearest neighbors—say ﬁve—are located and used togetherto determine the class ofthe test instance through a simple majority vote.(Notethat we used kto denote the number ofattributes earlier;this is a different,inde-pendent usage.) Another way ofprooﬁng the database against noise is to choosethe exemplars that are added to it selectively and judiciously;improved proce-dures,described in Chapter 6,address these shortcomings.Figure 4.15Ruling out an entire ball (gray) based on a target point (star) and its currentnearest neighbor.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 135The nearest-neighbor method originated many decades ago,and statisticiansanalyzed k-nearest-neighbor schemes in the early 1950s.Ifthe number oftrain-ing instances is large,it makes intuitive sense to use more than one nearestneighbor,but clearly this is dangerous ifthere are few instances.It can be shownthat when kand the number nofinstances both become inﬁnite in such a waythatk/nÆ0,the probability oferror approaches the theoretical minimum forthe dataset.The nearest-neighbor method was adopted as a classiﬁcationmethod in the early 1960s and has been widely used in the ﬁeld ofpattern recog-nition for more than three decades.Nearest-neighbor classiﬁcation was notoriously slow until kD-trees began tobe applied in the early 1990s,although the data structure itselfwas developedmuch earlier.In practice,these trees become inefﬁcient when the dimension ofthe space increases and are only worthwhile when the number ofattributes issmall—up to 10.Ball trees were developed much more recently and are aninstance ofa more general structure sometimes called a metric tree.Sophisti-cated algorithms can create metric trees that deal successfully with thousandsofdimensions.Instead ofstoring all training instances,you can compress them into regions.A very simple technique,mentioned at the end ofSection 4.1,is to just recordthe range ofvalues observed in the training data for each attribute and cate-gory.Given a test instance,you work out which ranges the attribute values fallinto and choose the category with the greatest number ofcorrect ranges for thatinstance.A slightly more elaborate technique is to construct intervals for eachattribute and use the training set to count the number oftimes each class occursfor each interval on each attribute.Numeric attributes can be discretized intointervals,and “intervals”consisting ofa single point can be used for nominalones.Then,given a test instance,you can determine which intervals it residesin and classify it by voting,a method called voting feature intervals.Thesemethods are very approximate,but very fast,and can be useful for initial analy-sis oflarge datasets.4.8ClusteringClustering techniques apply when there is no class to be predicted but ratherwhen the instances are to be divided into natural groups.These clusters pre-sumably reﬂect some mechanism at work in the domain from which instancesare drawn,a mechanism that causes some instances to bear a stronger resem-blance to each other than they do to the remaining instances.Clustering natu-rally requires different techniques to the classiﬁcation and association learningmethods we have considered so far.136CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 136As we saw in Section 3.9,there are different ways in which the result ofclus-tering can be expressed.The groups that are identiﬁed may be exclusive so thatany instance belongs in only one group.Or they may be overlapping so that aninstance may fall into several groups.Or they may be probabilistic,whereby aninstance belongs to each group with a certain probability.Or they may be hier-archical,such that there is a crude division ofinstances into groups at the toplevel,and each ofthese groups is reﬁned further—perhaps all the way down toindividual instances.Really,the choice among these possibilities should be dic-tated by the nature ofthe mechanisms that are thought to underlie the partic-ular clustering phenomenon.However,because these mechanisms are rarelyknown—the very existence ofclusters is,after all,something that we’re tryingto discover—and for pragmatic reasons too,the choice is usually dictated by theclustering tools that are available.We will examine an algorithm that forms clusters in numeric domains,par-titioning instances into disjoint clusters.Like the basic nearest-neighbor methodofinstance-based learning,it is a simple and straightforward technique that has been used for several decades.In Chapter 6 we examine newer clusteringmethods that perform incremental and probabilistic clustering.Iterative distance-based clusteringThe classic clustering technique is called k-means.First,you specify in advancehow many clusters are being sought:this is the parameter k.Then kpoints arechosen at random as cluster centers.All instances are assigned to their closestcluster center according to the ordinary Euclidean distance metric.Next the cen-troid,or mean,ofthe instances in each cluster is calculated—this is the “means”part.These centroids are taken to be new center values for their respective clus-ters.Finally,the whole process is repeated with the new cluster centers.Itera-tion continues until the same points are assigned to each cluster in consecutiverounds,at which stage the cluster centers have stabilized and will remain thesame forever.This clustering method is simple and effective.It is easy to prove that choos-ing the cluster center to be the centroid minimizes the total squared distancefrom each ofthe cluster’s points to its center.Once the iteration has stabilized,each point is assigned to its nearest cluster center,so the overall effect is to min-imize the total squared distance from all points to their cluster centers.But theminimum is a local one;there is no guarantee that it is the global minimum.The ﬁnal clusters are quite sensitive to the initial cluster centers.Completely dif-ferent arrangements can arise from small changes in the initial random choice.In fact,this is true ofall practical clustering techniques:it is almost always infea-sible to ﬁnd globally optimal clusters.To increase the chance ofﬁnding a global4.8CLUSTERING137P088407-Ch004.qxd  4/30/05  11:13 AM  Page 137minimum people often run the algorithm several times with different initialchoices and choose the best ﬁnal result—the one with the smallest total squareddistance.It is easy to imagine situations in which k-means fails to ﬁnd a good cluster-ing.Consider four instances arranged at the vertices ofa rectangle in two-dimensional space.There are two natural clusters,formed by grouping togetherthe two vertices at either end ofa short side.But suppose that the two initialcluster centers happen to fall at the midpoints ofthe longsides.This forms astable conﬁguration.The two clusters each contain the two instances at eitherend ofa long side—no matter how great the difference between the long andthe short sides.Faster distance calculationsThe k-means clustering algorithm usually requires several iterations,eachinvolving ﬁnding the distance ofkcluster centers from every instance to deter-mine its cluster.There are simple approximations that speed this up consider-ably.For example,you can project the dataset and make cuts along selected axes,instead ofusing the arbitrary hyperplane divisions that are implied by choos-ing the nearest cluster center.But this inevitably compromises the quality oftheresulting clusters.Here’s a better way ofspeeding things up.Finding the closest cluster centeris not so different from ﬁnding nearest neighbors in instance-based learning.Can the same efﬁcient solutions—kD-trees and ball trees—be used? Yes! Indeedthey can be applied in an even more efﬁcient way,because in each iteration ofk-means all the data points are processed together,whereas in instance-basedlearning test instances are processed individually.First,construct a kD-tree or ball tree for all the data points,which will remainstatic throughout the clustering procedure.Each iteration ofk-means producesa set ofcluster centers,and all data points must be examined and assigned tothe nearest center.One way ofprocessing the points is to descend the tree fromthe root until reaching a leafand check each individual point in the leafto ﬁndits closest cluster center.But it may be that the region represented by a higherinterior node falls entirely within the domain ofa single cluster center.In thatcase all the data points under that node can be processed in one blow!The aim ofthe exercise,after all,is to ﬁnd new positions for the cluster centersby calculating the centroid ofthe points they contain.The centroid can be cal-culated by keeping a running vector sum ofthe points in the cluster,and a countofhow many there are so far.At the end,just divide one by the other to ﬁndthe centroid.Suppose that with each node ofthe tree we store the vector sumofthe points within that node and a count ofthe number ofpoints.Ifthe wholenode falls within the ambit ofa single cluster,the running totals for that cluster138CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 138can be updated immediately.Ifnot,look inside the node by proceeding recur-sively down the tree.Figure 4.16 shows the same instances and ball tree as Figure 4.14,but withtwo cluster centers marked as black stars.Because all instances are assigned tothe closest center,the space is divided in two by the thick line shown in Figure4.16(a).Begin at the root ofthe tree in Figure 4.16(b),with initial values for thevector sum and counts for each cluster;all initial values are zero.Proceed recur-sively down the tree.When node A is reached,all points within it lie in cluster1,so cluster 1’s sum and count can be updated with the sum and count for nodeA,and we need descend no further.Recursing back to node B,its ball straddlesthe boundary between the clusters,so its points must be examined individually.When node C is reached,it falls entirely within cluster 2;again,we can updatecluster 2 immediately and need descend no further.The tree is only examineddown to the frontier marked by the dashed line in Figure 4.16(b),and the advan-tage is that the nodes below need not be opened—at least,not on this particu-lar iteration ofk-means.Next time,the cluster centers will have changed andthings may be different.DiscussionMany variants ofthe basic k-means procedure have been developed.Someproduce a hierarchical clustering by applying the algorithm with k=2 to theoverall dataset and then repeating,recursively,within each cluster.How do you choose k?Often nothing is known about the likely number ofclusters,and the whole point ofclustering is to ﬁnd out.One way is to try dif-ferent values and choose the best.To do this you need to learn how to evaluatethe success ofmachine learning,which is what Chapter 5 is about.We returnto clustering in Section 6.6.4.9Further readingThe 1R scheme was proposed and thoroughly investigated by Holte (1993).Itwas never really intended as a machine learning “method”:the point was moreto demonstrate that very simple structures underlie most ofthe practicaldatasets being used to evaluate machine learning methods at the time and thatputting high-powered inductive inference methods to work on simple datasetswas like using a sledgehammer to crack a nut.Why grapple with a complex deci-sion tree when a simple rule will do? The method that generates one simple ruleper class is the result ofwork by Lucio de Souza Coelho ofBrazil and Len TriggofNew Zealand,and it has been dubbed hyperpipes.A very simple algorithm,it has the advantage ofbeing extremely fast and is quite feasible even with anenormous number ofattributes.4.9FURTHER READING139P088407-Ch004.qxd  4/30/05  11:13 AM  Page 139(a)16610462422422222ABC(b)Figure 4.16A ball tree:(a) two cluster centers and their dividing line and (b) the cor-responding tree.P088407-Ch004.qxd  4/30/05  11:13 AM  Page 140Bayes was an eighteenth-century English philosopher who set out his theoryofprobability in “An essay towards solving a problem in the doctrine ofchances,”published in the Philosophical Transactions ofthe Royal Society ofLondon(Bayes 1763);the rule that bears his name has been a cornerstone ofprobability theory ever since.The difﬁculty with the application ofBayes’srule in practice is the assignment ofprior probabilities.Some statisticians,dubbed Bayesians,take the rule as gospel and insist that people make seriousattempts to estimate prior probabilities accurately—although such estimates areoften subjective.Others,non-Bayesians,prefer the kind ofprior-free analysisthat typically generates statistical conﬁdence intervals,which we will meet in thenext chapter.With a particular dataset,prior probabilities are usually reason-ably easy to estimate,which encourages a Bayesian approach to learning.Theindependence assumption made by the Naïve Bayes method is a great stumblingblock,however,and some attempts are being made to apply Bayesian analysiswithout assuming independence.The resulting models are called Bayesian net-works(Heckerman et al.1995),and we describe them in Section 6.7.Bayesian techniques had been used in the ﬁeld ofpattern recognition (Dudaand Hart 1973) for 20 years before they were adopted by machine learningresearchers (e.g.,see Langley et al.1992) and made to work on datasets withredundant attributes (Langley and Sage 1994) and numeric attributes (John andLangley 1995).The label Naïve Bayesis unfortunate because it is hard to usethis method without feeling simpleminded.However,there is nothing naïveabout its use in appropriate circumstances.The multinomial Naïve Bayes model,which is particularly appropriate for text classiﬁcation,was investigated byMcCallum and Nigam (1998).The classic paper on decision tree induction is by Quinlan (1986),whodescribes the basic ID3 procedure developed in this chapter.A comprehensivedescription ofthe method,including the improvements that are embodied inC4.5,appears in a classic book by Quinlan (1993),which gives a listing ofthecomplete C4.5 system,written in the C programming language.PRISM wasdeveloped by Cendrowska (1987),who also introduced the contact lens dataset.Association rules are introduced and described in the database literaturerather than in the machine learning literature.Here the emphasis is very muchon dealing with huge amounts ofdata rather than on sensitive ways oftestingand evaluating algorithms on limited datasets.The algorithm introduced in thischapter is the Apriori method developed by Agrawal and his associates (Agrawalet al.1993a,1993b;Agrawal and Srikant 1994).A survey ofassociation-rulemining appears in an article by Chen et al.(1996).Linear regression is described in most standard statistical texts,and a partic-ularly comprehensive treatment can be found in a book by Lawson and Hanson(1995).The use oflinear models for classiﬁcation enjoyed a great deal ofpop-ularity in the 1960s;Nilsson (1965) provides an excellent reference.He deﬁnes4.9FURTHER READING141P088407-Ch004.qxd  4/30/05  11:13 AM  Page 141a linear threshold unitas a binary test ofwhether a linear function is greater orless than zero and a linear machineas a set oflinear functions,one for each class,whose value for an unknown example is compared and the largest chosen as itspredicted class.In the distant past,perceptrons fell out offavor on publicationofan inﬂuential book that showed they had fundamental limitations (Minskyand Papert 1969);however,more complex systems oflinear functions haveenjoyed a resurgence in recent years in the form ofneural networks,describedin Section 6.3.The Winnow algorithms were introduced by Nick Littlestone inhis PhD thesis in 1989 (Littlestone 1988,1989).Multiresponse linear classiﬁershave found a new application recently for an operation called stackingthat com-bines the output ofother learning algorithms,described in Chapter 7 (seeWolpert 1992).Friedman (1996) describes the technique ofpairwise classiﬁca-tion,Fürnkranz (2002) further analyzes it,and Hastie and Tibshirani (1998)extend it to estimate probabilities using pairwise coupling.Fix and Hodges (1951) performed the ﬁrst analysis ofthe nearest-neighbormethod,and Johns (1961) pioneered its use in classiﬁcation problems.Coverand Hart (1967) obtained the classic theoretical result that,for large enoughdatasets,its probability oferror never exceeds twice the theoretical minimum;Devroye et al.(1996) showed that k-nearest neighbor is asymptotically optimalfor large kand nwithk/nÆ0.Nearest-neighbor methods gained popularity inmachine learning through the work ofAha (1992),who showed that instance-based learning can be combined with noisy exemplar pruning and attributeweighting and that the resulting methods perform well in comparison withother learning methods.We take this up again in Chapter 6.The kD-tree data structure was developed by Friedman et al.(1977).Ourdescription closely follows an explanation given by Andrew Moore in his PhDthesis (Moore 1991),who,along with Omohundro (1987),pioneered its use inmachine learning.Moore (2000) describes sophisticated ways ofconstructingball trees that perform well even with thousands ofattributes.We took our balltree example from lecture notes by Alexander Gray ofCarnegie-Mellon Uni-versity.The voting feature intervals method mentioned in the Discussion sub-section at the end ofSection 4.7 is described by Demiroz and Guvenir (1997).The k-means algorithm is a classic technique,and many descriptions andvariations are available (e.g.,see Hartigan 1975).The clever use ofkD-trees tospeed up k-means clustering,which we chose to illustrate using ball treesinstead,was pioneered by Moore and Pelleg (2000) in their X-means clusteringalgorithm.That algorithm also contains some other innovations,described inSection 6.6.142CHAPTER 4|ALGORITHMS:THE BASIC METHODSP088407-Ch004.qxd  4/30/05  11:13 AM  Page 142Evaluation is the key to making real progress in data mining.There are lots ofways ofinferring structure from data:we have encountered many already andwill see further reﬁnements,and new methods,in the next chapter.But to deter-mine which ones to use on a particular problem we need systematic ways toevaluate how different methods work and to compare one with another.Eval-uation is not as simple as it might appear at ﬁrst sight.What’s the problem? We have the training set;surely we can just look at howwell different methods do on that.Well,no:as we will see very shortly,per-formance on the training set is deﬁnitely not a good indicator ofperformanceon an independent test set.We need ways ofpredicting performance bounds inpractice,based on experiments with whatever data can be obtained.When a vast supply ofdata is available,this is no problem:just make a modelbased on a large training set,and try it out on another large test set.But althoughdata mining sometimes involves “big data”—particularly in marketing,sales,and customer support applications—it is often the case that data,quality data,is scarce.The oil slicks mentioned in Chapter 1 (pages 23–24) had to be detectedchapter5Credibility:Evaluating What’s Been Learned143P088407-Ch005.qxd  4/30/05  11:21 AM  Page 143and marked manually—a skilled and labor-intensive process—before beingused as training data.Even in the credit card application (pages 22–23),thereturned out to be only 1000 training examples ofthe appropriate type.The elec-tricity supply data (pages 24–25) went back 15 years,5000 days—but only 15Christmas Days and Thanksgivings,and just 4 February 29s and presidentialelections.The electromechanical diagnosis application (pages 25–26) was ableto capitalize on 20 years ofrecorded experience,but this yielded only 300 usableexamples offaults.Marketing and sales applications (pages 26–28) certainlyinvolve big data,but many others do not:training data frequently relies on spe-cialist human expertise—and that is always in short supply.The question ofpredicting performance based on limited data is an inter-esting,and still controversial,one.We will encounter many different techniques,ofwhich one—repeated cross-validation—is gaining ascendance and is proba-bly the evaluation method ofchoice in most practical limited-data situations.Comparing the performance ofdifferent machine learning methods on a givenproblem is another matter that is not so easy as it sounds:to be sure that appar-ent differences are not caused by chance effects,statistical tests are needed.Sofar we have tacitly assumed that what is being predicted is the ability to classifytest instances accurately;however,some situations involve predicting the classprobabilities rather than the classes themselves,and others involve predictingnumeric rather than nominal values.Different methods are needed in each case.Then we look at the question ofcost.In most practical data mining situationsthe cost ofa misclassiﬁcation error depends on the type oferror it is—whether,for example,a positive example was erroneously classiﬁed as negative or viceversa.When doing data mining,and evaluating its performance,it is often essen-tial to take these costs into account.Fortunately,there are simple techniques tomake most learning schemes cost sensitive without grappling with the internalsofthe algorithm.Finally,the whole notion ofevaluation has fascinating philo-sophical connections.For 2000 years philosophers have debated the question ofhow to evaluate scientiﬁc theories,and the issues are brought into sharp focusby data mining because what is extracted is essentially a “theory”ofthe data.5.1Training and testingFor classiﬁcation problems,it is natural to measure a classiﬁer’s performance interms ofthe errorrate.The classiﬁer predicts the class ofeach instance:ifit iscorrect,that is counted as a success;ifnot,it is an error.The error rate is just theproportion oferrors made over a whole set ofinstances,and it measures theoverall performance ofthe classiﬁer.Ofcourse,what we are interested in is the likely future performance on newdata,not the past performance on old data.We already know the classiﬁcations144CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 144ofeach instance in the training set,which after all is why we can use it for train-ing.We are not generally interested in learning about those classiﬁcations—although we might be ifour purpose is data cleansing rather than prediction.So the question is,is the error rate on old data likely to be a good indicator ofthe error rate on new data? The answer is a resounding no—not ifthe old datawas used during the learning process to train the classiﬁer.This is a surprising fact,and a very important one.Error rate on the train-ing set is notlikely to be a good indicator offuture performance.Why? Becausethe classiﬁer has been learned from the very same training data,any estimate ofperformance based on that data will be optimistic,and may be hopelesslyoptimistic.We have already seen an example ofthis in the labor relations dataset.Figure1.3(b) was generated directly from the training data,and Figure 1.3(a) wasobtained from it by a process ofpruning.The former is likely to be more accu-rate on the data that was used to train the classiﬁer but will probably performless well on independent test data because it is overﬁtted to the training data.The ﬁrst tree will look good according to the error rate on the training data,better than the second tree.But this does not reﬂect how they will perform onindependent test data.The error rate on the training data is called the resubstitutionerror,becauseit is calculated by resubstituting the training instances into a classiﬁer that wasconstructed from them.Although it is not a reliable predictor ofthe true errorrate on new data,it is nevertheless often useful to know.To predict the performance ofa classiﬁer on new data,we need to assess itserror rate on a dataset that played no part in the formation ofthe classiﬁer.Thisindependent dataset is called the testset.We assume that both the training dataand the test data are representative samples ofthe underlying problem.In some cases the test data might be distinct in nature from the training data.Consider,for example,the credit risk problem from Section 1.3.Suppose thebank had training data from branches in New York City and Florida and wantedto know how well a classiﬁer trained on one ofthese datasets would perform ina new branch in Nebraska.It should probably use the Florida data as test datato evaluate the New York-trained classiﬁer and the New York data to evaluatethe Florida-trained classiﬁer.Ifthe datasets were amalgamated before training,performance on the test data would probably not be a good indicator ofper-formance on future data in a completely different state.It is important that the test data was not used inanywayto create the clas-siﬁer.For example,some learning methods involve two stages,one to come upwith a basic structure and the second to optimize parameters involved in thatstructure,and separate sets ofdata may be needed in the two stages.Or youmight try out several learning schemes on the training data and then evaluatethem—on a fresh dataset,ofcourse—to see which one works best.But none of5.1TRAINING AND TESTING145P088407-Ch005.qxd  4/30/05  11:21 AM  Page 145this data may be used to determine an estimate ofthe future error rate.In suchsituations people often talk about three datasets:the trainingdata,the valida-tiondata,and the testdata.The training data is used by one or more learningmethods to come up with classiﬁers.The validation data is used to optimizeparameters ofthose classiﬁers,or to select a particular one.Then the test datais used to calculate the error rate ofthe ﬁnal,optimized,method.Each ofthethree sets must be chosen independently:the validation set must be differentfrom the training set to obtain good performance in the optimization or selec-tion stage,and the test set must be different from both to obtain a reliable esti-mate ofthe true error rate.It may be that once the error rate has been determined,the test data isbundled back into the training data to produce a new classiﬁer for actual use.There is nothing wrong with this:it is just a way ofmaximizing the amount ofdata used to generate the classiﬁer that will actually be employed in practice.What is important is that error rates are not quoted based on any ofthis data.Also,once the validation data has been used—maybe to determine the best typeoflearning scheme to use—then it can be bundled back into the training datato retrain that learning scheme,maximizing the use ofdata.Iflots ofdata is available,there is no problem:we take a large sample anduse it for training;then another,independent large sample ofdifferent data anduse it for testing.Provided that both samples are representative,the error rateon the test set will give a true indication offuture performance.Generally,thelarger the training sample the better the classiﬁer,although the returns begin todiminish once a certain volume oftraining data is exceeded.And the larger thetest sample,the more accurate the error estimate.The accuracy ofthe error esti-mate can be quantiﬁed statistically,as we will see in the next section.The real problem occurs when there is not a vast supply ofdata available.Inmany situations the training data must be classiﬁed manually—and so must thetest data,ofcourse,to obtain error estimates.This limits the amount ofdatathat can be used for training,validation,and testing,and the problem becomeshow to make the most ofa limited dataset.From this dataset,a certain amountis held over for testing—this is called the holdoutprocedure—and the remain-der is used for training (and,ifnecessary,part ofthat is set aside for validation).There’s a dilemma here:to ﬁnd a good classiﬁer,we want to use as much ofthedata as possible for training;to obtain a good error estimate,we want to use asmuch ofit as possible for testing.Sections 5.3 and 5.4 review widely usedmethods for dealing with this dilemma.5.2Predicting performanceSuppose we measure the error ofa classiﬁer on a test set and obtain a certainnumeric error rate—say 25%.Actually,in this section we refer to success rate146CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 146rather than error rate,so this corresponds to a success rate of75%.Now,this isonly an estimate.What can you say about the truesuccess rate on the targetpopulation? Sure,it’s expected to be close to 75%.But how close—within 5%?Within 10%? It must depend on the size ofthe test set.Naturally,we would bemore conﬁdent ofthe 75% ﬁgure ifit was based on a test set of10,000 instancesrather than on a test set of100 instances.But how much more conﬁdent wouldwe be?To answer these questions,we need some statistical reasoning.In statistics,asuccession ofindependent events that either succeed or fail is called a Bernoulliprocess.The classic example is coin tossing.Each toss is an independent event.Let’s say we always predict heads;but rather than “heads”or “tails,”each toss is considered a “success”or a “failure.”Let’s say the coin is biased,but we don’tknow what the probability ofheads is.Then,ifwe actually toss the coin 100times and 75 ofthem are heads,we have a situation much like the one describedpreviously for a classiﬁer with an observed 75% success rate on a test set.Whatcan we say about the true success probability? In other words,imagine that thereis a Bernoulli process—a biased coin—whose true (but unknown) success rateis p.Suppose that out ofNtrials,Sare successes:thus the observed success rateis f=S/N.The question is,what does this tell you about the true success rate p?The answer to this question is usually expressed as a conﬁdence interval;thatis,plies within a certain speciﬁed interval with a certain speciﬁed conﬁdence.For example,ifS=750 successes are observed out ofN=1000 trials,this indi-cates that the true success rate must be around 75%.But how close to 75%? Itturns out that with 80% conﬁdence,the true success rate plies between 73.2%and 76.7%.IfS=75 successes are observed out ofN=100 trials,this also indi-cates that the true success rate must be around 75%.But the experiment issmaller,and the 80% conﬁdence interval for pis wider,stretching from 69.1%to 80.1%.These ﬁgures are easy to relate to qualitatively,but how are they derived quan-titatively? We reason as follows:the mean and variance ofa single Bernoulli trialwith success rate pare pand p(1 -p),respectively.IfNtrials are taken from aBernoulli process,the expected success rate f=S/Nis a random variable withthe same mean p;the variance is reduced by a factor ofNto p(1 -p)/N.Forlarge N,the distribution ofthis random variable approaches the normal distri-bution.These are all facts ofstatistics:we will not go into how they are derived.The probability that a random variable X,with zero mean,lies within acertain conﬁdence range ofwidth 2zisFor a normal distribution,values ofcand corresponding values ofzare givenin tables printed at the back ofmost statistical texts.However,the tabulationsconventionally take a slightly different form:they give the conﬁdence that XwillPr-££[]=zXzc.5.2PREDICTING PERFORMANCE147P088407-Ch005.qxd  4/30/05  11:21 AM  Page 147148CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDlie outside the range,and they give it for the upper part ofthe range only:This is called a one-tailedprobability because it refers only to the upper “tail”ofthe distribution.Normal distributions are symmetric,so the probabilities forthe lower tailare just the same.Table 5.1 gives an example.Like other tables for the normal distribution,thisassumes that the random variable Xhas a mean ofzero and a variance ofone.Alternatively,you might say that the zﬁgures are measured in standarddevia-tionsfromthemean.Thus the ﬁgure for Pr[X≥z] =5% implies that there is a5% chance that Xlies more than 1.65 standard deviations above the mean.Because the distribution is symmetric,the chance that Xlies more than 1.65standard deviations from the mean (above or below) is 10%,orAll we need do now is reduce the random variable fto have zero mean and unitvariance.We do this by subtracting the mean pand dividing by the standarddeviation This leads toNow here is the procedure for ﬁnding conﬁdence limits.Given a particular con-ﬁdence ﬁgure c,consult Table 5.1 for the corresponding zvalue.To use the tableyou will ﬁrst have to subtract cfrom 1 and then halve the result,so that for c=90% you use the table entry for 5%.Linear interpolation can be used for inter-Pr-<--()<ÈÎÍ˘˚˙=zfpppNzc1.ppN1-().Pr-££[]=16516590..%.XPrXz£-[]PrXz≥[].Table 5.1Conﬁdence limits for the normal distribution.Pr[X≥z]z0.1%3.090.5%2.581%2.335%1.6510%1.2820%0.8440%0.25P088407-Ch005.qxd  4/30/05  11:21 AM  Page 1485.3CROSS-VALIDATION149mediate conﬁdence levels.Then write the inequality in the preceding expressionas an equality and invert it to ﬁnd an expression for p.The ﬁnal step involves solving a quadratic equation.Although not hard todo,it leads to an unpleasantly formidable expression for the conﬁdence limits:The ±in this expression gives two values for pthat represent the upper andlower conﬁdence boundaries.Although the formula looks complicated,it is nothard to work out in particular cases.This result can be used to obtain the values in the preceding numericexample.Setting f=75%,N=1000,and c=80% (so that z=1.28) leads to theinterval [0.732,0.767] for p,and N=100 leads to [0.691,0.801] for the same levelofconﬁdence.Note that the normal distribution assumption is only valid forlarge N(say,N>100).Thus f=75% and N=10 leads to conﬁdence limits[0.549,0.881]—but these should be taken with a grain ofsalt.5.3Cross-validationNow consider what to do when the amount ofdata for training and testing islimited.The holdout method reserves a certain amount for testing and uses theremainder for training (and sets part ofthat aside for validation,ifrequired).In practical terms,it is common to hold out one-third ofthe data for testingand use the remaining two-thirds for training.Ofcourse,you may be unlucky:the sample used for training (or testing)might not be representative.In general,you cannot tell whether a sample is rep-resentative or not.But there is one simple check that might be worthwhile:eachclass in the full dataset should be represented in about the right proportion inthe training and testing sets.If,by bad luck,all examples with a certain classwere missing from the training set,you could hardly expect a classiﬁer learnedfrom that data to perform well on the examples ofthat class—and the situationwould be exacerbated by the fact that the class would necessarily be overrepre-sented in the test set because none ofits instances made it into the training set!Instead,you should ensure that the random sampling is done in such a way as to guarantee that each class is properly represented in both training and testsets.This procedure is called stratiﬁcation,and we might speak ofstratiﬁedholdout.Although it is generally well worth doing,stratiﬁcation provides onlya primitive safeguard against uneven representation in training and test sets.A more general way to mitigate any bias caused by the particular samplechosen for holdout is to repeat the whole process,training and testing,severaltimes with different random samples.In each iteration a certain proportion—pfzNzfNfNzNzN=+±-+ÊËÁˆ¯˜+ÊËˆ¯22222241.P088407-Ch005.qxd  4/30/05  11:21 AM  Page 149say two-thirds—ofthe data is randomly selected for training,possibly withstratiﬁcation,and the remainder used for testing.The error rates on the differ-ent iterations are averaged to yield an overall error rate.This is the repeatedholdoutmethod oferror rate estimation.In a single holdout procedure,you might consider swapping the roles ofthetesting and training data—that is,train the system on the test data and test iton the training data—and average the two results,thus reducing the effect ofuneven representation in training and test sets.Unfortunately,this is only reallyplausible with a 50:50 split between training and test data,which is generallynot ideal—it is better to use more than halfthe data for training even at theexpense oftest data.However,a simple variant forms the basis ofan importantstatistical technique called cross-validation.In cross-validation,you decide on aﬁxed number offolds,or partitions ofthe data.Suppose we use three.Then thedata is split into three approximately equal partitions and each in turn is usedfor testing and the remainder is used for training.That is,use two-thirds fortraining and one-third for testing and repeat the procedure three times so that,in the end,every instance has been used exactly once for testing.This is calledthreefoldcross-validation,and ifstratiﬁcation is adopted as well—which it oftenis—it is stratiﬁedthreefoldcross-validation.The standard way ofpredicting the error rate ofa learning technique givena single,ﬁxed sample ofdata is to use stratiﬁed 10-fold cross-validation.Thedata is divided randomly into 10 parts in which the class is represented inapproximately the same proportions as in the full dataset.Each part is held outin turn and the learning scheme trained on the remaining nine-tenths;then itserror rate is calculated on the holdout set.Thus the learning procedure is exe-cuted a total of10 times on different training sets (each ofwhich have a lot incommon).Finally,the 10 error estimates are averaged to yield an overall errorestimate.Why 10? Extensive tests on numerous datasets,with different learning tech-niques,have shown that 10 is about the right number offolds to get the bestestimate oferror,and there is also some theoretical evidence that backs this up.Although these arguments are by no means conclusive,and debate continues torage in machine learning and data mining circles about what is the best schemefor evaluation,10-fold cross-validation has become the standard method inpractical terms.Tests have also shown that the use ofstratiﬁcation improvesresults slightly.Thus the standard evaluation technique in situations where onlylimited data is available is stratiﬁed 10-fold cross-validation.Note that neitherthe stratiﬁcation nor the division into 10 folds has to be exact:it is enough todivide the data into 10 approximately equal sets in which the various class valuesare represented in approximately the right proportion.Statistical evaluation isnot an exact science.Moreover,there is nothing magic about the exact number10:5-fold or 20-fold cross-validation is likely to be almost as good.150CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 150A single 10-fold cross-validation might not be enough to get a reliable errorestimate.Different 10-fold cross-validation experiments with the same learningmethod and dataset often produce different results,because ofthe effect ofrandom variation in choosing the folds themselves.Stratiﬁcation reduces thevariation,but it certainly does not eliminate it entirely.When seeking an accu-rate error estimate,it is standard procedure to repeat the cross-validationprocess 10 times—that is,10 times 10-fold cross-validation—and average theresults.This involves invoking the learning algorithm 100 times on datasets thatare all nine-tenths the size ofthe original.Obtaining a good measure ofper-formance is a computation-intensive undertaking.5.4Other estimatesTenfold cross-validation is the standard way ofmeasuring the error rate ofalearning scheme on a particular dataset;for reliable results,10 times 10-foldcross-validation.But many other methods are used instead.Two that are par-ticularly prevalent are leave-one-outcross-validation and the bootstrap.Leave-one-outLeave-one-out cross-validation is simply n-fold cross-validation,where nis thenumber ofinstances in the dataset.Each instance in turn is left out,and thelearning method is trained on all the remaining instances.It is judged by its cor-rectness on the remaining instance—one or zero for success or failure,respec-tively.The results ofall njudgments,one for each member ofthe dataset,areaveraged,and that average represents the ﬁnal error estimate.This procedure is an attractive one for two reasons.First,the greatest possi-ble amount ofdata is used for training in each case,which presumably increasesthe chance that the classiﬁer is an accurate one.Second,the procedure is deter-ministic:no random sampling is involved.There is no point in repeating it 10times,or repeating it at all:the same result will be obtained each time.Set againstthis is the high computational cost,because the entire learning procedure mustbe executed ntimes and this is usually quite infeasible for large datasets.Never-theless,leave-one-out seems to offer a chance ofsqueezing the maximum out ofa small dataset and obtaining as accurate an estimate as possible.But there is a disadvantage to leave-one-out cross-validation,apart from thecomputational expense.By its very nature,it cannot be stratiﬁed—worse thanthat,it guaranteesa nonstratiﬁed sample.Stratiﬁcation involves getting thecorrect proportion ofexamples in each class into the test set,and this is impos-sible when the test set contains only a single example.A dramatic,althoughhighly artiﬁcial,illustration ofthe problems this might cause is to imagine acompletely random dataset that contains the same number ofeach oftwo5.4OTHER ESTIMATES151P088407-Ch005.qxd  4/30/05  11:21 AM  Page 151classes.The best that an inducer can do with random data is to predict themajority class,giving a true error rate of50%.But in each fold ofleave-one-out,the opposite class to the test instance is in the majority—and therefore thepredictions will always be incorrect,leading to an estimated error rate of100%!The bootstrapThe second estimation method we describe,the bootstrap,is based on the sta-tistical procedure ofsampling withreplacement.Previously,whenever a samplewas taken from the dataset to form a training or test set,it was drawn withoutreplacement.That is,the same instance,once selected,could not be selectedagain.It is like picking teams for football:you cannot choose the same persontwice.But dataset instances are not like people.Most learning methods can usethe same instance twice,and it makes a difference in the result oflearning ifitis present in the training set twice.(Mathematical sticklers will notice that weshould not really be talking about “sets”at all ifthe same object can appear morethan once.)The idea ofthe bootstrap is to sample the dataset with replacement to forma training set.We will describe a particular variant,mysteriously (but for areason that will soon become apparent) called the 0.632bootstrap.For this,adataset ofninstances is sampled ntimes,with replacement,to give anotherdataset ofninstances.Because some elements in this second dataset will (almostcertainly) be repeated,there must be some instances in the original dataset thathave not been picked:we will use these as test instances.What is the chance that a particular instance will not be picked for the train-ing set? It has a 1/nprobability ofbeing picked each time and therefore a 1 -1/nprobability ofnot being picked.Multiply these probabilities togetheraccording to the number ofpicking opportunities,which is n,and the result isa ﬁgure of(where eis the base ofnatural logarithms,2.7183,not the error rate!).This givesthe chance ofa particular instance not being picked at all.Thus for a reason-ably large dataset,the test set will contain about 36.8% ofthe instances and thetraining set will contain about 63.2% ofthem (now you can see why it’s calledthe 0.632bootstrap).Some instances will be repeated in the training set,bring-ing it up to a total size ofn,the same as in the original dataset.The ﬁgure obtained by training a learning system on the training set and cal-culating its error over the test set will be a pessimistic estimate ofthe true errorrate,because the training set,although its size is n,nevertheless contains only63% ofthe instances,which is not a great deal compared,for example,with the1103681-ÊËˆ¯ª=-nen.152CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 15290% used in 10-fold cross-validation.To compensate for this,we combine thetest-set error rate with the resubstitution error on the instances in the trainingset.The resubstitution ﬁgure,as we warned earlier,gives a very optimistic esti-mate ofthe true error and should certainly not be used as an error ﬁgure on itsown.But the bootstrap procedure combines it with the test error rate to give aﬁnal estimate eas follows:Then,the whole bootstrap procedure is repeated several times,with differentreplacement samples for the training set,and the results averaged.The bootstrap procedure may be the best way ofestimating error for verysmall datasets.However,like leave-one-out cross-validation,it has disadvantagesthat can be illustrated by considering a special,artiﬁcial situation.In fact,thevery dataset we considered previously will do:a completely random dataset withtwo classes.The true error rate is 50% for any prediction rule.But a scheme thatmemorized the training set would give a perfect resubstitution score of100%so that etraining instances=0,and the 0.632 bootstrap will mix this in with a weightof0.368 to give an overall error rate ofonly 31.6% (0.632 ¥50% +0.368 ¥0%),which is misleadingly optimistic.5.5Comparing data mining methodsWe often need to compare two different learning methods on the same problemto see which is the better one to use.It seems simple:estimate the error usingcross-validation (or any other suitable estimation procedure),perhaps repeatedseveral times,and choose the scheme whose estimate is smaller.This is quitesufﬁcient in many practical applications:ifone method has a lower estimatederror than another on a particular dataset,the best we can do is to use the formermethod’s model.However,it may be that the difference is simply caused by esti-mation error,and in some circumstances it is important to determine whetherone scheme is really better than another on a particular problem.This is a stan-dard challenge for machine learning researchers.Ifa new learning algorithm isproposed,its proponents must show that it improves on the state ofthe art forthe problem at hand and demonstrate that the observed improvement is notjust a chance effect in the estimation process.This is a job for a statistical test that gives conﬁdence bounds,the kind wemet previously when trying to predict true performance from a given test-seterror rate.Ifthere were unlimited data,we could use a large amount for train-ing and evaluate performance on a large independent test set,obtaining conﬁ-dence bounds just as before.However,ifthe difference turns out to be signiﬁcantwe must ensure that this is not just because ofthe particular dataset we eee=¥+¥06320368...test instancestraining instances5.5COMPARING DATA MINING METHODS153P088407-Ch005.qxd  4/30/05  11:21 AM  Page 153happened to base the experiment on.What we want to determine is whetherone scheme is better or worse than another on average,across all possible train-ing and test datasets that can be drawn from the domain.Because the amountoftraining data naturally affects performance,all datasets should be the samesize:indeed,the experiment might be repeated with different sizes to obtain alearning curve.For the moment,assume that the supply ofdata is unlimited.For deﬁnite-ness,suppose that cross-validation is being used to obtain the error estimates(other estimators,such as repeated cross-validation,are equally viable).For eachlearning method we can draw several datasets ofthe same size,obtain an accu-racy estimate for each dataset using cross-validation,and compute the mean ofthe estimates.Each cross-validation experiment yields a different,independenterror estimate.What we are interested in is the mean accuracy across all possi-ble datasets ofthe same size,and whether this mean is greater for one schemeor the other.From this point ofview,we are trying to determine whether the mean ofa set ofsamples—cross-validation estimates for the various datasets that wesampled from the domain—is signiﬁcantly greater than,or signiﬁcantly lessthan,the mean ofanother.This is a job for a statistical device known as the t-test,or Student’st-test.Because the same cross-validation experiment can beused for both learning methods to obtain a matched pair ofresults for eachdataset,a more sensitive version ofthe t-test known as a pairedt-testcan beused.We need some notation.There is a set ofsamples x1,x2,...,xkobtained bysuccessive 10-fold cross-validations using one learning scheme,and a second setofsamples y1,y2,...,ykobtained by successive 10-fold cross-validations usingthe other.Each cross-validation estimate is generated using a different dataset(but all datasets are ofthe same size and from the same domain).We will getthe best results ifexactly the same cross-validation partitions are used for bothschemes so that x1and y1are obtained using the same cross-validation split,asare x2and y2,and so on.Denote the mean ofthe ﬁrst set ofsamples by x–andthe mean ofthe second set by y–.We are trying to determine whether x–is sig-niﬁcantly different from y–.Ifthere are enough samples,the mean (x–) ofa set ofindependent samples(x1,x2,...,xk) has a normal (i.e.,Gaussian) distribution,regardless ofthe dis-tribution underlying the samples themselves.We will call the true value ofthemean m.Ifwe knew the variance ofthat normal distribution,so that it could bereduced to have zero mean and unit variance,we could obtain conﬁdence limitson mgiven the mean ofthe samples (x–).However,the variance is unknown,andthe only way we can obtain it is to estimate it from the set ofsamples.That is not hard to do.The variance ofx–can be estimated by dividing thevariance calculated from the samples x1,x2,...,xk—call it s2x—by k.But the154CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 1545.5COMPARING DATA MINING METHODS155fact that we have to estimatethe variance changes things somewhat.We canreduce the distribution ofx–to have zero mean and unit variance by usingBecause the variance is only an estimate,this does nothave a normal distribu-tion (although it does become normal for large values ofk).Instead,it has whatis called a Student’sdistributionwithk-1degreesoffreedom.What this meansin practice is that we have to use a table ofconﬁdence intervals for Student’sdistribution rather than the conﬁdence table for the normal distribution givenearlier.For 9 degrees offreedom (which is the correct number ifwe are usingthe average of10 cross-validations) the appropriate conﬁdence limits are shownin Table 5.2.Ifyou compare them with Table 5.1 you will see that the Student’sﬁgures are slightly more conservative—for a given degree ofconﬁdence,theinterval is slightly wider—and this reﬂects the additional uncertainty caused by having to estimate the variance.Different tables are needed for differentnumbers ofdegrees offreedom,and ifthere are more than 100 degrees offreedom the conﬁdence limits are very close to those for the normal distribu-tion.Like Table 5.1,the ﬁgures in Table 5.2 are for a “one-sided”conﬁdenceinterval.To decide whether the means x–and y–,each an average ofthe same numberkofsamples,are the same or not,we consider the differences dibetween corre-sponding observations,di=xi-yi.This is legitimate because the observationsare paired.The mean ofthis difference is just the difference between the twomeans,d–=x–-y–,and,like the means themselves,it has a Student’s distributionwith k-1 degrees offreedom.Ifthe means are the same,the difference is zero(this is called the nullhypothesis);ifthey’re signiﬁcantly different,the differencewill be signiﬁcantly different from zero.So for a given conﬁdence level,we willcheck whether the actual difference exceeds the conﬁdence limit.xkx-ms2.Table 5.2Conﬁdence limits for Student’s distribution with 9 degrees of freedom.Pr[X≥z]z0.1%4.300.5%3.251%2.825%1.8310%1.3820%0.88P088407-Ch005.qxd  4/30/05  11:21 AM  Page 155First,reduce the difference to a zero-mean,unit-variance variable called thet-statistic:where s2dis the variance ofthe difference samples.Then,decide on a conﬁdencelevel—generally,5% or 1% is used in practice.From this the conﬁdence limit zis determined using Table 5.2 ifkis 10;ifit is not,a conﬁdence table oftheStudent’s distribution for the kvalue in question is used.A two-tailed test isappropriate because we do not know in advance whether the mean ofthe x’s islikely to be greater than that ofthe y’s or vice versa:thus for a 1% test we usethe value corresponding to 0.5% in Table 5.2.Ifthe value oftaccording to thepreceding formula is greater than z,or less than -z,we reject the null hypothe-sis that the means are the same and conclude that there really is a signiﬁcant dif-ference between the two learning methods on that domain for that dataset size.Two observations are worth making on this procedure.The ﬁrst is technical:what ifthe observations were not paired? That is,what ifwe were unable,forsome reason,to assess the error ofeach learning scheme on the same datasets?What ifthe number ofdatasets for each scheme was not even the same? Theseconditions could arise ifsomeone else had evaluated one ofthe methods andpublished several different estimates for a particular domain and dataset size—or perhaps just their mean and variance—and we wished to compare this witha different learning method.Then it is necessary to use a regular,nonpaired t-test.Ifthe means are normally distributed,as we are assuming,the differencebetween the means is also normally distributed.Instead oftaking the mean ofthe difference,d–,we use the difference ofthe means,x–-y–.Ofcourse,that’s thesame thing:the mean ofthe difference isthe difference ofthe means.But thevariance ofthe difference d–is notthe same.Ifthe variance ofthe samples x1,x2,...,xkis s2xand the variance ofthe samples y1,y2,...,y1is s2y,the best esti-mate ofthe variance ofthe difference ofthe means isIt is this variance (or rather,its square root) that should be used as the denom-inator ofthe t-statistic given previously.The degrees offreedom,necessary forconsulting Student’s conﬁdence tables,should be taken conservatively to be theminimum ofthe degrees offreedom ofthe two samples.Essentially,knowingthat the observations are paired allows the use ofa better estimate for the vari-ance,which will produce tighter conﬁdence bounds.The second observation concerns the assumption that there is essentiallyunlimited data so that several independent datasets ofthe right size can be used.ssxyk221+.tdkd=s2156CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 1565.6PREDICTING PROBABILITIES157In practice there is usually only a single dataset oflimited size.What can bedone? We could split the data into (perhaps 10) subsets and perform a cross-validation on each.However,the overall result will only tell us whether a learn-ing scheme is preferable for that particular size—perhaps one-tenth oftheoriginal dataset.Alternatively,the original dataset could be reused—forexample,with different randomizations ofthe dataset for each cross-validation.2However,the resulting cross-validation estimates will not be independentbecause they are not based on independent datasets.In practice,this means thata difference may be judged to be signiﬁcant when in fact it is not.In fact,justincreasing the number ofsamples k,that is,the number ofcross-validation runs,will eventually yield an apparently signiﬁcant difference because the value ofthet-statistic increases without bound.Various modiﬁcations ofthe standard t-test have been proposed to circum-vent this problem,all ofthem heuristic and lacking sound theoretical justiﬁca-tion.One that appears to work well in practice is the correctedresampledt-test.Assume for the moment that the repeated holdout method is used instead ofcross-validation,repeated ktimes on different random splits ofthe same datasetto obtain accuracy estimates for two learning methods.Each time,n1instancesare used for training and n2for testing,and differences diare computed fromperformance on the test data.The corrected resampled t-test uses the modiﬁedstatisticin exactly the same way as the standard t-statistic.A closer look at the formulashows that its value cannot be increased simply by increasing k.The same mod-iﬁed statistic can be used with repeated cross-validation,which is just a specialcase ofrepeated holdout in which the individual test sets for onecross-validation do not overlap.For 10-fold cross-validation repeated 10 times,k=100,n2/n1=0.1/0.9,and s2dis based on 100 differences.5.6Predicting probabilitiesThroughout this section we have tacitly assumed that the goal is to maximizethe success rate ofthe predictions.The outcome for each test instance is eithercorrect,ifthe prediction agrees with the actual value for that instance,or incor-rect,ifit does not.There are no grays:everything is black or white,correct ortdknnd=+ÊËˆ¯1212s2The method was advocated in the ﬁrst edition ofthis book.P088407-Ch005.qxd  4/30/05  11:21 AM  Page 157incorrect.In many situations,this is the most appropriate perspective.Ifthelearning scheme,when it is actually applied,results in either a correct or anincorrect prediction,success is the right measure to use.This is sometimes calleda 0-1lossfunction:the “loss”is either zero ifthe prediction is correct or oneifit is not.The use oflossis conventional,although a more optimistic termi-nology might couch the outcome in terms ofproﬁt instead.Other situations are softer edged.Most learning methods can associate aprobability with each prediction (as the Naïve Bayes method does).It might bemore natural to take this probability into account when judging correctness.Forexample,a correct outcome predicted with a probability of99% should perhapsweigh more heavily than one predicted with a probability of51%,and,in a two-class situation,perhaps the latter is not all that much better than an incorrectoutcome predicted with probability 51%.Whether it is appropriate to take pre-diction probabilities into account depends on the application.Ifthe ultimateapplication really is just a prediction ofthe outcome,and no prizes are awardedfor a realistic assessment ofthe likelihood ofthe prediction,it does not seemappropriate to use probabilities.Ifthe prediction is subject to further process-ing,however—perhaps involving assessment by a person,or a cost analysis,ormaybe even serving as input to a second-level learning process—then it maywell be appropriate to take prediction probabilities into account.Quadratic loss functionSuppose that for a single instance there are kpossible outcomes,or classes,andfor a given instance the learning scheme comes up with a probability vector p1,p2,...,pkfor the classes (where these probabilities sum to 1).The actualoutcome for that instance will be one ofthe possible classes.However,it is con-venient to express it as a vector a1,a2,...,akwhose ith component,where iisthe actual class,is 1 and all other components are 0.We can express the penaltyassociated with this situation as a loss function that depends on both the pvectorand the avector.One criterion that is frequently used to evaluate probabilistic prediction isthe quadraticlossfunction:Note that this is for a single instance:the summation is over possible outputsnot over different instances.Just one ofthe a’s will be 1 and the rest will be 0,so the sum contains contributions ofp2jfor the incorrect predictions and (1 -pi)2for the correct one.Consequently,it can be written122-+Âppijj,pajjj-()Â2.158CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 1585.6PREDICTING PROBABILITIES159where iis the correct class.When the test set contains several instances,the lossfunction is summed over them all.It is an interesting theoretical fact that ifyou seek to minimize the value ofthe quadratic loss function in a situation in which the actual class is generatedprobabilistically,the best strategy is to choose for the pvector the actual prob-abilities ofthe different outcomes,that is,pi=Pr[class =i].Ifthe true proba-bilities are known,they will be the best values for p.Ifthey are not,a systemthat strives to minimize the quadratic loss function will be encouraged to useits best estimate ofPr[class =i] as the value for pi.This is quite easy to see.Denote the true probabilities by p*1,p*2,...,p*kso thatp*i=Pr[class =i].The expected value ofthe quadratic loss function for a testinstance can be rewritten as follows:The ﬁrst stage just involves bringing the expectation inside the sum and expand-ing the square.For the second,pjis just a constant and the expected value ofajis simply p*j;moreover,because ajis either 0 or 1,a2j=ajand its expected valueis p*jtoo.The third stage is straightforward algebra.To minimize the resultingsum,it is clear that it is best to choose pj=p*jso that the squared term disap-pears and all that is left is a term that is just the variance ofthe true distribu-tion governing the actual class.Minimizing the squared error has a long history in prediction problems.Inthe present context,the quadratic loss function forces the predictor to be honestabout choosing its best estimate ofthe probabilities—or,rather,it gives prefer-ence to predictors that are able to make the best guess at the true probabilities.Moreover,the quadratic loss function has some useful theoretical properties thatwe will not go into here.For all these reasons it is frequently used as the crite-rion ofsuccess in probabilistic prediction situations.Informational loss functionAnother popular criterion for the evaluation ofprobabilistic prediction is theinformationallossfunction:where the ith prediction is the correct one.This is in fact identical to the nega-tive ofthe log-likelihood function that is optimized by logistic regression,described in Section 4.6.It represents the information (in bits) required toexpress the actual class iwith respect to the probability distribution p1,p2,...,-log2piEpaEpEpaEappppppppjjjjjjjjjjjjjjjjjj-()[]=[]-[]+[]()=-+=-+-ÂÂÂÂ22222221(**)((*)*(*)).P088407-Ch005.qxd  4/30/05  11:21 AM  Page 159pk.In other words,ifyou were given the probability distribution and someonehad to communicate to you which class was the one that actually occurred,thisis the number ofbits that person would need to encode the information iftheydid it as effectively as possible.(Ofcourse,it is always possible to use morebits.)Because probabilities are always less than one,their logarithms are negative,andthe minus sign makes the outcome positive.For example,in a two-class situa-tion—heads or tails—with an equal probability ofeach class,the occurrence ofa head would take 1 bit to transmit,because -log21/2 is 1.The expected value ofthe informational loss function,ifthe true probabili-ties are p*1,p*2,...,p*k,isLike the quadratic loss function,this expression is minimized by choosing pj=p*j,in which case the expression becomes the entropy ofthe true distribution:Thus the informational loss function also rewards honesty in predictors thatknow the true probabilities,and encourages predictors that do not to putforward their best guess.The informational loss function also has a gamblinginterpretation in whichyou imagine gambling on the outcome,placing odds on each possible class andwinning according to the class that comes up.Successive instances are like suc-cessive bets:you carry wins (or losses) over from one to the next.The logarithmofthe total amount ofmoney you win over the whole test set is the value oftheinformational loss function.In gambling,it pays to be able to predict the oddsas accurately as possible;in that sense,honesty pays,too.One problem with the informational loss function is that ifyou assign aprobability ofzero to an event that actually occurs,the function’s value is minusinﬁnity.This corresponds to losing your shirt when gambling.Prudent puntersnever bet everythingon a particular event,no matter how certain it appears.Likewise,prudent predictors operating under the informational loss functiondo not assign zero probability to any outcome.This leads to a problem whenno information is available about that outcome on which to base a prediction:this is called the zero-frequencyproblem,and various plausible solutions havebeen proposed,such as the Laplace estimator discussed for Naïve Bayes on page91.DiscussionIfyou are in the business ofevaluating predictions ofprobabilities,which ofthetwo loss functions should you use? That’s a good question,and there is no uni-versally agreed-upon answer—it’s really a matter oftaste.Both do the funda-----ppppppkk1212222*log**log*...*log*.----ppppppkk1212222*log*log...*log.160CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 160mental job expected ofa loss function:they give maximum reward to predic-tors that are capable ofpredicting the true probabilities accurately.However,there are some objective differences between the two that may help you forman opinion.The quadratic loss function takes account not only ofthe probability assignedto the event that actually occurred,but also the other probabilities.For example,in a four-class situation,suppose you assigned 40% to the class that actuallycame up and distributed the remainder among the other three classes.Thequadratic loss will depend on how you distributed it because ofthe sum ofthe p2jthat occurs in the expression given earlier for the quadratic loss function.The loss will be smallest ifthe 60% was distributed evenly among the threeclasses:an uneven distribution will increase the sum ofthe squares.The infor-mational loss function,on the other hand,depends solely on the probabilityassigned to the class that actually occurred.Ifyou’re gambling on a particularevent coming up,and it does,who cares how you distributed the remainder ofyour money among the other events?Ifyou assign a very small probability to the class that actually occurs,theinformation loss function will penalize you massively.The maximum penalty,for a zero probability,is inﬁnite.The gambling world penalizes mistakes like thisharshly,too! The quadratic loss function,on the other hand,is milder,beingbounded bywhich can never exceed 2.Finally,proponents ofthe informational loss function point to a generaltheory ofperformance assessment in learning called the minimumdescriptionlength(MDL)principle.They argue that the size ofthe structures that a schemelearns can be measured in bits ofinformation,and ifthe same units are usedto measure the loss,the two can be combined in useful and powerful ways.Wereturn to this in Section 5.9.5.7Counting the costThe evaluations that have been discussed so far do not take into account thecost ofmaking wrong decisions,wrong classiﬁcations.Optimizing classiﬁcationrate without considering the cost ofthe errors often leads to strange results.Inone case,machine learning was being used to determine the exact day that eachcow in a dairy herd was in estrus,or “in heat.”Cows were identiﬁed by elec-tronic ear tags,and various attributes were used such as milk volume and chem-ical composition (recorded automatically by a high-tech milking machine),andmilking order—for cows are regular beasts and generally arrive in the milking12+Âpjj,5.7COUNTING THE COST161P088407-Ch005.qxd  4/30/05  11:21 AM  Page 161shed in the same order,except in unusual circumstances such as estrus.In amodern dairy operation it’s important to know when a cow is ready:animalsare fertilized by artiﬁcial insemination and missing a cycle will delay calvingunnecessarily,causing complications down the line.In early experiments,machine learning methods stubbornly predicted that each cow was neverinestrus.Like humans,cows have a menstrual cycle ofapproximately 30 days,sothis “null”rule is correct about 97% ofthe time—an impressive degree ofaccu-racy in any agricultural domain! What was wanted,ofcourse,were rules thatpredicted the “in estrus”situation more accurately than the “not in estrus”one:the costs ofthe two kinds oferror were different.Evaluation by classiﬁcationaccuracy tacitly assumes equal error costs.Other examples in which errors cost different amounts include loan deci-sions:the cost oflending to a defaulter is far greater than the lost-business costofrefusing a loan to a nondefaulter.And oil-slick detection:the cost offailingto detect an environment-threatening real slick is far greater than the cost ofafalse alarm.And load forecasting:the cost ofgearing up electricity generatorsfor a storm that doesn’t hit is far less than the cost ofbeing caught completelyunprepared.And diagnosis:the cost ofmisidentifying problems with a machinethat turns out to be free offaults is less than the cost ofoverlooking problemswith one that is about to fail.And promotional mailing:the cost ofsending junkmail to a household that doesn’t respond is far less than the lost-business costofnot sending it to a household that would have responded.Why—these areall the examples ofChapter 1! In truth,you’d be hard pressed to ﬁnd an appli-cation in which the costs ofdifferent kinds oferror were the same.In the two-class case with classes yesand no,lend or not lend,mark a suspi-cious patch as an oil slick or not,and so on,a single prediction has the four dif-ferent possible outcomes shown in Table 5.3.The truepositives(TP) and truenegatives(TN) are correct classiﬁcations.A falsepositive(FP) occurs when theoutcome is incorrectly predicted as yes(or positive) when it is actually no(neg-ative).A falsenegative(FN) occurs when the outcome is incorrectly predictedas negative when it is actually positive.The truepositiverateis TP divided 162CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDTable 5.3Different outcomes of a two-class prediction.Predicted classyesnoActualyestrue false positivenegativeclassnofalse true positivenegativeP088407-Ch005.qxd  4/30/05  11:21 AM  Page 162by the total number ofpositives,which is TP +FN;the falsepositiverateis FP divided by the total number ofnegatives,FP +TN.The overall success rate is the number ofcorrect classiﬁcations divided by the total number ofclassiﬁcations:Finally,the error rate is one minus this.In a multiclass prediction,the result on a test set is often displayed as a two-dimensional confusion matrixwith a row and column for each class.Each matrixelement shows the number oftest examples for which the actual class is the rowand the predicted class is the column.Good results correspond to large numbersdown the main diagonal and small,ideally zero,off-diagonal elements.Table5.4(a) shows a numeric example with three classes.In this case the test set has200 instances (the sum ofthe nine numbers in the matrix),and 88 +40 +12 =140 ofthem are predicted correctly,so the success rate is 70%.But is this a fair measure ofoverall success? How many agreements wouldyou expect by chance?This predictor predicts a total of120 a’s,60 b’s,and 20c’s;what ifyou had a random predictor that predicted the same total numbersofthe three classes? The answer is shown in Table 5.4(b).Its ﬁrst row dividesthe 100 a’s in the test set into these overall proportions,and the second andthird rows do the same thing for the other two classes.Ofcourse,the row andcolumn totals for this matrix are the same as before—the number ofinstanceshasn’t changed,and we have ensured that the random predictor predicts thesame number ofa’s,b’s,and c’s as the actual predictor.This random predictor gets 60 +18 +4 =82 instances correct.A measurecalled the Kappa statistictakes this expected ﬁgure into account by deducting it from the predictor’s successes and expressing the result as a proportion ofthe total for a perfect predictor,to yield 140 -82 =58 extra successes out TPTNTPTNFPFN++++.5.7COUNTING THE COST163Table 5.4Different outcomes of a three-class prediction: (a) actual and (b) expected.Predicted classPredicted classabcTotalabcTotalActuala88102100Actuala603010100classb1440660classb3618660c18101240c2412440Total1206020Total1206020(a)(b)P088407-Ch005.qxd  4/30/05  11:21 AM  Page 163ofa possible total of200 -82 =118,or 49.2%.The maximum value ofKappais 100%,and the expected value for a random predictor with the same columntotals is zero.In summary,the Kappa statistic is used to measure the agreementbetween predicted and observed categorizations ofa dataset,while correctingfor agreement that occurs by chance.However,like the plain success rate,it doesnot take costs into account.Cost-sensitive classiﬁcationIfthe costs are known,they can be incorporated into a ﬁnancial analysis ofthedecision-making process.In the two-class case,in which the confusion matrixis like that ofTable 5.3,the two kinds oferror—false positives and false nega-tives—will have different costs;likewise,the two types ofcorrect classiﬁcationmay have different beneﬁts.In the two-class case,costs can be summarized inthe form ofa 2 ¥2 matrix in which the diagonal elements represent the twotypes ofcorrect classiﬁcation and the off-diagonal elements represent the twotypes oferror.In the multiclass case this generalizes to a square matrix whosesize is the number ofclasses,and again the diagonal elements represent the costofcorrect classiﬁcation.Table 5.5(a) and (b) shows default cost matrixes for thetwo- and three-class cases whose values simply give the number oferrors:mis-classiﬁcation costs are all 1.Taking the cost matrix into account replaces the success rate by the averagecost (or,thinking more positively,proﬁt) per decision.Although we will not doso here,a complete ﬁnancial analysis ofthe decision-making process might alsotake into account the cost ofusing the machine learning tool—including thecost ofgathering the training data—and the cost ofusing the model,or deci-sion structure,that it produces—that is,the cost ofdetermining the attributesfor the test instances.Ifall costs are known,and the projected number ofthe164CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDTable 5.5Default cost matrixes: (a) a two-class case and (b) a three-class case.Predicted Predicted classclassyesnoabcActualyes01Actuala011classno10classb101c110(a)(b)P088407-Ch005.qxd  4/30/05  11:21 AM  Page 164different outcomes in the cost matrix can be estimated—say,using cross-validation—it is straightforward to perform this kind ofﬁnancial analysis.Given a cost matrix,you can calculate the cost ofa particular learned modelon a given test set just by summing the relevant elements ofthe cost matrix forthe model’s prediction for each test instance.Here,the costs are ignored whenmaking predictions,but taken into account when evaluating them.Ifthe model outputs the probability associated with each prediction,it canbe adjusted to minimize the expected cost ofthe predictions.Given a set ofpre-dicted probabilities for each outcome on a certain test instance,one normallyselects the most likely outcome.Instead,the model could predict the class withthe smallest expected misclassiﬁcation cost.For example,suppose in a three-class situation the model assigns the classes a,b,and cto a test instance withprobabilities pa,pb,and pc,and the cost matrix is that in Table 5.5(b).Ifit pre-dicts a,the expected cost ofthe prediction is obtained by multiplying the ﬁrstcolumn ofthe matrix,[0,1,1],by the probability vector,[pa,pb,pc],yielding pb+pcor 1 -pabecause the three probabilities sum to 1.Similarly,the costs forpredicting the other two classes are 1 -pband 1 -pc.For this cost matrix,choos-ing the prediction with the lowest expected cost is the same as choosing the onewith the greatest probability.For a different cost matrix it might be different.We have assumed that the learning method outputs probabilities,as NaïveBayes does.Even ifthey do not normally output probabilities,most classiﬁerscan easily be adapted to compute them.In a decision tree,for example,the prob-ability distribution for a test instance is just the distribution ofclasses at thecorresponding leaf.Cost-sensitive learningWe have seen how a classiﬁer,built without taking costs into consideration,canbe used to make predictions that are sensitive to the cost matrix.In this case,costs are ignored at training time but used at prediction time.An alternative isto do just the opposite:take the cost matrix into account during the trainingprocess and ignore costs at prediction time.In principle,better performancemight be obtained ifthe classiﬁer were tailored by the learning algorithm to thecost matrix.In the two-class situation,there is a simple and general way to make anylearning method cost sensitive.The idea is to generate training data with a dif-ferent proportion ofyesand noinstances.Suppose that you artiﬁcially increasethe number ofnoinstances by a factor of10 and use the resulting dataset fortraining.Ifthe learning scheme is striving to minimize the number oferrors,itwill come up with a decision structure that is biased toward avoiding errors onthe noinstances,because such errors are effectively penalized 10-fold.Ifdata5.7COUNTING THE COST165P088407-Ch005.qxd  4/30/05  11:21 AM  Page 165with the original proportion ofnoinstances is used for testing,fewer errors willbe made on these than on yesinstances—that is,there will be fewer false posi-tives than false negatives—because false positives have been weighted 10 timesmore heavily than false negatives.Varying the proportion ofinstances in thetraining set is a general technique for building cost-sensitive classiﬁers.One way to vary the proportion oftraining instances is to duplicate instancesin the dataset.However,many learning schemes allow instances to be weighted.(As we mentioned in Section 3.2,this is a common technique for handlingmissing values.) Instance weights are normally initialized to one.To build cost-sensitive trees the weights can be initialized to the relative cost ofthe two kindsoferror,false positives and false negatives.Lift chartsIn practice,costs are rarely known with any degree ofaccuracy,and people willwant to ponder various scenarios.Imagine you’re in the direct mailing businessand are contemplating a mass mailout ofa promotional offer to 1,000,000households—most ofwhom won’t respond,ofcourse.Let us say that,based onprevious experience,the proportion who normally respond is known to be 0.1%(1000 respondents).Suppose a data mining tool is available that,based onknown information about the households,identiﬁes a subset of100,000 forwhich the response rate is 0.4% (400 respondents).It may well pay offto restrictthe mailout to these 100,000 households—that depends on the mailing costcompared with the return gained for each response to the offer.In marketingterminology,the increase in response rate,a factor offour in this case,is knownas the liftfactor yielded by the learning tool.Ifyou knew the costs,you coulddetermine the payoffimplied by a particular lift factor.But you probably want to evaluate other possibilities,too.The same datamining scheme,with different parameter settings,may be able to identify400,000 households for which the response rate will be 0.2% (800 respondents),corresponding to a lift factor oftwo.Again,whether this would be a more prof-itable target for the mailout can be calculated from the costs involved.It maybe necessary to factor in the cost ofcreating and using the model—includingcollecting the information that is required to come up with the attribute values.After all,ifdeveloping the model is very expensive,a mass mailing may be morecost effective than a targeted one.Given a learning method that outputs probabilities for the predicted class ofeach member ofthe set oftest instances (as Naïve Bayes does),your job is toﬁnd subsets oftest instances that have a high proportion ofpositive instances,higher than in the test set as a whole.To do this,the instances should be sortedin descending order ofpredicted probability ofyes.Then,to ﬁnd a sample ofagiven size with the greatest possible proportion ofpositive instances,just read166CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 166the requisite number ofinstances offthe list,starting at the top.Ifeach testinstance’s class is known,you can calculate the lift factor by simply counting thenumber ofpositive instances that the sample includes,dividing by the samplesize to obtain a success proportion and dividing by the success proportion forthe complete test set to determine the lift factor.Table 5.6 shows an example for a small dataset with 150 instances,ofwhich50 are yesresponses—an overall success proportion of33%.The instances havebeen sorted in descending probability order according to the predicted proba-bility ofa yesresponse.The ﬁrst instance is the one that the learning schemethinks is most likely to be positive,the second is the next most likely,and so on.The numeric values ofthe probabilities are unimportant:rank is the only thingthat matters.With each rank is given the actual class ofthe instance.Thus thelearning method was right about items 1 and 2—they are indeed positives—butwrong about item 3,which turned out to be a negative.Now,ifyou were seekingthe most promising sample ofsize 10 but only knew the predicted probabilitiesand not the actual classes,your best bet would be the top ten ranking instances.Eight ofthese are positive,so the success proportion for this sample is 80%,cor-responding to a lift factor offour.Ifyou knew the different costs involved,you could work them out for eachsample size and choose the most proﬁtable.But a graphical depiction ofthevarious possibilities will often be far more revealing than presenting a single“optimal”decision.Repeating the preceding operation for different-sizedsamples allows you to plot a lift chart like that ofFigure 5.1.The horizontal axisshows the sample size as a proportion ofthe total possible mailout.The verti-cal axis shows the number ofresponses obtained.The lower left and upper rightpoints correspond to no mailout at all,with a response of0,and a full mailout,with a response of1000.The diagonal line gives the expected result for different-5.7COUNTING THE COST167Table 5.6Data for a lift chart.RankPredicted Actual classRankPredicted Actual classprobabilityprobability10.95yes110.77no20.93yes120.76yes30.93no130.73yes40.88yes140.65no50.86yes150.63yes60.85yes160.58no70.82yes170.56yes80.80yes180.49no90.80no190.48yes100.79yes.........P088407-Ch005.qxd  4/30/05  11:21 AM  Page 167sized random samples.But we do not choose random samples;we choose thoseinstances which,according to the data mining tool,are most likely to generatea positive response.These correspond to the upper line,which is derived bysumming the actual responses over the corresponding percentage ofthe instancelist sorted in probability order.The two particular scenarios described previ-ously are marked:a 10% mailout that yields 400 respondents and a 40% onethat yields 800.Where you’d like to be in a lift chart is near the upper left-hand corner:atthe very best,1000 responses from a mailout ofjust 1000,where you send onlyto those households that will respond and are rewarded with a 100% successrate.Any selection procedure worthy ofthe name will keep you above the diag-onal—otherwise,you’d be seeing a response that was worse than for randomsampling.So the operating part ofthe diagram is the upper triangle,and thefarther to the northwest the better.ROC curvesLift charts are a valuable tool,widely used in marketing.They are closely relatedto a graphical technique for evaluating data mining schemes known as ROCcurves,which are used in just the same situation as the preceding one,in whichthe learner is trying to select samples oftest instances that have a high propor-tion ofpositives.The acronym stands for receiver operating characteristic,a termused in signal detection to characterize the tradeoffbetween hit rate and falsealarm rate over a noisy channel.ROC curves depict the performance ofa clas-siﬁer without regard to class distribution or error costs.They plot the number168CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNED02004006008001000020%40%60%80%100%sample sizenumber of respondentsFigure 5.1A hypothetical lift chart.P088407-Ch005.qxd  4/30/05  11:21 AM  Page 168ofpositives included in the sample on the vertical axis,expressed as a percent-age ofthe total number ofpositives,against the number ofnegatives includedin the sample,expressed as a percentage ofthe total number ofnegatives,onthe horizontal axis.The vertical axis is the same as that ofthe lift chart exceptthat it is expressed as a percentage.The horizontal axis is slightly different—number ofnegatives rather than sample size.However,in direct marketing sit-uations in which the proportion ofpositives is very small anyway (like 0.1%),there is negligible difference between the size ofa sample and the number ofnegatives it contains,so the ROC curve and lift chart look very similar.As withlift charts,the northwest corner is the place to be.Figure 5.2 shows an example ROC curve—the jagged line—for the sample oftest data in Table 5.6.You can follow it along with the table.From the origin,go up two (two positives),along one (one negative),up ﬁve (ﬁve positives),along one (one negative),up one,along one,up two,and so on.Each point cor-responds to drawing a line at a certain position on the ranked list,counting theyes’s and no’s above it,and plotting them vertically and horizontally,respectively.As you go farther down the list,corresponding to a larger sample,the numberofpositives and negatives both increase.The jagged ROC line in Figure 5.2 depends intimately on the details oftheparticular sample oftest data.This sample dependence can be reduced by apply-ing cross-validation.For each different number ofno’s—that is,each positionalong the horizontal axis—take just enough ofthe highest-ranked instances toinclude that number ofno’s,and count the number ofyes’s they contain.Finally,average that number over different folds ofthe cross-validation.The result is a5.7COUNTING THE COST169true positives100%80%60%40%20%0false positives100%080%60%40%20%Figure 5.2A sample ROC curve.P088407-Ch005.qxd  4/30/05  11:21 AM  Page 169smooth curve like that in Figure 5.2—although in reality such curves do notgenerally look quite so smooth.This is just one way ofusing cross-validation to generate ROC curves.Asimpler approach is to collect the predicted probabilities for all the various testsets (ofwhich there are 10 in a 10-fold cross-validation),along with the trueclass labels ofthe corresponding instances,and generate a single ranked listbased on this data.This assumes that the probability estimates from the classi-ﬁers built from the different training sets are all based on equally sized randomsamples ofthe data.It is not clear which method is preferable.However,thelatter method is easier to implement.Ifthe learning scheme does not allow the instances to be ordered,you canﬁrst make it cost sensitive as described earlier.For each fold ofa 10-fold cross-validation,weight the instances for a selection ofdifferent cost ratios,train thescheme on each weighted set,count the true positives and false positives in thetest set,and plot the resulting point on the ROC axes.(It doesn’t matter whetherthe test set is weighted or not because the axes in the ROC diagram are expressedas the percentage oftrue and false positives.) However,for inherently cost-sensitive probabilistic classiﬁers such as Naïve Bayes it is far more costly thanthe method described previously because it involves a separate learning problemfor every point on the curve.It is instructive to look at cross-validated ROC curves obtained using differ-ent learning methods.For example,in Figure 5.3,method A excels ifa small,focused sample is sought;that is,ifyou are working toward the left-hand sideofthe graph.Clearly,ifyou aim to cover just 40% ofthe true positives you170CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNED020%40%60%80%100%020%40%60%80%100%false positivestrue positivesABFigure 5.3ROC curves for two learning methods.P088407-Ch005.qxd  4/30/05  11:21 AM  Page 170should choose method A,which gives a false positive rate ofaround 5%,ratherthan method B,which gives more than 20% false positives.But method B excelsifyou are planning a large sample:ifyou are covering 80% ofthe true positives,method B will give a false positive rate of60% as compared with method A’s80%.The shaded area is called the convex hullofthe two curves,and you shouldalways operate at a point that lies on the upper boundary ofthe convex hull.What about the region in the middle where neither method A nor methodB lies on the convex hull? It is a remarkable fact that you can get anywhere inthe shaded region by combining methods A and B and using them at randomwith appropriate probabilities.To see this,choose a particular probability cutofffor method A that gives true and false positive rates oftAand fA,respectively,and another cutofffor method B that gives tBand fB.Ifyou use these twoschemes at random with probability pand q,where p +q=1,then you will gettrue and false positive rates ofp.tA+q.tBand p.fA+q.fB.This represents a pointlying on the straight line joining the points (tA,fA) and (tB,fB),and by varying pand qyou can trace out the entire line between these two points.Using thisdevice,the entire shaded region can be reached.Only ifa particular scheme gen-erates a point that lies on the convex hull should it be used alone:otherwise,itwould always be better to use a combination ofclassiﬁers corresponding to apoint that lies on the convex hull.Recall–precision curvesPeople have grappled with the fundamental tradeoffillustrated by lift charts andROC curves in a wide variety ofdomains.Information retrieval is a goodexample.Given a query,a Web search engine produces a list ofhits that repre-sent documents supposedly relevant to the query.Compare one system thatlocates 100 documents,40 ofwhich are relevant,with another that locates 400documents,80 ofwhich are relevant.Which is better? The answer should nowbe obvious:it depends on the relative cost offalse positives,documents that arereturned that aren’t relevant,and false negatives,documents that are relevantthat aren’t returned.Information retrieval researchers deﬁne parameters calledrecalland precision:For example,ifthe list ofyes’s and no’s in Table 5.6 represented a ranked list ofretrieved documents and whether they were relevant or not,and the entire collection contained a total of40 relevant documents,then “recall at 10”wouldprecisionnumber of documents retrieved that are relevanttotal number of documents that are retrieved=.recallnumber of documents retrieved that are relevanttotal number of documents that are relevant=5.7COUNTING THE COST171P088407-Ch005.qxd  4/30/05  11:21 AM  Page 171refer to recall for the top ten documents,that is,8/40 =5%;while “precision at10”would be 8/10 =80%.Information retrieval experts use recall–precisioncurvesthat plot one against the other,for different numbers ofretrieved docu-ments,in just the same way as ROC curves and lift charts—except that becausethe axes are different,the curves are hyperbolic in shape and the desired oper-ating point is toward the upper right.DiscussionTable 5.7 summarizes the three different ways we have met ofevaluating thesame basic tradeoff;TP,FP,TN,and FN are the number oftrue positives,falsepositives,true negatives,and false negatives,respectively.You want to choose aset ofinstances with a high proportion ofyesinstances and a high coverage ofthe yesinstances:you can increase the proportion by (conservatively) using asmaller coverage,or (liberally) increase the coverage at the expense ofthe pro-portion.Different techniques give different tradeoffs,and can be plotted as dif-ferent lines on any ofthese graphical charts.People also seek single measures that characterize performance.Two that areused in information retrieval are 3-point average recall,which gives the averageprecision obtained at recall values of20%,50%,and 80%,and 11-point averagerecall,which gives the average precision obtained at recall values of0%,10%,20%,30%,40%,50%,60%,70%,80%,90%,and 100%.Also used in informa-tion retrieval is the F-measure,which is:22¥¥+=◊◊++recallprecisionrecallprecisionTP2TPFPFN172CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDTable 5.7Different measures used to evaluate the false positive versus the false negative tradeoff.DomainPlotAxesExplanation of axeslift chartmarketingTP vs.TPnumber of true positivessubset sizesubset sizeROC curvecommunicationsTP rate vs.TP rateFP rateFP raterecall–precisioninformationrecall vs.recallsame as TP rate tpcurveretrievalprecisionprecisionTPTPFP+¥100%fp=+¥FPFPTN100%tp=+¥TPTPFN100%TPFPTPFPTNFN++++¥100%P088407-Ch005.qxd  4/30/05  11:21 AM  Page 172Different terms are used in different domains.Medics,for example,talk aboutthe sensitivityand speciﬁcityofdiagnostic tests.Sensitivity refers to the propor-tion ofpeople with disease who have a positive test result,that is,tp.Speciﬁcityrefers to the proportion ofpeople without disease who have a negative testresult,which is 1 -fp.Sometimes the product ofthese is used as an overallmeasure:Finally,ofcourse,there is our old friend the success rate:To summarize ROC curves in a single quantity,people sometimes use the areaunder the curve (AUC) because,roughly speaking the larger the area the betterthe model.The area also has a nice interpretation as the probability that theclassiﬁer ranks a randomly chosen positive instance above a randomly chosennegative one.Although such measures may be useful ifcosts and class distri-butions are unknown and one method must be chosen to handle all situations,no single number is able to capture the tradeoff.That can only be done by two-dimensional depictions such as lift charts,ROC curves,and recall–preci-sion diagrams.Cost curvesROC curves and their relatives are very useful for exploring the tradeoffs amongdifferent classiﬁers over a range ofcosts.However,they are not ideal for evalu-ating machine learning models in situations with known error costs.Forexample,it is not easy to read offthe expected cost ofa classiﬁer for a ﬁxed costmatrix and class distribution.Neither can you easily determine the ranges ofapplicability ofdifferent classiﬁers.For example,from the crossover pointbetween the two ROC curves in Figure 5.3 it is hard to tell for what cost andclass distributions classiﬁer A outperforms classiﬁer B.Cost curvesare a different kind ofdisplay on which a single classiﬁer corre-sponds to a straight line that shows how the performance varies as the class dis-tribution changes.Again,they work best in the two-class case,although you canalways make a multiclass problem into a two-class one by singling out one classand evaluating it against the remaining ones.Figure 5.4(a) plots the expected error against the probability ofone oftheclasses.You could imagine adjusting this probability by resampling the test setin a nonuniform way.We denote the two classes using +and -.The diagonalsshow the performance oftwo extreme classiﬁers:one always predicts +,givingTPTNTPFPTNFN++++.sensitivityspecificityTPTNTPFNFPTN¥=-()=◊+()◊+()tpfp15.7COUNTING THE COST173P088407-Ch005.qxd  4/30/05  11:21 AM  Page 173an expected error ofone ifthe dataset contains no +instances and zero ifall itsinstances are +;the other always predicts -,giving the opposite performance.The dashed horizontal line shows the performance ofthe classiﬁer that is alwayswrong,and the X-axis itselfrepresents the classiﬁer that is always correct.Inpractice,ofcourse,neither ofthese is realizable.Good classiﬁers have low error rates,so where you want to be is as close to the bottom ofthe diagram aspossible.The line marked A represents the error rate ofa particular classiﬁer.Ifyoucalculate its performance on a certain test set,its false positive rate fpis itsexpected error on a subsample ofthe test set that contains only negative exam-ples (p[+] =0),and its false negative rate fnis the error on a subsample thatcontains only positive examples (p[+] =1).These are the values ofthe inter-cepts at the left and right,respectively.You can see immediately from the plotthat ifp[+] is smaller than about 0.2,predictor A is outperformed by the extremeclassiﬁer that always predicts -,and ifit is larger than about 0.65,the otherextreme classiﬁer is better.174CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDprobability p [+]expectederroralways pick –always pick +always wrongalways rightAfnfp000.510.51(a)Figure 5.4Effect ofvarying the probability threshold:(a) the error curve and (b) thecost curve.P088407-Ch005.qxd  4/30/05  11:21 AM  Page 174So far we have not taken costs into account,or rather we have used the defaultcost matrix in which all errors cost the same.Cost curves,which do take costinto account,look very similar—very similar indeed—but the axes are differ-ent.Figure 5.4(b) shows a cost curve for the same classiﬁer A (note that the ver-tical scale has been enlarged,for convenience,and ignore the gray lines for now).It plots the expected cost ofusing A against the probability cost function,whichis a distorted version ofp[+] that retains the same extremes:zero when p[+] =0 and one when p[+] =1.Denote by C[+|-] the cost ofpredicting +when theinstance is actually –,and the reverse by C[-|+].Then the axes ofFigure 5.4(b)areWe are assuming here that correct predictions have no cost:C[+|+] =C[-|-] =0.Ifthat is not the case the formulas are a little more complex.The maximum value that the normalized expected cost can have is 1—thatis why it is “normalized.”One nice thing about cost curves is that the extremeNormalized expected costProbability cost function=¥+[]+¥-+[]()+[]=+[]+-[]+[]+-[]+-[]-+[]fnpfppppCpCpCCCC1.5.7COUNTING THE COST175probability cost function pC [+]normalizedexpectedcostfnfp000.250.50.51AB(b)Figure 5.4(continued)P088407-Ch005.qxd  4/30/05  11:21 AM  Page 175cost values at the left and right sides ofthe graph are fpand fn,just as they arefor the error curve,so you can draw the cost curve for any classiﬁer very easily.Figure 5.4(b) also shows classiﬁer B,whose expected cost remains the sameacross the range—that is,its false positive and false negative rates are equal.Asyou can see,it outperforms classiﬁer A ifthe probability cost function exceedsabout 0.45,and knowing the costs we could easily work out what this corre-sponds to in terms ofclass distribution.In situations that involve different classdistributions,cost curves make it easy to tell when one classiﬁer will outper-form another.In what circumstances might this be useful? To return to the example ofpre-dicting when cows will be in estrus,their 30-day cycle,or 1/30 prior probabil-ity,is unlikely to vary greatly (barring a genetic cataclysm!).But a particularherd may have different proportions ofcows that are likely to reach estrus inany given week,perhaps synchronized with—who knows?—the phase ofthemoon.Then,different classiﬁers would be appropriate at different times.In theoil spill example,different batches ofdata may have different spill probabilities.In these situations cost curves can help to show which classiﬁer to use when.Each point on a lift chart,ROC curve,or recall–precision curve represents aclassiﬁer,typically obtained using different threshold values for a method suchas Naïve Bayes.Cost curves represent each classiﬁer using a straight line,and asuite ofclassiﬁers will sweep out a curved envelope whose lower limit showshow well that type ofclassiﬁer can do ifthe parameter is well chosen.Figure5.4(b) indicates this with a few gray lines.Ifthe process were continued,it wouldsweep out the dotted parabolic curve.The operating region ofclassiﬁer B ranges from a probability cost value ofabout 0.25 to a value ofabout 0.75.Outside this region,classiﬁer B is outper-formed by the trivial classiﬁers represented by dashed lines.Suppose we decideto use classiﬁer B within this range and the appropriate trivial classiﬁer belowand above it.All points on the parabola are certainly better than this scheme.But how much better? It is hard to answer such questions from an ROC curve,but the cost curve makes them easy.The performance difference is negligible ifthe probability cost value is around 0.5,and below a value ofabout 0.2 andabove 0.8 it is barely perceptible.The greatest difference occurs at probabilitycost values of0.25 and 0.75 and is about 0.04,or 4% ofthe maximum possiblecost ﬁgure.5.8Evaluating numeric predictionAll the evaluation measures we have described pertain to classiﬁcation situa-tions rather than numeric prediction situations.The basic principles—using anindependent test set rather than the training set for performance evaluation,the176CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 176holdout method,and cross-validation—apply equally well to numeric predic-tion.But the basic quality measure offered by the error rate is no longer appro-priate:errors are not simply present or absent;they come in different sizes.Several alternative measures,summarized in Table 5.8,can be used to evalu-ate the success ofnumeric prediction.The predicted values on the test instancesare p1,p2,...,pn;the actual values are a1,a2,...,an.Notice that pimeans some-thing very different here from what it did in the last section:there it was theprobability that a particular prediction was in the ith class;here it refers to thenumeric value ofthe prediction for the ith test instance.Mean-squared erroris the principal and most commonly used measure;sometimes the square root is taken to give it the same dimensions as the pre-dicted value itself.Many mathematical techniques (such as linear regression,explained in Chapter 4) use the mean-squared error because it tends to be theeasiest measure to manipulate mathematically:it is,as mathematicians say,“wellbehaved.”However,here we are considering it as a performance measure:all theperformance measures are easy to calculate,so mean-squared error has no par-ticular advantage.The question is,is it an appropriate measure for the task athand?Mean absolute erroris an alternative:just average the magnitude ofthe indi-vidual errors without taking account oftheir sign.Mean-squared error tends toexaggerate the effect ofoutliers—instances whose prediction error is larger thanthe others—but absolute error does not have this effect:all sizes oferror aretreated evenly according to their magnitude.Sometimes it is the relativerather than absoluteerror values that are ofimpor-tance.For example,ifa 10% error is equally important whether it is an error of50 in a prediction of500 or an error of0.2 in a prediction of2,then averagesofabsolute error will be meaningless:relative errors are appropriate.This effectwould be taken into account by using the relative errors in the mean-squarederror calculation or the mean absolute error calculation.Relative squared errorin Table 5.8 refers to something quite different.Theerror is made relative to what it would have been ifa simple predictor had beenused.The simple predictor in question is just the average ofthe actual values from the training data.Thus relative squared error takes the total squarederror and normalizes it by dividing by the total squared error ofthe default predictor.The next error measure goes by the glorious name ofrelative absolute errorand is just the total absolute error,with the same kind ofnormalization.In thesethree relative error measures,the errors are normalized by the error ofthesimple predictor that predicts average values.The ﬁnal measure in Table 5.8 is the correlation coefﬁcient,which measuresthe statistical correlation between the a’s and the p’s.The correlation coefﬁcientranges from 1 for perfectly correlated results,through 0 when there is no cor-5.8EVALUATING NUMERIC PREDICTION177P088407-Ch005.qxd  4/30/05  11:21 AM  Page 177relation,to -1 when the results are perfectly correlated negatively.Ofcourse,negative values should not occur for reasonable prediction methods.Correla-tion is slightly different from the other measures because it is scale independentin that,ifyou take a particular set ofpredictions,the error is unchanged ifallthe predictions are multiplied by a constant factor and the actual values are leftunchanged.This factor appears in every term ofSPAin the numerator and inevery term ofSPin the denominator,thus canceling out.(This is not true forthe relative error ﬁgures,despite normalization:ifyou multiply all the predic-tions by a large constant,then the difference between the predicted and theactual values will change dramatically,as will the percentage errors.) It is alsodifferent in that good performance leads to a large value ofthe correlation coef-ﬁcient,whereas because the other methods measure error,good performance isindicated by small values.Which ofthese measures is appropriate in any given situation is a matter thatcan only be determined by studying the application itself.What are we tryingto minimize? What is the cost ofdifferent kinds oferror? Often it is not easy todecide.The squared error measures and root squared error measures weigh large178CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDTable 5.8Performance measures for numeric prediction*.Performance measureFormulamean-squared errorroot mean-squared errormean absolute errorrelative squared errorroot relative squared errorrelative absolute errorcorrelation coefﬁcient*pare predicted values and aare actual values.SppnSaanpiiAii=-()-=-()-ÂÂ2211, and SSSSppaanPAPAPAiii,, where =-()-()-Â1papaaaaannn111-+--+-++......papaaaaannn1122122-()+-()-()+-()++......papaaaaaanannnii11221221-()+-()-()+-()=++Â......, where papannn11-++-...papannn1122-()++-()...papannn1122-()+-()+...P088407-Ch005.qxd  4/30/05  11:21 AM  Page 178discrepancies much more heavily than small ones,whereas the absolute errormeasures do not.Taking the square root (root mean-squared error) just reducesthe ﬁgure to have the same dimensionality as the quantity being predicted.Therelative error ﬁgures try to compensate for the basic predictability or unpre-dictability ofthe output variable:ifit tends to lie fairly close to its average value,then you expect prediction to be good and the relative ﬁgure compensate forthis.Otherwise,ifthe error ﬁgure in one situation is far greater than that inanother situation,it may be because the quantity in the ﬁrst situation is inher-ently more variable and therefore harder to predict,not because the predictoris any worse.Fortunately,it turns out that in most practical situations the best numericprediction method is still the best no matter which error measure is used.Forexample,Table 5.9 shows the result offour different numeric prediction tech-niques on a given dataset,measured using cross-validation.Method D is the bestaccording to all ﬁve metrics:it has the smallest value for each error measure andthe largest correlation coefﬁcient.Method C is the second best by all ﬁve metrics.The performance ofmethods A and B is open to dispute:they have the samecorrelation coefﬁcient,method A is better than method B according to bothmean-squared and relative squared errors,and the reverse is true for bothabsolute and relative absolute error.It is likely that the extra emphasis that thesquaring operation gives to outliers accounts for the differences in this case.When comparing two different learning schemes that involve numeric pre-diction,the methodology developed in Section 5.5 still applies.The only dif-ference is that success rate is replaced by the appropriate performance measure(e.g.,root mean-squared error) when performing the signiﬁcance test.5.9The minimum description length principleWhat is learned by a machine learning method is a kind of“theory”ofthedomain from which the examples are drawn,a theory that is predictive in that5.9THE MINIMUM DESCRIPTION LENGTH PRINCIPLE179Table 5.9Performance measures for four numeric prediction models.ABCDroot mean-squared error67.891.763.357.4mean absolute error41.338.533.429.2root relative squared error42.2%57.2%39.4%35.8%relative absolute error43.1%40.1%34.8%30.4%correlation coefﬁcient0.880.880.890.91P088407-Ch005.qxd  4/30/05  11:21 AM  Page 179it is capable ofgenerating new facts about the domain—in other words,the classofunseen instances.Theory is a rather grandiose term:we are using it here onlyin the sense ofa predictive model.Thus theories might comprise decision treesor sets ofrules—they don’t have to be any more “theoretical”than that.There is a long-standing tradition in science that,other things being equal,simple theories are preferable to complex ones.This is known as Occam’s razorafter the medieval philosopher William ofOccam (or Ockham).Occam’s razorshaves philosophical hairs offa theory.The idea is that the best scientiﬁc theoryis the smallest one that explains all the facts.As Albert Einstein is reputed tohave said,“Everything should be made as simple as possible,but no simpler.”Ofcourse,quite a lot is hidden in the phrase “other things being equal,”and itcan be hard to assess objectively whether a particular theory really does “explain”all the facts on which it is based—that’s what controversy in science is all about.In our case,in machine learning,most theories make errors.Ifwhat is learnedis a theory,then the errors it makes are like exceptionsto the theory.One wayto ensure that other things areequal is to insist that the information embodiedin the exceptions is included as part ofthe theory when its “simplicity”is judged.Imagine an imperfect theory for which there are a few exceptions.Not all thedata is explained by the theory,but most is.What we do is simply adjoin theexceptions to the theory,specifying them explicitly as exceptions.This newtheory is larger:that is a price that,quite justiﬁably,has to be paid for its inabil-ity to explain all the data.However,it may be that the simplicity—is it too muchto call it elegance?—ofthe original theory is sufﬁcient to outweigh the fact thatit does not quite explain everything compared with a large,baroque theory thatis more comprehensive and accurate.For example,ifKepler’s three laws ofplanetary motion did not at the timeaccount for the known data quite so well as Copernicus’s latest reﬁnement ofthe Ptolemaic theory ofepicycles,they had the advantage ofbeing far lesscomplex,and that would have justiﬁed any slight apparent inaccuracy.Keplerwas well aware ofthe beneﬁts ofhaving a theory that was compact,despite thefact that his theory violated his own aesthetic sense because it depended on“ovals”rather than pure circular motion.He expressed this in a forcefulmetaphor:“I have cleared the Augean stables ofastronomy ofcycles and spirals,and left behind me only a single cartload ofdung.”The minimum description lengthor MDL principle takes the stance that thebest theory for a body ofdata is one that minimizes the size ofthe theory plusthe amount ofinformation necessary to specify the exceptions relative to thetheory—the smallest cartload ofdung.In statistical estimation theory,this hasbeen applied successfully to various parameter-ﬁtting problems.It applies tomachine learning as follows:given a set ofinstances,a learning method infersa theory—be it ever so simple;unworthy,perhaps,to be called a “theory”—fromthem.Using a metaphor ofcommunication,imagine that the instances are to180CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 180be transmitted through a noiseless channel.Any similarity that is detectedamong them can be exploited to give a more compact coding.According to theMDL principle,the best generalization is the one that minimizes the number ofbits required to communicate the generalization,along with the examples fromwhich it was made.Now the connection with the informational loss function introduced inSection 5.6 should be starting to emerge.That function measures the error interms ofthe number ofbits required to transmit the instances,given the prob-abilistic predictions made by the theory.According to the MDL principle weneed to add to this the “size”ofthe theory in bits,suitably encoded,to obtainan overall ﬁgure for complexity.However,the MDL principle refers to theinformation required to transmit the examples from which the theory wasformed,that is,the traininginstances—not a test set.The overﬁtting problemis avoided because a complex theory that overﬁts will be penalized relative to asimple one by virtue ofthe fact that it takes more bits to encode.At one extremeis a very complex,highly overﬁtted theory that makes no errors on the trainingset.At the other is a very simple theory—the null theory—which does not helpat all when transmitting the training set.And in between are theories ofinter-mediate complexity,which make probabilistic predictions that are imperfectand need to be corrected by transmitting some information about the trainingset.The MDL principle provides a means ofcomparing all these possibilities onan equal footing to see which is the best.We have found the holy grail:an eval-uation scheme that works on the training set alone and does not need a sepa-rate test set.But the devil is in the details,as we will see.Suppose a learning method comes up with a theory T,based on a trainingset Eofexamples,that requires a certain number ofbits L[T] to encode (Lforlength).Given the theory,the training set itselfcan be encoded in a certainnumber ofbits,L[E|T].L[E|T] is in fact given by the informational loss func-tion summed over all members ofthe training set.Then the total descriptionlength oftheory plus training set isand the MDL principle recommends choosing the theory Tthat minimizes thissum.There is a remarkable connection between the MDL principle and basic prob-ability theory.Given a training set E,we seek the “most likely”theory T,that is,the theory for which the a posteriori probability Pr[T|E]—the probability afterthe examples have been seen—is maximized.Bayes’s rule ofconditional prob-ability,the same rule that we encountered in Section 4.2,dictates thatPrPrPrPrTEETTE[]=[][][].LLTET[]+[]5.9THE MINIMUM DESCRIPTION LENGTH PRINCIPLE181P088407-Ch005.qxd  4/30/05  11:21 AM  Page 181Taking negative logarithms,Maximizing the probability is the same as minimizing its negative logarithm.Now (as we saw in Section 5.6) the number ofbits required to code somethingis just the negative logarithm ofits probability.Furthermore,the ﬁnal term,logPr[E],depends solely on the training set and not on the learning method.Thus choosing the theory that maximizes the probability Pr[T|E] is tantamountto choosing the theory that minimizes—in other words,the MDL principle!This astonishing correspondence with the notion ofmaximizing the a posteriori probability ofa theory after the training set has been taken intoaccount gives credence to the MDL principle.But it also points out where the problems will sprout when the MDL principle is applied in practice.Thedifﬁculty with applying Bayes’s rule directly is in ﬁnding a suitable prior prob-ability distribution Pr[T] for the theory.In the MDL formulation,that trans-lates into ﬁnding how to code the theory Tinto bits in the most efﬁcient way.There are many ways ofcoding things,and they all depend on presuppositionsthat must be shared by encoder and decoder.Ifyou know in advance that thetheory is going to take a certain form,you can use that information to encodeit more efﬁciently.How are you going to actually encode T?The devil is in thedetails.Encoding Ewith respect to Tto obtain L[E|T] seems a little more straight-forward:we have already met the informational loss function.But actually,when you encode one member ofthe training set after another,you are encod-ing a sequencerather than a set.It is not necessary to transmit the training setin any particular order,and it ought to be possible to use that fact to reduce thenumber ofbits required.Often,this is simply approximated by subtracting logn! (where nis the number ofelements in E),which is the number ofbitsneeded to specify a particular permutation ofthe training set (and because thisis the same for all theories,it doesn’t actually affect the comparison betweenthem).But one can imagine using the frequency ofthe individual errors toreduce the number ofbits needed to code them.Ofcourse,the more sophisti-cated the method that is used to code the errors,the less the need for a theoryin the ﬁrst place—so whether a theory is justiﬁed or not depends to some extenton how the errors are coded.The details,the details.We will not go into the details ofdifferent coding methods here.The wholequestion ofusing the MDL principle to evaluate a learning scheme based solelyon the training data is an area ofactive research and vocal disagreement amongresearchers.LLETT[]+[]-[]=-[]-[]+[]loglogloglog.PrPrPrPrTEETTE182CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 182We end this section as we began,on a philosophical note.It is important toappreciate that Occam’s razor,the preference ofsimple theories over complexones,has the status ofa philosophical position or “axiom”rather than some-thing that can be proved from ﬁrst principles.Although it may seem self-evidentto us,this is a function ofour education and the times we live in.A preferencefor simplicity is—or may be—culture speciﬁc rather than absolute.The Greek philosopher Epicurus (who enjoyed good food and wine and supposedly advocated sensual pleasure—in moderation—as the highest good)expressed almost the opposite sentiment.His principle ofmultiple explanationsadvises “ifmore than one theory is consistent with the data,keep them all”onthe basis that ifseveral explanations are equally in agreement,it may be possi-ble to achieve a higher degree ofprecision by using them together—and anyway,it would be unscientiﬁc to discard some arbitrarily.This brings to mindinstance-based learning,in which all the evidence is retained to provide robustpredictions,and resonates strongly with decision combination methods such asbagging and boosting (described in Chapter 7) that actually do gain predictivepower by using multiple explanations together.5.10Applying the MDL principle to clusteringOne ofthe nice things about the MDL principle is that unlike other evaluationcriteria,it can be applied under widely different circumstances.Although insome sense equivalent to Bayes’s rule in that,as we saw previously,devising acoding scheme for theories is tantamount to assigning them a prior probabilitydistribution,schemes for coding are somehow far more tangible and easier tothink about in concrete terms than intuitive prior probabilities.To illustrate thiswe will brieﬂy describe—without entering into coding details—how you mightgo about applying the MDL principle to clustering.Clustering seems intrinsically difﬁcult to evaluate.Whereas classiﬁcation orassociation learning has an objective criterion ofsuccess—predictions made ontest cases are either right or wrong—this is not so with clustering.It seems thatthe only realistic evaluation is whether the result oflearning—the clustering—proves useful in the application context.(It is worth pointing out that really thisis the case for all types oflearning,not just clustering.)Despite this,clustering can be evaluated from a description length perspec-tive.Suppose a cluster-learning technique divides the training set Einto kclus-ters.Ifthese clusters are natural ones,it should be possible to use them to encodeEmore efﬁciently.The best clustering will support the most efﬁcient encoding.One way ofencoding the instances in Ewith respect to a given clustering isto start by encoding the cluster centers—the average value ofeach attribute overall instances in the cluster.Then,for each instance in E,transmit which cluster5.10APPLYING THE MDL PRINCIPLE TO CLUSTERING183P088407-Ch005.qxd  4/30/05  11:21 AM  Page 183it belongs to (in log2kbits) followed by its attribute values with respect to thecluster center—perhaps as the numeric difference ofeach attribute value fromthe center.Couched as it is in terms ofaverages and differences,this descrip-tion presupposes numeric attributes and raises thorny questions about how to code numbers efﬁciently.Nominal attributes can be handled in a similarmanner:for each cluster there is a probability distribution for the attributevalues,and the distributions are different for different clusters.The coding issuebecomes more straightforward:attribute values are coded with respect to therelevant probability distribution,a standard operation in data compression.Ifthe data exhibits extremely strong clustering,this technique will result ina smaller description length than simply transmitting the elements ofEwithoutany clusters.However,ifthe clustering effect is not so strong,it will likelyincrease rather than decrease the description length.The overhead oftransmit-ting cluster-speciﬁc distributions for attribute values will more than offset theadvantage gained by encoding each training instance relative to the cluster it liesin.This is where more sophisticated coding techniques come in.Once the clustercenters have been communicated,it is possible to transmit cluster-speciﬁc prob-ability distributions adaptively,in tandem with the relevant instances:theinstances themselves help to deﬁne the probability distributions,and the prob-ability distributions help to deﬁne the instances.We will not venture furtherinto coding techniques here.The point is that the MDL formulation,properlyapplied,may be ﬂexible enough to support the evaluation ofclustering.Butactually doing it satisfactorily in practice is not easy.5.11Further readingThe statistical basis ofconﬁdence tests is well covered in most statistics texts,which also give tables ofthe normal distribution and Student’s distribution.(Weuse an excellent course text,Wild and Seber 1995,which we recommend verystrongly ifyou can get hold ofit.) “Student”is the nom de plume ofa statisti-cian called William Gosset,who obtained a post as a chemist in the Guinnessbrewery in Dublin,Ireland,in 1899 and invented the t-test to handle smallsamples for quality control in brewing.The corrected resampled t-test was pro-posed by Nadeau and Bengio (2003).Cross-validation is a standard statisticaltechnique,and its application in machine learning has been extensively investi-gated and compared with the bootstrap by Kohavi (1995a).The bootstrap tech-nique itselfis thoroughly covered by Efron and Tibshirani (1993).The Kappa statistic was introduced by Cohen (1960).Ting (2002) has inves-tigated a heuristic way ofgeneralizing to the multiclass case the algorithm givenin Section 5.7 to make two-class learning schemes cost sensitive.Lift charts aredescribed by Berry and Linoff(1997).The use ofROC analysis in signal detec-184CHAPTER 5|CREDIBILITY:EVALUATING WHAT’S BEEN LEARNEDP088407-Ch005.qxd  4/30/05  11:21 AM  Page 184tion theory is covered by Egan (1975);this work has been extended for visual-izing and analyzing the behavior ofdiagnostic systems (Swets 1988) and is alsoused in medicine (Beck and Schultz 1986).Provost and Fawcett (1997) broughtthe idea ofROC analysis to the attention ofthe machine learning and datamining community.Witten et al.(1999b) explain the use ofrecall and precisionin information retrieval systems;the F-measure is described by van Rijsbergen(1979).Drummond and Holte (2000) introduced cost curves and investigatedtheir properties.The MDL principle was formulated by Rissanen (1985).Kepler’s discovery ofhis economical three laws ofplanetary motion,and his doubts about them,arerecounted by Koestler (1964).Epicurus’s principle ofmultiple explanations is mentioned by Li and Vityani(1992),quoting from Asmis (1984).5.11FURTHER READING185P088407-Ch005.qxd  4/30/05  11:21 AM  Page 185P088407-Ch005.qxd  4/30/05  11:21 AM  Page 186We have seen the basic ideas ofseveral machine learning methods and studiedin detail how to assess their performance on practical data mining problems.Now we are well prepared to look at real,industrial-strength,machine learningalgorithms.Our aim is to explain these algorithms both at a conceptual leveland with a fair amount oftechnical detail so that you can understand them fullyand appreciate the key implementation issues that arise.In truth,there is a world ofdifference between the simplistic methodsdescribed in Chapter 4 and the actual algorithms that are widely used in prac-tice.The principles are the same.So are the inputs and outputs—methods ofknowledge representation.But the algorithms are far more complex,principallybecause they have to deal robustly and sensibly with real-world problems suchas numeric attributes,missing values,and—most challenging ofall—noisy data.To understand how the various methods cope with noise,we will have to drawon some ofthe statistical knowledge that we learned in Chapter 5.Chapter 4 opened with an explanation ofhow to infer rudimentary rules andwent on to examine statistical modeling and decision trees.Then we returnedchapter6Implementations:Real Machine Learning Schemes187P088407-Ch006.qxd  4/30/05  11:18 AM  Page 187to rule induction and continued with association rules,linear models,thenearest-neighbor method ofinstance-based learning,and clustering.Thepresent chapter develops all these topics except association rules,which havealready been covered in adequate detail.We begin with decision tree induction and work up to a full description ofthe C4.5 system,a landmark decision tree program that is probably the machinelearning workhorse most widely used in practice to date.Next we describe deci-sion rule induction.Despite the simplicity ofthe idea,inducing decision rulesthat perform comparably with state-of-the-art decision trees turns out to bequite difﬁcult in practice.Most high-performance rule inducers ﬁnd an initialrule set and then reﬁne it using a rather complex optimization stage that dis-cards or adjusts individual rules to make them work better together.We describethe ideas that underlie rule learning in the presence ofnoise,and then go on tocover a scheme that operates by forming partial decision trees,an approach that has been demonstrated to perform as well as other state-of-the-art rulelearners yet avoids their complex and ad hoc heuristics.Following this,we takea brieflook at how to generate rules with exceptions,which were described inSection 3.5.There has been resurgence ofinterest in linear models with the introductionofsupport vector machines,a blend oflinear modeling and instance-based learning.Support vector machines select a small number ofcritical boundaryinstances called support vectorsfrom each class and build a linear discriminantfunction that separates them as widely as possible.These systems transcend the limitations oflinear boundaries by making it practical to include extra nonlinear terms in the function,making it possible to form quadratic,cubic,and higher-order decision boundaries.The same techniques can be applied to the perceptron described in Section 4.6 to implement complex decision bound-aries.An older technique for extending the perceptron is to connect unitstogether into multilayer “neural networks.”All these ideas are described inSection 6.3.The next section ofthe chapter describes instance-based learners,develop-ing the simple nearest-neighbor method introduced in Section 4.7 and showingsome more powerful alternatives that perform explicit generalization.Follow-ing that,we extend linear regression for numeric prediction to a more sophis-ticated procedure that comes up with the tree representation introduced inSection 3.7 and go on to describe locally weighted regression,an instance-basedstrategy for numeric prediction.Next we return to clustering and review somemethods that are more sophisticated than simple k-means,methods thatproduce hierarchical clusters and probabilistic clusters.Finally,we look atBayesian networks,a potentially very powerful way ofextending the Naïve Bayesmethod to make it less “naïve”by dealing with datasets that have internaldependencies.188CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 188Because ofthe nature ofthe material it contains,this chapter differs from theothers in the book.Sections can be read independently,and each section is self-contained,including the references to further reading,which are gatheredtogether in a Discussionsubsection at the end ofeach section.6.1Decision treesThe ﬁrst machine learning scheme that we will develop in detail derives fromthe simple divide-and-conquer algorithm for producing decision trees that wasdescribed in Section 4.3.It needs to be extended in several ways before it is readyfor use on real-world problems.First we consider how to deal with numericattributes and,after that,missing values.Then we look at the all-importantproblem ofpruning decision trees,because although trees constructed by thedivide-and-conquer algorithm as described perform well on the training set,they are usually overﬁtted to the training data and do not generalize well toindependent test sets.Next we consider how to convert decision trees to classi-ﬁcation rules.In all these aspects we are guided by the popular decision treealgorithm C4.5,which,with its commercial successor C5.0,has emerged as theindustry workhorse for off-the-shelfmachine learning.Finally,we look at theoptions provided by C4.5 and C5.0 themselves.Numeric attributesThe method we have described only works when all the attributes are nominal,whereas,as we have seen,most real datasets contain some numeric attributes.It is not too difﬁcult to extend the algorithm to deal with these.For a numericattribute we will restrict the possibilities to a two-way,or binary,split.Supposewe use the version ofthe weather data that has some numeric features (Table1.3).Then,when temperature is being considered for the ﬁrst split,the tem-perature values involved are646568697071727580818385noyesyesnoyesyesyesnoyesyesnoyesyesno(Repeated values have been collapsed together.) There are only 11 possible posi-tions for the breakpoint—8 ifthe breakpoint is not allowed to separate itemsofthe same class.The information gain for each can be calculated in the usualway.For example,the test temperature<71.5 produces four yes’s and two no’s,whereas temperature>71.5 produces ﬁve yes’s and three no’s,and so the infor-mation value ofthis test isinfo4,2info4,2info5,3bits.[][]()=()¥[]()+()¥[]()=,,.5361481409396.1DECISION TREES189P088407-Ch006.qxd  4/30/05  11:18 AM  Page 189It is common to place numeric thresholds halfway between the values thatdelimit the boundaries ofa concept,although something might be gained byadopting a more sophisticated policy.For example,we will see later thatalthough the simplest form ofinstance-based learning puts the dividing linebetween concepts in the middle ofthe space between them,other methods thatinvolve more than just the two nearest examples have been suggested.When creating decision trees using the divide-and-conquer method,once theﬁrst attribute to split on has been selected,a top-level tree node is created thatsplits on that attribute,and the algorithm proceeds recursively on each ofthechild nodes.For each numeric attribute,it appears that the subset ofinstancesat each child node must be re-sorted according to that attribute’s values—and,indeed,this is how programs for inducing decision trees are usually written.However,it is not actually necessary to re-sort because the sort order at a parentnode can be used to derive the sort order for each child,leading to a speedierimplementation.Consider the temperature attribute in the weather data,whosesort order (this time including duplicates) is64656869707172727575808183857659414812101121331The italicized number below each temperature value gives the number oftheinstance that has that value:thus instance number 7 has temperature value 64,instance 6 has temperature value 65,and so on.Suppose we decide to split atthe top level on the attribute outlook.Consider the child node for which outlook =sunny—in fact the examples with this value ofoutlookare numbers 1,2,8,9,and 11.Ifthe italicized sequence is stored with the example set (and adifferent sequence must be stored for each numeric attribute)—that is,instance7 contains a pointer to instance 6,instance 6 points to instance 5,instance 5points to instance 9,and so on—then it is a simple matter to read offthe exam-ples for which outlook =sunnyin order.All that is necessary is to scan throughthe instances in the indicated order,checking the outlookattribute for each andwriting down the ones with the appropriate value:981121Thus repeated sorting can be avoided by storing with each subset ofinstancesthe sort order for that subset according to each numeric attribute.The sort ordermust be determined for each numeric attribute at the beginning;no furthersorting is necessary thereafter.When a decision tree tests a nominal attribute as described in Section 4.3,abranch is made for each possible value ofthe attribute.However,we haverestricted splits on numeric attributes to be binary.This creates an importantdifference between numeric attributes and nominal ones:once you havebranched on a nominal attribute,you have used all the information that it offers,190CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 190whereas successive splits on a numeric attribute may continue to yield newinformation.Whereas a nominal attribute can only be tested once on any pathfrom the root ofa tree to the leaf,a numeric one can be tested many times.Thiscan yield trees that are messy and difﬁcult to understand because the tests onany single numeric attribute are not located together but can be scattered alongthe path.An alternative,which is harder to accomplish but produces a morereadable tree,is to allow a multiway test on a numeric attribute,testing againstseveral constants at a single node ofthe tree.A simpler but less powerful solu-tion is to prediscretize the attribute as described in Section 7.2.Missing valuesThe next enhancement to the decision-tree-building algorithm deals with theproblems ofmissing values.Missing values are endemic in real-world datasets.As explained in Chapter 2 (page 58),one way ofhandling them is to treat themas just another possible value ofthe attribute;this is appropriate ifthe fact thatthe attribute is missing is signiﬁcant in some way.In that case no further actionneed be taken.But ifthere is no particular signiﬁcance in the fact that a certaininstance has a missing attribute value,a more subtle solution is needed.It istempting to simply ignore all instances in which some ofthe values are missing,but this solution is often too draconian to be viable.Instances with missingvalues often provide a good deal ofinformation.Sometimes the attributeswhose values are missing play no part in the decision,in which case theseinstances are as good as any other.One question is how to apply a given decision tree to an instance in whichsome ofthe attributes to be tested have missing values.We outlined a solutionin Section 3.2 that involves notionally splitting the instance into pieces,using anumeric weighting method,and sending part ofit down each branch in pro-portion to the number oftraining instances going down that branch.Eventu-ally,the various parts ofthe instance will each reach a leafnode,and thedecisions at these leafnodes must be recombined using the weights that havepercolated to the leaves.The information gain and gain ratio calculationsdescribed in Section 4.3 can also be applied to partial instances.Instead ofhaving integer counts,the weights are used when computing both gain ﬁgures.Another question is how to partition the training set once a splitting attrib-ute has been chosen,to allow recursive application ofthe decision tree forma-tion procedure on each ofthe daughter nodes.The same weighting procedureis used.Instances for which the relevant attribute value is missing are notion-ally split into pieces,one piece for each branch,in the same proportion as theknown instances go down the various branches.Pieces ofthe instance con-tribute to decisions at lower nodes in the usual way through the informationgain calculation,except that they are weighted accordingly.They may be further6.1DECISION TREES191P088407-Ch006.qxd  4/30/05  11:18 AM  Page 191split at lower nodes,ofcourse,ifthe values ofother attributes are unknown aswell.PruningWhen we looked at the labor negotiations problem in Chapter 1,we found thatthe simple decision tree in Figure 1.3(a) actually performs better than the morecomplex one in Figure 1.3(b)—and it makes more sense too.Now it is time tolearn how to prune decision trees.By building the complete tree and pruning it afterward we are adopting astrategy ofpostpruning(sometimes called backward pruning) rather thanprepruning(or forward pruning).Prepruning would involve trying to decideduring the tree-building process when to stop developing subtrees—quite anattractive prospect because that would avoid all the work ofdeveloping subtreesonly to throw them away afterward.However,postpruning does seem to offersome advantages.For example,situations occur in which two attributes indi-vidually seem to have nothing to contribute but are powerful predictors whencombined—a sort ofcombination-lock effect in which the correct combinationofthe two attribute values is very informative whereas the attributes taken indi-vidually are not.Most decision tree builders postprune;it is an open questionwhether prepruning strategies can be developed that perform as well.Two rather different operations have been considered for postpruning:subtree replacementand subtree raising.At each node,a learning scheme mightdecide whether it should perform subtree replacement,subtree raising,or leavethe subtree as it is,unpruned.Subtree replacement is the primary pruning oper-ation,and we look at it ﬁrst.The idea is to select some subtrees and replace themwith single leaves.For example,the whole subtree in Figure 1.3(a),involvingtwo internal nodes and four leafnodes,has been replaced by the single leafbad.This will certainly cause the accuracy on the training set to decrease ifthe orig-inal tree was produced by the decision tree algorithm described previouslybecause that continued to build the tree until all leafnodes were pure (or untilall attributes had been tested).However,it may increase the accuracy on an inde-pendently chosen test set.When subtree replacement is implemented,it proceeds from the leaves andworks back up toward the root.In the Figure 1.3 example,the whole subtree inFigure 1.3(a) would not be replaced at once.First,consideration would be givento replacing the three daughter nodes in the health plan contributionsubtreewith a single leafnode.Assume that a decision is made to perform this replace-ment—we will explain how this decision is made shortly.Then,continuing towork back from the leaves,consideration would be given to replacing theworking hours per weeksubtree,which now has just two daughter nodes,with asingle leafnode.In the Figure 1.3 example this replacement was indeed made,192CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 192which accounts for the entire subtree in Figure 1.3(a) being replaced by a singleleafmarked bad.Finally,consideration would be given to replacing the two daughter nodes in the wage increase 1st yearsubtree with a single leafnode.In this case that decision was not made,so the tree remains as shown inFigure 1.3(a).Again,we will examine how these decisions are actually madeshortly.The second pruning operation,subtree raising,is more complex,and it is notclear that it is necessarily always worthwhile.However,because it is used in theinﬂuential decision tree-building system C4.5,we describe it here.Subtreeraising does not occur in the Figure 1.3 example,so use the artiﬁcial exampleofFigure 6.1 for illustration.Here,consideration is given to pruning the tree inFigure 6.1(a),and the result is shown in Figure 6.1(b).The entire subtree fromC downward has been “raised”to replace the B subtree.Note that although thedaughters ofB and C are shown as leaves,they can be entire subtrees.Ofcourse,ifwe perform this raising operation,it is necessary to reclassify the examples atthe nodes marked 4 and 5 into the new subtree headed by C.This is why thedaughters ofthat node are marked with primes:1¢,2¢,and 3¢—to indicate thatthey are not the same as the original daughters 1,2,and 3 but differ by the inclu-sion ofthe examples originally covered by 4 and 5.Subtree raising is a potentially time-consuming operation.In actual imple-mentations it is generally restricted to raising the subtree ofthe most popularbranch.That is,we consider doing the raising illustrated in Figure 6.1 providedthat the branch from B to C has more training examples than the branches fromB to node 4 or from B to node 5.Otherwise,if(for example) node 4 were themajority daughter ofB,we would consider raising node 4 to replace B andreclassifying all examples under C,as well as the examples from node 5,into thenew node.Estimating error ratesSo much for the two pruning operations.Now we must address the question ofhow to decide whether to replace an internal node with a leaf(for subtreereplacement),or whether to replace an internal node with one ofthe nodesbelow it (for subtree raising).To make this decision rationally,it is necessary toestimate the error rate that would be expected at a particular node given anindependently chosen test set.We need to estimate the error at internal nodesas well as at leafnodes.Ifwe had such an estimate,it would be clear whetherto replace,or raise,a particular subtree simply by comparing the estimated errorofthe subtree with that ofits proposed replacement.Before estimating the errorfor a subtree proposed for raising,examples that lie under siblings ofthe currentnode—the examples at nodes 4 and 5 ofFigure 6.1—would have to be tem-porarily reclassiﬁed into the raised tree.6.1DECISION TREES193P088407-Ch006.qxd  4/30/05  11:18 AM  Page 193It is no use taking the training set error as the error estimate:that would notlead to any pruning because the tree has been constructed expressly for that par-ticular training set.One way ofcoming up with an error estimate is the stan-dard veriﬁcation technique:hold back some ofthe data originally given and useit as an independent test set to estimate the error at each node.This is calledreduced-errorpruning.It suffers from the disadvantage that the actual tree isbased on less data.The alternative is to try to make some estimate oferror based on the train-ing data itself.That is what C4.5 does,and we will describe its method here.Itis a heuristic based on some statistical reasoning,but the statistical underpin-ning is rather weak and ad hoc.However,it seems to work well in practice.Theidea is to consider the set ofinstances that reach each node and imagine thatthe majority class is chosen to represent that node.That gives a certain numberof“errors,”E,out ofthe total number ofinstances,N.Now imagine that thetrue probability oferror at the node is q,and that the Ninstances are generatedby a Bernoulli process with parameter q,ofwhich Eturn out to be errors.This is almost the same situation as we considered when looking at theholdout method in Section 5.2,where we calculated conﬁdence intervals on the true success probability pgiven a certain observed success rate.There aretwo differences.One is trivial:here we are looking at the error rate qrather than the success rate p;these are simply related by p +q =1.The second is more serious:here the ﬁgures Eand Nare measured from the training data,whereas in Section 5.2 we were considering independent test data instead.Because ofthis difference,we make a pessimistic estimate ofthe error rate byusing the upper conﬁdence limit rather than by stating the estimate as a conﬁ-dence range.194CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESABC45123(a)AC1¢2¢3¢(b)Figure 6.1Example ofsubtree raising,where node C is “raised”to subsume node B.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 194The mathematics involved is just the same as before.Given a particular con-ﬁdence c(the default ﬁgure used by C4.5 is c =25%),we ﬁnd conﬁdence limitszsuch thatwhere Nis the number ofsamples,f=E/Nis the observed error rate,and qisthe true error rate.As before,this leads to an upper conﬁdence limit for q.Nowwe use that upper conﬁdence limit as a (pessimistic) estimate for the error rateeat the node:Note the use ofthe +sign before the square root in the numerator to obtain theupper conﬁdence limit.Here,zis the number ofstandard deviations corre-sponding to the conﬁdence c,which for c=25% is z=0.69.To see how all this works in practice,let’s look again at the labor negotiationsdecision tree ofFigure 1.3,salient parts ofwhich are reproduced in Figure 6.2with the number oftraining examples that reach the leaves added.We use thepreceding formula with a 25% conﬁdence ﬁgure,that is,with z =0.69.Considerthe lower left leaf,for which E =2,N =6,and so f=0.33.Plugging these ﬁguresinto the formula,the upper conﬁdence limit is calculated as e =0.47.That meansthat instead ofusing the training set error rate for this leaf,which is 33%,wewill use the pessimistic estimate of47%.This is pessimistic indeed,consideringthat it would be a bad mistake to let the error rate exceed 50% for a two-classproblem.But things are worse for the neighboring leaf,where E =1 and N =2,because the upper conﬁdence becomes e =0.72.The third leafhas the same value ofeas the ﬁrst.The next step is to combine the error estimates forthese three leaves in the ratio ofthe number ofexamples they cover,6:2:6,which leads to a combined error estimate of0.51.Now we consider the errorestimate for the parent node,health plan contribution.This covers nine badexamples and ﬁve good ones,so the training set error rate is f=5/14.For thesevalues,the preceding formula yields a pessimistic error estimate ofe =0.46.Because this is less than the combined error estimate ofthe three children,theyare pruned away.The next step is to consider the working hours per weeknode,which now hastwo children that are both leaves.The error estimate for the ﬁrst,with E=1 andN=2,is e=0.72,and for the second it is e=0.46 as we have just seen.Com-bining these in the appropriate ratio of2:14 leads to a value that is higher thanefzNzfNfNzNzN=++-++22222241.PrfqqqNzc--()>ÈÎÍ˘˚˙=1,6.1DECISION TREES195P088407-Ch006.qxd  4/30/05  11:18 AM  Page 195196CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESthe error estimate for the working hoursnode,so the subtree is pruned away andreplaced by a leafnode.The estimated error ﬁgures obtained in these examples should be taken witha grain ofsalt because the estimate is only a heuristic one and is based on anumber ofshaky assumptions:the use ofthe upper conﬁdence limit;theassumption ofa normal distribution;and the fact that statistics from the train-ing set are used.However,the qualitative behavior ofthe error formula is correctand the method seems to work reasonably well in practice.Ifnecessary,theunderlying conﬁdence level,which we have taken to be 25%,can be tweaked toproduce more satisfactory results.Complexity of decision tree inductionNow that we have learned how to accomplish the pruning operations,we haveﬁnally covered all the central aspects ofdecision tree induction.Let’s take stockand consider the computational complexity ofinducing decision trees.We willuse the standard order notation:O(n) stands for a quantity that grows at mostlinearly with n,O(n2) grows at most quadratically with n,and so on.Suppose that the training data contains ninstances and mattributes.We needto make some assumption about the size ofthe tree,and we will assume that itsdepth is on the order oflogn,that is,O(logn).This is the standard rate ofgrowth ofa tree with nleaves,provided that it remains “bushy”and doesn’tdegenerate into a few very long,stringy branches.Note that we are tacitly assum-wage increase first year≤ 2.5> 2.51 bad1 good4 bad2 good1 bad1 good4 bad2 good≤ 36health plan contribution> 36working hoursper weeknonehalffullFigure 6.2Pruning the labor negotiations decision tree.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 196ing that most ofthe instances are different from each other,and—this is almostthe same thing—that the mattributes provide enough tests to allow theinstances to be differentiated.For example,ifthere were only a few binary attri-butes,they would allow only so many instances to be differentiated and the tree could not grow past a certain point,rendering an “in the limit”analysismeaningless.The computational cost ofbuilding the tree in the ﬁrst place isConsider the amount ofwork done for one attribute over all nodes ofthe tree.Not all the examples need to be considered at each node,ofcourse.But at eachpossible tree depth,the entire set ofninstances must be considered.Becausethere are log ndifferent depths in the tree,the amount ofwork for this oneattribute is O(nlogn).At each node all attributes are considered,so the totalamount ofwork is O(mnlogn).This reasoning makes some assumptions.Ifsome attributes are numeric,theymust be sorted,but once the initial sort has been done there is no need to re-sort at each tree depth ifthe appropriate algorithm is used (described earlier onpage 190).The initial sort takes O(nlogn) operations for each ofup to mattrib-utes:thus the preceding complexity ﬁgure is unchanged.Ifthe attributes arenominal,all attributes do not have to be considered at each tree node—becauseattributes that are used further up the tree cannot be reused.However,ifattrib-utes are numeric,they can be reused and so they have to be considered at everytree level.Next,consider pruning by subtree replacement.First,an error estimate must be made for every tree node.Provided that counts are maintained appropriately,this is linear in the number ofnodes in the tree.Then each node needs to be considered for replacement.The tree has at most nleaves,onefor each instance.Ifit was a binary tree,each attribute being numeric or two-valued,that would give it 2n-1 nodes;multiway branches would only serveto decrease the number ofinternal nodes.Thus the complexity ofsubtreereplacement isFinally,subtree lifting has a basic complexity equal to subtree replacement.But there is an added cost because instances need to be reclassiﬁed during thelifting operation.During the whole process,each instance may have to be reclas-siﬁed at every node between its leafand the root,that is,as many as O(logn)times.That makes the total number ofreclassiﬁcations O(nlogn).And reclas-siﬁcation is not a single operation:one that occurs near the root will take O(logn) operations,and one ofaverage depth will take halfofthis.Thus the totalcomplexity ofsubtree lifting is as follows:On().Omnnlog.()6.1DECISION TREES197P088407-Ch006.qxd  4/30/05  11:18 AM  Page 197Taking into account all these operations,the full complexity ofdecision treeinduction isFrom trees to rulesIt is possible to read a set ofrules directly offa decision tree,as noted in Section3.3,by generating a rule for each leafand making a conjunction ofall the testsencountered on the path from the root to that leaf.This produces rules that areunambiguous in that it doesn’t matter in what order they are executed.However,the rules are more complex than necessary.The estimated error rate described previously provides exactly the mecha-nism necessary to prune the rules.Given a particular rule,each condition in itis considered for deletion by tentatively removing it,working out which ofthetraining examples are now covered by the rule,calculating from this a pes-simistic estimate ofthe error rate ofthe new rule,and comparing this with thepessimistic estimate for the original rule.Ifthe new rule is better,delete thatcondition and carry on,looking for other conditions to delete.Leave the rulewhen there are no conditions left that will improve it ifthey are removed.Onceall rules have been pruned in this way,it is necessary to see whether there areany duplicates and remove them from the rule set.This is a greedy approach to detecting redundant conditions in a rule,andthere is no guarantee that the best set ofconditions will be removed.Animprovement would be to consider all subsets ofconditions,but this is usuallyprohibitively expensive.Another solution might be to use an optimization tech-nique such as simulated annealing or a genetic algorithm to select the bestversion ofthis rule.However,the simple greedy solution seems to produce quitegood rule sets.The problem,even with the greedy method,is computational cost.For everycondition that is a candidate for deletion,the effect ofthe rule must be reeval-uated on all the training instances.This means that rule generation from treestends to be very slow,and the next section describes much faster methods thatgenerate classiﬁcation rules directly without forming a decision tree ﬁrst.C4.5: Choices and optionsWe ﬁnish our study ofdecision trees by making a few remarks about practicaluse ofthe landmark decision tree program C4.5 and its successor C5.0.Thesewere devised by J.Ross Quinlan over a 20-year period beginning in the late1970s.A complete description ofC4.5,the early 1990s version,appears as anexcellent and readable book (Quinlan 1993),along with the full source code.OOmnnnnloglog.()+()()2Onnlog()()2198CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 198The more recent version,C5.0,is available commercially.Its decision tree induc-tion seems to be essentially the same as that used by C4.5,and tests show somedifferences but negligible improvements.However,its rule generation is greatlysped up and clearly uses a different technique,although this has not beendescribed in the open literature.C4.5 works essentially as described in the preceding sections.The default con-ﬁdence value is set at 25% and works reasonably well in most cases;possibly itshould be altered to a lower value,which causes more drastic pruning,iftheactual error rate ofpruned trees on test sets is found to be much higher thanthe estimated error rate.There is one other important parameter whose effectis to eliminate tests for which almost all ofthe training examples have the sameoutcome.Such tests are often oflittle use.Consequently,tests are not incorpo-rated into the decision tree unless they have at least two outcomes that have atleast a minimum number ofinstances.The default value for this minimum is2,but it is controllable and should perhaps be increased for tasks that have a lotofnoisy data.DiscussionTop-down induction ofdecision trees is probably the most extensivelyresearched method ofmachine learning used in data mining.Researchers haveinvestigated a panoply ofvariations for almost every conceivable aspect ofthelearning process—for example,different criteria for attribute selection or modiﬁed pruning methods.However,they are rarely rewarded by substantialimprovements in accuracy over a spectrum ofdiverse datasets.Sometimes thesize ofthe induced trees is signiﬁcantly reduced when a different pruning strat-egy is adopted,but often the same effect can be achieved by setting C4.5’spruning parameter to a smaller value.In our description ofdecision trees,we have assumed that only one attribute is used to split the data into subsets at each node ofthe tree.However,it is possible to allow tests that involve several attributes at a time.For example,with numeric attributes each test can be on a linear combinationofattribute values.Then the ﬁnal tree consists ofa hierarchy oflinear modelsofthe kind we described in Section 4.6,and the splits are no longer restrictedto being axis-parallel.Trees with tests involving more than one attribute arecalled multivariatedecision trees,in contrast to the simple univariatetrees that we normally use.Multivariate tests were introduced with the classiﬁcationand regression trees(CART) system for learning decision trees (Breiman et al.1984).They are often more accurate and smaller than univariate trees but takemuch longer to generate and are also more difﬁcult to interpret.We brieﬂymention one way ofgenerating them using principal components analysis inSection 7.3 (page 309).6.1DECISION TREES199P088407-Ch006.qxd  4/30/05  11:18 AM  Page 1996.2Classiﬁcation rulesWe call the basic covering algorithm for generating rules that was described inSection 4.4 a separate-and-conquer technique because it identiﬁes a rule thatcovers instances in the class (and excludes ones not in the class),separates themout,and continues on those that are left.Such algorithms have been used as thebasis ofmany systems that generate rules.There we described a simple correct-ness-based measure for choosing what test to add to the rule at each stage.However,there are many other possibilities,and the particular criterion that is used has a signiﬁcant effect on the rules produced.We examine different criteria for choosing tests in this section.We also look at how the basic rule-generation algorithm can be extended to more practical situations by accom-modating missing values and numeric attributes.But the real problem with all these rule-generation schemes is that they tendto overﬁt the training data and do not generalize well to independent test sets,particularly on noisy data.To be able to generate good rule sets for noisy data,it is necessary to have some way ofmeasuring the real worth ofindividual rules.The standard approach to assessing the worth ofrules is to evaluate their errorrate on an independent set ofinstances,held back from the training set,and weexplain this next.After that,we describe two industrial-strength rule learners:one that combines the simple separate-and-conquer technique with a globaloptimization step and another one that works by repeatedly building partialdecision trees and extracting rules from them.Finally,we consider how to gen-erate rules with exceptions,and exceptions to the exceptions.Criteria for choosing testsWhen we introduced the basic rule learner in Section 4.4,we had to ﬁgure outa way ofdeciding which ofmany possible tests to add to a rule to prevent itfrom covering any negative examples.For this we used the test that maximizesthe ratiowhere tis the total number ofinstances that the new rule will cover,and pisthe number ofthese that are positive—that is,that belong to the class in ques-tion.This attempts to maximize the “correctness”ofthe rule on the basis thatthe higher the proportion ofpositive examples it covers,the more correct a ruleis.One alternative is to calculate an information gain:where pand tare the number ofpositive instances and the total number ofinstances covered by the new rule,as before,and Pand Tare the correspondingpptPTloglog,-ÈÎÍ˘˚˙pt200CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2006.2CLASSIFICATION RULES201number ofinstances that satisﬁed the rule beforethe new test was added.Therationale for this is that it represents the total information gained regarding the current positive examples,which is given by the number ofthem that satisfythe new test,multiplied by the information gained regarding each one.The basic criterion for choosing a test to add to a rule is to ﬁnd one thatcovers as many positive examples as possible,while covering as few negativeexamples as possible.The original correctness-based heuristic,which is just thepercentage ofpositive examples among all examples covered by the rule,attainsa maximum when no negative examples are covered regardless ofthe numberofpositive examples covered by the rule.Thus a test that makes the rule exactwill be preferred to one that makes it inexact,no matter how few positive exam-ples the former rule covers or how many positive examples the latter covers.Forexample,ifwe can choose between a test that covers one example,which is pos-itive,this criterion will prefer it over a test that covers 1000 positive examplesalong with one negative one.The information-based heuristic,on the other hand,places far more empha-sis on covering a large number ofpositive examples regardless ofwhether therule so created is exact.Ofcourse,both algorithms continue adding tests untilthe ﬁnal rule produced is exact,which means that the rule will be ﬁnished earlierusing the correctness measure,whereas more terms will have to be added iftheinformation-based measure is used.Thus the correctness-based measure mightﬁnd special cases and eliminate them completely,saving the larger picture forlater (when the more general rule might be simpler because awkward specialcases have already been dealt with),whereas the information-based one will tryto generate high-coverage rules ﬁrst and leave the special cases until later.It isby no means obvious that either strategy is superior to the other at producingan exact rule set.Moreover,the whole situation is complicated by the fact that,as described later,rules may be pruned and inexact ones tolerated.Missing values, numeric attributesAs with divide-and-conquer decision tree algorithms,the nasty practical con-siderations ofmissing values and numeric attributes need to be addressed.Infact,there is not much more to say.Now that we know how these problems canbe solved for decision tree induction,appropriate solutions for rule inductionare easily given.When producing rules using covering algorithms,missing values can best betreated as though they don’t match any ofthe tests.This is particularly suitablewhen a decision list is being produced because it encourages the learning algo-rithm to separate out positive instances using tests that are known to succeed.It has the effect that either instances with missing values are dealt with by rulesinvolving other attributes that are not missing,or any decisions about them areP088407-Ch006.qxd  4/30/05  11:18 AM  Page 201deferred until most ofthe other instances have been taken care of,at which timetests will probably emerge that involve other attributes.Covering algorithms fordecision lists have a decided advantage over decision tree algorithms in thisrespect:tricky examples can be left until late in the process,at which time theywill appear less tricky because most ofthe other examples have already beenclassiﬁed and removed from the instance set.Numeric attributes can be dealt with in exactly the same way as they are fortrees.For each numeric attribute,instances are sorted according to theattribute’s value and,for each possible threshold,a binary less-than/greater-thantest is considered and evaluated in exactly the same way that a binary attributewould be.Generating good rulesSuppose you don’t want to generate perfect rules that guarantee to give thecorrect classiﬁcation on all instances in the training set,but would rather gen-erate “sensible”ones that avoid overﬁtting the training set and thereby stand abetter chance ofperforming well on new test instances.How do you decidewhich rules are worthwhile? How do you tell when it becomes counterproduc-tive to continue adding terms to a rule to exclude a few pesky instances ofthewrong type,all the while excluding more and more instances ofthe right type,too?Let’s look at a few examples ofpossible rules—some good and some bad—for the contact lens problem in Table 1.1.Consider ﬁrst the ruleIf astigmatism =yes and tear production rate =normalthen recommendation =hardThis gives a correct result for four ofthe six cases that it covers;thus its success fraction is 4/6.Suppose we add a further term to make the rule a“perfect”one:If astigmatism =yes and tear production rate =normaland age =young then recommendation =hardThis improves accuracy to 2/2.Which rule is better? The second one is moreaccurate on the training data but covers only two cases,whereas the ﬁrst onecovers six.It may be that the second version is just overﬁtting the training data.For a practical rule learner we need a principled way ofchoosing the appropri-ate version ofa rule,preferably one that maximizes accuracy on future test data.Suppose we split the training data into two parts that we will call a growingsetand a pruning set.The growing set is used to form a rule using the basic cov-ering algorithm.Then a test is deleted from the rule,and the effect is evaluatedby trying out the truncated rule on the pruning set and seeing whether it 202CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2026.2CLASSIFICATION RULES203performs better than the original rule.This pruning process repeats until therule cannot be improved by deleting any further tests.The whole procedure isrepeated for each class,obtaining one best rule for each class,and the overallbest rule is established by evaluating the rules on the pruning set.This rule isthen added to the rule set,the instances it covers removed from the trainingdata—from both growing and pruning sets—and the process is repeated.Why not do the pruning as we build the rule up,rather than building up thewhole thing and then throwing parts away? That is,why not preprune ratherthan postprune? Just as when pruning decision trees it is often best to grow thetree to its maximum size and then prune back,so with rules it is often best tomake a perfect rule and then prune it.Who knows? Adding that last term maymake a really good rule,a situation that we might never have noticed had weadopted an aggressive prepruning strategy.It is essential that the growing and pruning sets are separate,because it is mis-leading to evaluate a rule on the very data used to form it:that would lead toserious errors by preferring rules that were overﬁtted.Usually the training set issplit so that two-thirds ofinstances are used for growing and one-third forpruning.A disadvantage,ofcourse,is that learning occurs from instances in thegrowing set only,and so the algorithm might miss important rules because somekey instances had been assigned to the pruning set.Moreover,the wrong rulemight be preferred because the pruning set contains only one-third ofthe dataand may not be completely representative.These effects can be ameliorated byresplitting the training data into growing and pruning sets at each cycle ofthealgorithm,that is,after each rule is ﬁnally chosen.The idea ofusing a separate pruning set for pruning—which is applicable todecision trees as well as rule sets—is called reduced-error pruning.The variantdescribed previously prunes a rule immediately after it has been grown and iscalled incremental reduced-error pruning.Another possibility is to build a full,unpruned rule set ﬁrst,pruning it afterwards by discarding individual tests.However,this method is much slower.Ofcourse,there are many different ways to assess the worth ofa rule basedon the pruning set.A simple measure is to consider how well the rule would doat discriminating the predicted class from other classes ifit were the only rulein the theory,operating under the closed world assumption.Ifit gets pinstancesright out ofthe tinstances that it covers,and there are Pinstances ofthis classout ofa total Tofinstances altogether,then it gets ppositive instances right.The instances that it does not cover include N-nnegative ones,where n =t-pis the number ofnegative instances that the rule covers and N =T-Pis thetotal number ofnegative instances.Thus the rule has an overall success ratio ofpNnT+-()[],P088407-Ch006.qxd  4/30/05  11:18 AM  Page 203and this quantity,evaluated on the test set,has been used to evaluate the successofa rule when using reduced-error pruning.This measure is open to criticism because it treats noncoverage ofnegativeexamples as equally important as coverage ofpositive ones,which is unrealisticin a situation where what is being evaluated is one rule that will eventually servealongside many others.For example,a rule that gets p =2000 instances rightout ofa total coverage of3000 (i.e.,it gets n =1000 wrong) is judged as moresuccessful than one that gets p =1000 out ofa total coverage of1001 (i.e.,n =1 wrong),because [p+(N-n)]/Tis [1000 +N]/Tin the ﬁrst case but only[999 +N]/Tin the second.This is counterintuitive:the ﬁrst rule is clearly lesspredictive than the second,because it has 33.0% as opposed to only 0.1% chance ofbeing incorrect.Using the success rate p/tas a measure,as in the original formulation ofthecovering algorithm (Figure 4.8),is not the perfect solution either,because itwould prefer a rule that got a single instance right (p =1) out ofa total cover-age of1 (so n =0) to the far more useful rule that got 1000 right out of1001.Another heuristic that has been used is (p-n)/t,but that suffers from exactlythe same problem because (p-n)/t=2p/t-1 and so the result,when compar-ing one rule with another,is just the same as with the success rate.It seems hardto ﬁnd a simple measure ofthe worth ofa rule that corresponds with intuitionin all cases.Whatever heuristic is used to measure the worth ofa rule,the incrementalreduced-error pruning algorithm is the same.A possible rule learning algorithmbased on this idea is given in Figure 6.3.It generates a decision list,creating rulesfor each class in turn and choosing at each stage the best version ofthe ruleaccording to its worth on the pruning data.The basic covering algorithm forrule generation (Figure 4.8) is used to come up with good rules for each class,choosing conditions to add to the rule using the accuracy measure p/tthat wedescribed earlier.This method has been used to produce rule-induction schemes that canprocess vast amounts ofdata and operate very quickly.It can be accelerated bygenerating rules for the classes in order rather than generating a rule for eachclass at every stage and choosing the best.A suitable ordering is the increasingorder in which they occur in the training set so that the rarest class is processedﬁrst and the most common ones are processed later.Another signiﬁcantspeedup is obtained by stopping the whole process when a rule ofsufﬁcientlylow accuracy is generated,so as not to spend time generating a lot ofrules atthe end with very small coverage.However,very simple terminating conditions(such as stopping when the accuracy for a rule is lower than the default accu-racy for the class it predicts) do not give the best performance,and the onlyconditions that have been found that seem to perform well are rather compli-cated ones based on the MDL principle.204CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2046.2CLASSIFICATION RULES205Using global optimizationIn general,rules generated using incremental reduced-error pruning in thismanner seem to perform quite well,particularly on large datasets.However,ithas been found that a worthwhile performance advantage can be obtained byperforming a global optimization step on the set ofrules induced.The motiva-tion is to increase the accuracy ofthe rule set by revising or replacing individ-ual rules.Experiments show that both the size and the performance ofrule setsare signiﬁcantly improved by postinduction optimization.On the other hand,the process itselfis rather complex.To give an idea ofhow elaborate—and heuristic—industrial-strength rulelearners become,Figure 6.4 shows an algorithm called RIPPER,an acronym forrepeated incremental pruning to produce error reduction.Classes are examined inincreasing size and an initial set ofrules for the class is generated using incre-mental reduced-error pruning.An extra stopping condition is introduced thatdepends on the description length ofthe examples and rule set.The descriptionlength DLis a complex formula that takes into account the number ofbitsneeded to send a set ofexamples with respect to a set ofrules,the number ofbits required to send a rule with kconditions,and the number ofbits neededto send the integer k—times an arbitrary factor of50% to compensate for pos-sible redundancy in the attributes.Having produced a rule set for the class,eachrule is reconsidered and two variants produced,again using reduced-errorpruning—but at this stage,instances covered by other rules for the class areremoved from the pruning set,and success rate on the remaining instances is used as the pruning criterion.Ifone ofthe two variants yields a better Initialize E to the instance setSplit E into Grow and Prune in the ratio 2:1  For each class C for which Grow and Prune both contain an instance    Use the basic covering algorithm to create the best perfect rule for class C    Calculate the worth w(R) for the rule on Prune, and of the rule with the   final condition omitted w(R-)     While w(R-) > w(R), remove the final condition from the rule and repeat the   previous step  From the rules generated, select the one with the largest w(R)  Print the rule  Remove the instances covered by the rule from E ContinueFigure 6.3Algorithm for forming rules by incremental reduced-error pruning.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 205206CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMES(a) Initialize E to the instance setFor each class C, from smallest to largest    BUILD:         Split E into Growing and Pruning sets in the ratio 2:1        Repeat until (a) there are no more uncovered examples of C; or (b) the description length (DL) of ruleset and examples is 64 bits greater than the smallest DL found so far, or (c) the error rate exceeds 50%:           GROW phase: Grow a rule by greedily adding conditions until the rule  is 100% accurate by testing every possible value of each attribute and selecting the condition with greatest information gain G            PRUNE phase: Prune conditions in last-to-first order. Continue as long as the worth W of the rule increases    OPTIMIZE:        GENERATE VARIANTS:         For each rule R for class C,           Split E afresh into Growing and Pruning sets           Remove all instances from the Pruning set that are covered by other rules for C            Use GROW and PRUNE to generate and prune two competing rules from the newly-split data: R1 is a new rule, rebuilt from scratch; R2 is generated by greedily adding antecedents to R.           Prune using the metric A (instead of W) on this reduced data        SELECT REPRESENTATIVE:         Replace R by whichever of R, R1 and R2 has the smallest DL.    MOP UP:        If there are residual uncovered instances of class C, return to the BUILD stage to generate more rules based on these instances.    CLEAN UP:        Calculate DL for the whole ruleset and for the ruleset with each rule in         Remove instances covered by the rules just generatedturn omitted; delete any rule that increases the DLContinueFigure 6.4RIPPER:(a) algorithm for rule learning and (b) meaning ofsymbols.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2066.2CLASSIFICATION RULES207description length,it replaces the rule.Next we reactivate the original buildingphase to mop up any newly uncovered instances ofthe class.A ﬁnal check ismade to ensure that each rule contributes to the reduction ofdescription length,before proceeding to generate rules for the next class.Obtaining rules from partial decision treesThere is an alternative approach to rule induction that avoids global optimiza-tion but nevertheless produces accurate,compact,rule sets.The method com-bines the divide-and-conquer strategy for decision tree learning with theseparate-and-conquer one for rule learning.It adopts the separate-and-conquerstrategy in that it builds a rule,removes the instances it covers,and continuescreating rules recursively for the remaining instances until none are left.However,it differs from the standard approach in the way that each rule iscreated.In essence,to make a single rule,a pruned decision tree is built for thecurrent set ofinstances,the leafwith the largest coverage is made into a rule,and the tree is discarded.The prospect ofrepeatedly building decision trees only to discard most ofthem is not as bizarre as it ﬁrst seems.Using a pruned tree to obtain a ruleinstead ofbuilding it incrementally by adding conjunctions one at a time avoidsa tendency to overprune that is a characteristic problem ofthe basic separate-and-conquer rule learner.Using the separate-and-conquer methodology in con-junction with decision trees adds ﬂexibility and speed.It is indeed wasteful tobuild a full decision tree just to obtain a single rule,but the process can be accel-erated signiﬁcantly without sacriﬁcing the preceding advantages.The key idea is to build a partial decision tree instead ofa fully explored one.A partial decision tree is an ordinary decision tree that contains branches to(b) DL: see text G = p[log(p/t) − log(P/T)]W = p + 1t + 2A =p + n′T; accuracy for this rulep = number of positive examples covered by this rule (true positives)n = number of negative examples covered by this rule (false negatives)t = p + n; total number of examples covered by this rulen′  = N – n; number of negative examples not covered by this rule (true negatives)P = number of positive examples of this classN = number of negative examples of this class T = P + N; total number of examples of this class Figure 6.4(continued)P088407-Ch006.qxd  4/30/05  11:18 AM  Page 207undeﬁned subtrees.To generate such a tree,the construction and pruning oper-ations are integrated in order to ﬁnd a “stable”subtree that can be simpliﬁed nofurther.Once this subtree has been found,tree building ceases and a single ruleis read off.The tree-building algorithm is summarized in Figure 6.5:it splits a set ofinstances recursively into a partial tree.The ﬁrst step chooses a test and dividesthe instances into subsets accordingly.The choice is made using the same infor-mation-gain heuristic that is normally used for building decision trees (Section4.3).Then the subsets are expanded in increasing order oftheir average entropy.The reason for this is that the later subsets will most likely not end up beingexpanded,and a subset with low average entropy is more likely to result in asmall subtree and therefore produce a more general rule.This proceeds recur-sively until a subset is expanded into a leaf,and then continues further by back-tracking.But as soon as an internal node appears that has all its childrenexpanded into leaves,the algorithm checks whether that node is better replacedby a single leaf.This is just the standard subtree replacement operation ofdecision tree pruning (Section 6.1).Ifreplacement is performed the algorithmbacktracks in the standard way,exploring siblings ofthe newly replaced node.However,ifduring backtracking a node is encountered all ofwhose children arenot leaves—and this will happen as soon as a potential subtree replacement isnotperformed—then the remaining subsets are left unexplored and the corre-sponding subtrees are left undeﬁned.Because ofthe recursive structure ofthealgorithm,this event automatically terminates tree generation.Figure 6.6 shows a step-by-step example.During the stages in Figure 6.6(a)through (c),tree building continues recursively in the normal way—except that208CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESExpand-subset (S):  Choose a test T and use it to split the set of examples into subsets  Sort subsets into increasing order of average entropy  while (there is a subset X that has not yet been expanded         AND all subsets expanded so far are leaves)    expand-subset(X)  if (all the subsets expanded are leaves      AND estimated error for subtree ≥ estimated error for node)    undo expansion into subsets and make node a leafFigure 6.5Algorithm for expanding examples into a partial tree.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2086.2CLASSIFICATION RULES209at each point the lowest-entropy sibling is chosen for expansion:node 3 between stages (a) and (b).Gray elliptical nodes are as yet unexpanded;rec-tangular ones are leaves.Between stages (b) and (c),the rectangular node willhave lower entropy than its sibling,node 5,but cannot be expanded furtherbecause it is a leaf.Backtracking occurs and node 5 is chosen for expansion.Once stage (c) is reached,there is a node—node 5—that has all ofits childrenexpanded into leaves,and this triggers pruning.Subtree replacement for node5 is considered and accepted,leading to stage (d).Then node 3 is considered forsubtree replacement,and this operation is again accepted.Backtracking con-tinues,and node 4,having lower entropy than node 2,is expanded into twoleaves.Now subtree replacement is considered for node 4:suppose that node 4is not replaced.At this point,the process terminates with the three-leafpartialtree ofstage (e).Ifthe data is noise-free and contains enough instances to prevent the algo-rithm from doing any pruning,just one path ofthe full decision tree has to beexplored.This achieves the greatest possible performance gain over the naïve2314(a)52314(b)52314(c)2314(d)Figure 6.6Example ofbuilding a partial tree.214(e)P088407-Ch006.qxd  4/30/05  11:18 AM  Page 209method that builds a full decision tree each time.The gain decreases as morepruning takes place.For datasets with numeric attributes,the asymptotic timecomplexity ofthe algorithm is the same as building the full decision tree,because in this case the complexity is dominated by the time required to sortthe attribute values in the ﬁrst place.Once a partial tree has been built,a single rule is extracted from it.Each leafcorresponds to a possible rule,and we seek the “best”leafofthose subtrees (typically a small minority) that have been expanded into leaves.Experimentsshow that it is best to aim at the most general rule by choosing the leafthatcovers the greatest number ofinstances.When a dataset contains missing values,they can be dealt with exactly as theyare when building decision trees.Ifan instance cannot be assigned to any givenbranch because ofa missing attribute value,it is assigned to each ofthe brancheswith a weight proportional to the number oftraining instances going down thatbranch,normalized by the total number oftraining instances with known valuesat the node.During testing,the same procedure is applied separately to eachrule,thus associating a weight with the application ofeach rule to the testinstance.That weight is deducted from the instance’s total weight before it ispassed to the next rule in the list.Once the weight has reduced to zero,the pre-dicted class probabilities are combined into a ﬁnal classiﬁcation according tothe weights.This yields a simple but surprisingly effective method for learning decisionlists for noisy data.Its main advantage over other comprehensive rule-generation schemes is simplicity,because other methods require a complexglobal optimization stage to achieve the same level ofperformance.Rules with exceptionsIn Section 3.5 we learned that a natural extension ofrules is to allow them tohave exceptions,and exceptions to the exceptions,and so on—indeed the wholerule set can be considered as exceptions to a default classiﬁcation rule that isused when no other rules apply.The method ofgenerating a “good”rule,usingone ofthe measures described in the previous section,provides exactly themechanism needed to generate rules with exceptions.First,a default class is selected for the top-level rule:it is natural to use theclass that occurs most frequently in the training data.Then,a rule is found per-taining to any class other than the default one.Ofall such rules it is natural toseek the one with the most discriminatory power,for example,the one with thebest evaluation on a test set.Suppose this rule has the formif <condition> then class =<new class>210CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2106.2CLASSIFICATION RULES211It is used to split the training data into two subsets:one containing all instancesfor which the rule’s condition is trueand the other containing those for whichit is false.Ifeither subset contains instances ofmore than one class,the algo-rithm is invoked recursively on that subset.For the subset for which the condi-tion is true,the “default class”is the new class as speciﬁed by the rule;for the subset for which the condition is false,the default class remains as it wasbefore.Let’s examine how this algorithm would work for the rules with exceptionsgiven in Section 3.5 for the Iris data ofTable 1.4.We will represent the rules inthe graphical form shown in Figure 6.7,which is in fact equivalent to the textualrules we gave in Figure 3.5.The default ofIris setosais the entry node at the topleft.Horizontal,dotted paths show exceptions,so the next box,which containsa rule that concludes Iris versicolor,is an exception to the default.Below this isan alternative,a second exception—alternatives are shown by vertical,solidlines—leading to the conclusion Iris virginica.Following the upper path alonghorizontally leads to an exception to the Iris versicolorrule that overrides itwhenever the condition in the top right box holds,with the conclusion Iris vir-ginica.Below this is an alternative,leading (as it happens) to the same conclu-sion.Returning to the box at bottom center,this has its own exception,the lowerright box,which gives the conclusion Iris versicolor.The numbers at the lowerright ofeach box give the “coverage”ofthe rule,expressed as the number of--> Iris setosa         50/150petal length ≥ 2.45petal width < 1.75petal length < 5.35petal length ≥ 4.95petal width < 1.55sepal length < 4.95sepal width ≥ 2.45petal length < 4.85sepal length < 5.95vpetal length ≥ 3.35--> Iris versicolor49/52--> Iris virginica2/2--> Iris virginica1/1--> Iris virginica47/48--> Iris versicolor1/1Exceptions arerepresented asdotted paths, alternatives assolid ones.Figure 6.7Rules with exceptions for the iris data.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 211examples that satisfy it divided by the number that satisfy its condition but notits conclusion.For example,the condition in the top center box applies to 52 ofthe examples,and 49 ofthem are Iris versicolor.The strength ofthis represen-tation is that you can get a very good feeling for the effect ofthe rules from theboxes toward the left-hand side;the boxes at the right cover just a few excep-tional cases.To create these rules,the default is ﬁrst set to Iris setosaby taking the mostfrequently occurring class in the dataset.This is an arbitrary choice because forthis dataset all classes occur exactly 50 times;as shown in Figure 6.7 this default“rule”is correct in 50 of150 cases.Then the best rule that predicts another classis sought.In this case it isif petal length ≥2.45 and petal length <5.355 and petal width <1.75 then Iris versicolorThis rule covers 52 instances,ofwhich 49 are Iris versicolor.It divides the datasetinto two subsets:the 52 instances that do satisfy the condition ofthe rule andthe remaining 98 that do not.We work on the former subset ﬁrst.The default class for these instances isIris versicolor:there are only three exceptions,all ofwhich happen to be Iris virginica.The best rule for this subset that does not predict Iris versicoloris identiﬁed next:if petal length ≥4.95 and petal width <1.55 then Iris virginicaIt covers two ofthe three Iris virginicasand nothing else.Again it divides thesubset into two:those instances that satisfy its condition and those that do not.Fortunately,in this case,all instances that satisfy the condition do indeed have the class Iris virginica,so there is no need for a further exception.However,the remaining instances still include the third Iris virginica,along with49 Iris versicolors,which are the default at this point.Again the best rule issought:if sepal length <4.95 and sepal width ≥2.45 then Iris virginicaThis rule covers the remaining Iris virginicaand nothing else,so it also has noexceptions.Furthermore,all remaining instances in the subset that do not satisfyits condition have the class Iris versicolor,which is the default,so no more needsto be done.Return now to the second subset created by the initial rule,the instances thatdo not satisfy the conditionpetal length ≥2.45 and petal length <5.355 and petal width <1.75212CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2126.2CLASSIFICATION RULES213Ofthe rules for these instances that do not predict the default class Iris setosa,the best isif petal length ≥3.35 then Iris virginicaIt covers all 47 Iris virginicasthat are in the example set (3 were removed by theﬁrst rule,as explained previously).It also covers 1 Iris versicolor.This needs tobe taken care ofas an exception,by the ﬁnal rule:if petal length <4.85 and sepal length <5.95 then Iris versicolorFortunately,the set ofinstances that do notsatisfy its condition are all thedefault,Iris setosa.Thus the procedure is ﬁnished.The rules that are produced have the property that most ofthe examples arecovered by the high-level rules and the lower-level ones really do representexceptions.For example,the last exception clause in the preceding rules and thedeeply nested elseclause both cover a solitary example,and removing themwould have little effect.Even the remaining nested exception rule covers onlytwo examples.Thus one can get an excellent feeling for what the rules do byignoring all the deeper structure and looking only at the ﬁrst level or two.Thatis the attraction ofrules with exceptions.DiscussionAll algorithms for producing classiﬁcation rules that we have described use thebasic covering or separate-and-conquer approach.For the simple,noise-freecase this produces PRISM (Cendrowska 1987),an algorithm that is simple andeasy to understand.When applied to two-class problems with the closed worldassumption,it is only necessary to produce rules for one class:then the rulesare in disjunctive normal form and can be executed on test instances withoutany ambiguity arising.When applied to multiclass problems,a separate rule setis produced for each class:thus a test instance may be assigned to more thanone class,or to no class,and further heuristics are necessary ifa unique pre-diction is sought.To reduce overﬁtting in noisy situations,it is necessary to produce rules thatare not “perfect”even on the training set.To do this it is necessary to have ameasure for the “goodness,”or worth,ofa rule.With such a measure it is thenpossible to abandon the class-by-class approach ofthe basic covering algorithmand start by generating the very best rule,regardless ofwhich class it predicts,and then remove all examples covered by this rule and continue the process.This yields a method for producing a decision list rather than a set ofinde-pendent classiﬁcation rules,and decision lists have the important advantage thatthey do not generate ambiguities when interpreted.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 213The idea ofincremental reduced-error pruning is due to Fürnkranz and Widmer (1994) and forms the basis for fast and effective rule induction.The RIPPER rule learner is due to Cohen (1995),although the publisheddescription appears to differ from the implementation in precisely how the description length (DL) affects the stopping condition.What we have pre-sented here is the basic idea ofthe algorithm;there are many more details inthe implementation.The whole question ofmeasuring the value ofa rule has not yet been satis-factorily resolved.Many different measures have been proposed,some blatantlyheuristic and others based on information-theoretical or probabilistic grounds.However,there seems to be no consensus on what the best measure to use is.An extensive theoretical study ofvarious criteria has been performed byFürnkranz and Flach (2005).The rule-learning method based on partial decision trees was developed byFrank and Witten (1998).It produces rule sets that are as accurate as those gen-erated by C4.5 and more accurate than other fast rule-induction methods.However,its main advantage over other schemes is not performance but sim-plicity:by combining the top-down decision tree induction method with sepa-rate-and-conquer rule learning,it produces good rule sets without any need forglobal optimization.The procedure for generating rules with exceptions was developed as anoption in the Induct system by Gaines and Compton (1995),who called themripple-downrules.In an experiment with a large medical dataset (22,000instances,32 attributes,and 60 classes),they found that people can understandlarge systems ofrules with exceptions more readily than equivalent systems ofregular rules because that is the way that they think about the complex medicaldiagnoses that are involved.Richards and Compton (1998) describe their roleas an alternative to classic knowledge engineering.6.3Extending linear modelsSection 4.6 described how simple linear models can be used for classiﬁcation insituations where all attributes are numeric.Their biggest disadvantage is thatthey can only represent linear boundaries between classes,which makes themtoo simple for many practical applications.Support vector machines use linearmodels to implement nonlinear class boundaries.(Although it is a widely used term,support vector machinesis something ofa misnomer:these are algorithms,not machines.) How can this be possible? The trick is easy:trans-form the input using a nonlinear mapping;in other words,transform theinstance space into a new space.With a nonlinear mapping,a straight line inthe new space doesn’t look straight in the original instance space.A linear model214CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2146.3EXTENDING LINEAR MODELS215constructed in the new space can represent a nonlinear decision boundary inthe original space.Imagine applying this idea directly to the ordinary linear models in Section4.6.For example,the original set ofattributes could be replaced by one givingall products ofnfactors that can be constructed from these attributes.Anexample for two attributes,including all products with three factors,isHere,xis the outcome,a1 and a2are the two attribute values,and there are fourweights wito be learned.As described in Section 4.6,the result can be used forclassiﬁcation by training one linear system for each class and assigning anunknown instance to the class that gives the greatest output x—the standardtechnique ofmultiresponse linear regression.Then,a1 and a2will be the attrib-ute values for the test instance.To generate a linear model in the space spannedby these products,each training instance is mapped into the new space by computing all possible three-factor products ofits two attribute values.Thelearning algorithm is then applied to the transformed instances.To classify aninstance,it is processed by the same transformation prior to classiﬁcation.Thereis nothing to stop us from adding in more synthetic attributes.For example,ifa constant term were included,the original attributes and all two-factor prod-ucts ofthem would yield a total ofeight weights to be learned.(Alternatively,adding an additional attribute whose value was always a constant would havethe same effect.) Indeed,polynomials ofsufﬁciently high degree can approxi-mate arbitrary decision boundaries to any required accuracy.It seems too good to be true—and it is.As you will probably have guessed,problems arise with this procedure because ofthe large number ofcoefﬁcientsintroduced by the transformation in any realistic setting.The ﬁrst snag is com-putational complexity.With 10 attributes in the original dataset,suppose wewant to include all products with ﬁve factors:then the learning algorithm willhave to determine more than 2000 coefﬁcients.Ifits run time is cubic in thenumber ofattributes,as it is for linear regression,training will be infeasible.That is a problem ofpracticality.The second problem is one ofprinciple:over-ﬁtting.Ifthe number ofcoefﬁcients is large relative to the number oftraininginstances,the resulting model will be “too nonlinear”—it will overﬁt the train-ing data.There are just too many parameters in the model.The maximum margin hyperplaneSupport vector machines solve both problems.They are based on an algorithmthat ﬁnds a special kind oflinear model:the maximum margin hyperplane.Wealready know what a hyperplane is—it’s just another word for a linear model.To visualize a maximum margin hyperplane,imagine a two-class dataset whosexwawaawaawa=+++11321223122423.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 215classes are linearly separable;that is,there is a hyperplane in instance space thatclassiﬁes all training instances correctly.The maximum margin hyperplane isthe one that gives the greatest separation between the classes—it comes no closerto either than it has to.An example is shown in Figure 6.8,in which the classesare represented by open and ﬁlled circles,respectively.Technically,the convexhullofa set ofpoints is the tightest enclosing convex polygon:its outlineemerges when you connect every point ofthe set to every other point.Becausewe have supposed that the two classes are linearly separable,their convex hullscannot overlap.Among all hyperplanes that separate the classes,the maximummargin hyperplane is the one that is as far away as possible from both convexhulls—it is the perpendicular bisector ofthe shortest line connecting the hulls,which is shown dashed in the ﬁgure.The instances that are closest to the maximum margin hyperplane—the oneswith minimum distance to it—are called support vectors.There is always at leastone support vector for each class,and often there are more.The important thingis that the set ofsupport vectors uniquely deﬁnes the maximum margin hyper-plane for the learning problem.Given the support vectors for the two classes,we can easily construct the maximum margin hyperplane.All other traininginstances are irrelevant—they can be deleted without changing the position andorientation ofthe hyperplane.A hyperplane separating the two classes might be writtenxwwawa=++01122216CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESmaximum margin hyperplanesupport vectorsFigure 6.8A maximum margin hyperplane.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2166.3EXTENDING LINEAR MODELS217in the two-attribute case,where a1 and a2are the attribute values,and there arethree weights wito be learned.However,the equation deﬁning the maximummargin hyperplane can be written in another form,in terms ofthe supportvectors.Write the class value yofa training instance as either 1 (for yes,it is in this class) or -1 (for no,it is not).Then the maximum margin hyperplane isHere,yiis the class value oftraining instance a(i);while band aiare numericparameters that have to be determined by the learning algorithm.Note that a(i)and aare vectors.The vector arepresents a test instance—just as the vector [a1,a2] represented a test instance in the earlier formulation.The vectors a(i)are the support vectors,those circled in Figure 6.8;they are selected membersofthe training set.The term a(i)◊arepresents the dot product ofthe test instancewith one ofthe support vectors.Ifyou are not familiar with dot product nota-tion,you should still be able to understand the gist ofwhat follows:just thinkofa(i)as the whole set ofattribute values for the ith support vector.Finally,band aiare parameters that determine the hyperplane,just as the weights w0,w1,and w2are parameters that determine the hyperplane in the earlier formulation.It turns out that ﬁnding the support vectors for the instance sets and deter-mining the parameters band aibelongs to a standard class ofoptimizationproblems known as constrained quadratic optimization.There are off-the-shelfsoftware packages for solving these problems (see Fletcher 1987 for a com-prehensive and practical account ofsolution methods).However,the com-putational complexity can be reduced,and learning can be accelerated,ifspecial-purpose algorithms for training support vector machines are applied—but the details ofthese algorithms lie beyond the scope ofthis book (Platt 1998).Nonlinear class boundariesWe motivated the introduction ofsupport vector machines by claiming thatthey can be used to model nonlinear class boundaries.However,so far we haveonly described the linear case.Consider what happens when an attribute trans-formation,as described previously,is applied to the training data before deter-mining the maximum margin hyperplane.Recall that there are two problemswith the straightforward application ofsuch transformations to linear models:infeasible computational complexity on the one hand and overﬁtting on theother.With support vectors,overﬁtting is unlikely to occur.The reason is that it isinevitably associated with instability:changing one or two instance vectors willmake sweeping changes to large sections ofthe decision boundary.But thexbyiiii=+()◊Âaaa is support vector.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 217maximum margin hyperplane is relatively stable:it only moves iftraininginstances are added or deleted that are support vectors—and this is true evenin the high-dimensional space spanned by the nonlinear transformation.Over-ﬁtting is caused by too much ﬂexibility in the decision boundary.The supportvectors are global representatives ofthe whole set oftraining points,and thereare usually few ofthem,which gives little ﬂexibility.Thus overﬁtting is unlikelyto occur.What about computational complexity? This is still a problem.Suppose thatthe transformed space is a high-dimensional one so that the transformedsupport vectors and test instance have many components.According to the pre-ceding equation,every time an instance is classiﬁed its dot product with allsupport vectors must be calculated.In the high-dimensional space produced bythe nonlinear mapping this is rather expensive.Obtaining the dot productinvolves one multiplication and one addition for each attribute,and the numberofattributes in the new space can be huge.This problem occurs not only duringclassiﬁcation but also during training,because the optimization algorithms haveto calculate the same dot products very frequently.Fortunately,it turns out that it is possible to calculate the dot product beforethe nonlinear mapping is performed,on the original attribute set.A high-dimensional version ofthe preceding equation is simplywhere nis chosen as the number offactors in the transformation (three in theexample we used earlier).Ifyou expand the term (a(i)◊a)n,you will ﬁnd that itcontains all the high-dimensional terms that would have been involved ifthetest and training vectors were ﬁrst transformed by including all products ofnfactors and the dot product was taken ofthe result.(Ifyou actually do the cal-culation,you will notice that some constant factors—binomial coefﬁcients—are introduced.However,these do not matter:it is the dimensionality ofthespace that concerns us;the constants merely scale the axes.) Because ofthismathematical equivalence,the dot products can be computed in the originallow-dimensional space,and the problem becomes feasible.In implementationterms,you take a software package for constrained quadratic optimization andevery time a(i)◊ais evaluated you evaluate (a(i)◊a)ninstead.It’s as simple as that,because in both the optimization and the classiﬁcation algorithms these vectorsare only ever used in this dot product form.The training vectors,including thesupport vectors,and the test instance all remain in the original low-dimensionalspace throughout the calculations.The function (x◊y)n,which computes the dot product oftwo vectors xand yand raises the result to the power n,is called a polynomial kernel.A good xbyiiin=+()◊()Âaaa,218CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2186.3EXTENDING LINEAR MODELS219way ofchoosing the value ofnis to start with 1 (a linear model) and incre-ment it until the estimated error ceases to improve.Usually,quite small valuessufﬁce.Other kernel functions can be used instead to implement different nonlinearmappings.Two that are often suggested are the radial basis function (RBF) kerneland the sigmoid kernel.Which one produces the best results depends on theapplication,although the differences are rarely large in practice.It is interestingto note that a support vector machine with the RBF kernel is simply a type ofneural network called an RBF network(which we describe later),and one withthe sigmoid kernel implements another type ofneural network,a multilayerperceptron with one hidden layer (also described later).Throughout this section,we have assumed that the training data is linearlyseparable—either in the instance space or in the new space spanned by the non-linear mapping.It turns out that support vector machines can be generalized tothe case where the training data is not separable.This is accomplished by placingan upper bound on the preceding coefﬁcients ai.Unfortunately,this parametermust be chosen by the user,and the best setting can only be determined byexperimentation.Also,in all but trivial cases,it is not possible to determine apriori whether the data is linearly separable or not.Finally,we should mention that compared with other methods such as deci-sion tree learners,even the fastest training algorithms for support vectormachines are slow when applied in the nonlinear setting.On the other hand,they often produce very accurate classiﬁers because subtle and complex deci-sion boundaries can be obtained.Support vector regressionThe concept ofa maximum margin hyperplane only applies to classiﬁcation.However,support vector machine algorithms have been developed for numericprediction that share many ofthe properties encountered in the classiﬁcationcase:they produce a model that can usually be expressed in terms ofa fewsupport vectors and can be applied to nonlinear problems using kernel func-tions.As with regular support vector machines,we will describe the conceptsinvolved but do not attempt to describe the algorithms that actually perform thework.As with linear regression,covered in Section 4.6,the basic idea is to ﬁnd afunction that approximates the training points well by minimizing the predic-tion error.The crucial difference is that all deviations up to a user-speciﬁedparameter eare simply discarded.Also,when minimizing the error,the risk ofoverﬁtting is reduced by simultaneously trying to maximize the ﬂatness ofthefunction.Another difference is that what is minimized is normally the predic-P088407-Ch006.qxd  4/30/05  11:18 AM  Page 219tions’absolute error instead ofthe squared error used in linear regression.(There are,however,versions ofthe algorithm that use the squared errorinstead.)A user-speciﬁed parameter edeﬁnes a tube around the regression functionin which errors are ignored:for linear support vector regression,the tube is acylinder.Ifall training points can ﬁt within a tube ofwidth 2e,the algorithmoutputs the function in the middle ofthe ﬂattest tube that encloses them.Inthis case the total perceived error is zero.Figure 6.9(a) shows a regressionproblem with one attribute,a numeric class,and eight instances.In this case ewas set to 1,so the width ofthe tube around the regression function (indicatedby dotted lines) is 2.Figure 6.9(b) shows the outcome ofthe learning processwhen eis set to 2.As you can see,the wider tube makes it possible to learn aﬂatter function.The value ofecontrols how closely the function will ﬁt the training data.Toolarge a value will produce a meaningless predictor—in the extreme case,when2eexceeds the range ofclass values in the training data,the regression line ishorizontal and the algorithm just predicts the mean class value.On the otherhand,for small values ofethere may be no tube that encloses all the data.Inthat case some training points will have nonzero error,and there will be a trade-offbetween the prediction error and the tube’s ﬂatness.In Figure 6.9(c),ewasset to 0.5 and there is no tube ofwidth 1 that encloses all the data.For the linear case,the support vector regression function can be writtenAs with classiﬁcation,the dot product can be replaced by a kernel function fornonlinear problems.The support vectors are all those points that do not fallstrictly within the tube—that is,the points outside the tube and on its border.As with classiﬁcation,all other points have coefﬁcient 0 and can be deleted fromthe training data without changing the outcome ofthe learning process.In con-trast to the classiﬁcation case,the aimay be negative.We have mentioned that as well as minimizing the error,the algorithm simul-taneously tries to maximize the ﬂatness ofthe regression function.In Figure6.9(a) and (b),where there is a tube that encloses all the training data,the algo-rithm simply outputs the ﬂattest tube that does so.However,in Figure 6.9(c)there is no tube with error 0,and a tradeoffis struck between the predictionerror and the tube’s ﬂatness.This tradeoffis controlled by enforcing an upperlimit Con the absolute value ofthe coefﬁcients ai.The upper limit restricts theinﬂuence ofthe support vectors on the shape ofthe regression function and isa parameter that the user must specify in addition to e.The larger Cis,the moreclosely the function can ﬁt the data.In the degenerate case e=0 the algorithmsimply performs least-absolute-error regression under the coefﬁcient size con-xbiii=+()◊Âaaa is support vector.220CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2206.3EXTENDING LINEAR MODELS22102468 1002468 10classattribute(a)02468 1002468 10classattribute(b) 0 2 4 6 8 10 0 2 4 6 8 10classattribute(c)Figure 6.9Support vector regression:(a) e=1,(b) e=2,and (c) e=0.5.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 221straint,and all training instances become support vectors.Conversely,ifeislarge enough that the tube can enclose all the data,the error becomes zero,thereis no tradeoffto make,and the algorithm outputs the ﬂattest tube that enclosesthe data irrespective ofthe value ofC.The kernel perceptronIn Section 4.6 we introduced the perceptron algorithm for learning a linear clas-siﬁer.It turns out that the kernel trick can also be used to upgrade this algo-rithm to learn nonlinear decision boundaries.To see this,we ﬁrst revisit thelinear case.The perceptron algorithm repeatedly iterates through the trainingdata instance by instance and updates the weight vector every time one oftheseinstances is misclassiﬁed based on the weights learned so far.The weight vectoris updated simply by adding or subtracting the instance’s attribute values to orfrom it.This means that the ﬁnal weight vector is just the sum ofthe instancesthat have been misclassiﬁed.The perceptron makes its predictions based onwhetheris greater or less than zero—where wiis the weight for the ith attribute and aithe corresponding attribute value ofthe instance that we wish to classify.Instead,we could useHere,a¢(j) is the jth misclassiﬁed training instance,a¢(j)iis its ith attribute value,and y(j) is its class value (either +1 or -1).To implement this we no longer keeptrack ofan explicit weight vector:we simply store the instances misclassiﬁed sofar and use the preceding expression to make a prediction.It looks like we’ve gained nothing—in fact,the algorithm is much slowerbecause it iterates through all misclassiﬁed training instances every time a pre-diction is made.However,closer inspection ofthis formula reveals that it canbe expressed in terms ofdot products between instances.First,swap the sum-mation signs to yieldThe second sum is just a dot product between two instances and can be writtenasyjjj()¢()◊Âaa.yjajaiiij()¢()ÂÂ.yjajaiiji()¢()ÂÂ.waiiiÂ222CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2226.3EXTENDING LINEAR MODELS223This rings a bell! A similar expression for support vector machines enabled theuse ofkernels.Indeed,we can apply exactly the same trick here and use a kernelfunction instead ofthe dot product.Writing this function as K(...) givesIn this way the perceptron algorithm can learn a nonlinear classiﬁer simply bykeeping track ofthe instances that have been misclassiﬁed during the trainingprocess and using this expression to form each prediction.Ifa separating hyperplane exists in the high-dimensional space implicitlycreated by the kernel function,this algorithm will learn one.However,it won’tlearn the maximum margin hyperplane found by a support vector machine clas-siﬁer.This means that classiﬁcation performance is usually worse.On the plusside,the algorithm is easy to implement and supports incremental learning.This classiﬁer is called the kernel perceptron.It turns out that all sorts ofalgo-rithms for learning linear models can be upgraded by applying the kernel trickin a similar fashion.For example,logistic regression can be turned into kernellogistic regression.The same applies to regression problems:linear regression canalso be upgraded using kernels.A drawback ofthese advanced methods forlinear and logistic regression (ifthey are done in a straightforward manner) isthat the solution is not “sparse”:every training instance contributes to the solu-tion vector.In support vector machines and the kernel perceptron,only someofthe training instances affect the solution,and this can make a big differenceto computational efﬁciency.The solution vector found by the perceptron algorithm depends greatly onthe order in which the instances are encountered.One way to make the algo-rithm more stable is to use all the weight vectors encountered during learning,not just the ﬁnal one,letting them vote on a prediction.Each weight vector con-tributes a certain number ofvotes.Intuitively,the “correctness”ofa weightvector can be measured roughly as the number ofsuccessive trials after its incep-tion in which it correctly classiﬁed subsequent instances and thus didn’t have tobe changed.This measure can be used as the number ofvotes given to the weightvector,giving an algorithm known as the voted perceptronthat performs almostas well as a support vector machine.(Note that,as previously mentioned,thevarious weight vectors in the voted perceptron don’t need to be stored explic-itly,and the kernel trick can be applied here too.)Multilayer perceptronsUsing a kernel is not the only way to create a nonlinear classiﬁer based on theperceptron.In fact,kernel functions are a recent development in machine yjKjj()¢()()Âaa,.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 223learning.Previously,neural network proponents used a different approach for nonlinear classiﬁcation:they connected many simple perceptron-like models in a hierarchical structure.This can represent nonlinear decision boundaries.Section 4.6 explained that a perceptron represents a hyperplane in instancespace.We mentioned there that it is sometimes described as an artiﬁcial“neuron.”Ofcourse,human and animal brains successfully undertake verycomplex classiﬁcation tasks—for example,image recognition.The functional-ity ofeach individual neuron in a brain is certainly not sufﬁcient to performthese feats.How can they be solved by brain-like structures? The answer lies inthe fact that the neurons in the brain are massively interconnected,allowing aproblem to be decomposed into subproblems that can be solved at the neuronlevel.This observation inspired the development ofnetworks ofartiﬁcialneurons—neural nets.Consider the simple datasets in Figure 6.10.Figure 6.10(a) shows a two-dimensional instance space with four instances that have classes 0 and 1,repre-sented by white and black dots,respectively.No matter how you draw a straightline through this space,you will not be able to ﬁnd one that separates all theblack points from all the white ones.In other words,the problem is not linearlyseparable,and the simple perceptron algorithm will fail to generate a separat-ing hyperplane (in this two-dimensional instance space a hyperplane is just astraight line).The situation is different in Figure 6.10(b) and Figure 6.10(c):both these problems are linearly separable.The same holds for Figure 6.10(d),which shows two points in a one-dimensional instance space (in thecase ofone dimension the separating hyperplane degenerates to a separatingpoint).Ifyou are familiar with propositional logic,you may have noticed that thefour situations in Figure 6.10 correspond to four types oflogical connectives.Figure 6.10(a) represents a logical XOR,where the class is 1 ifand only ifexactlyone ofthe attributes has value 1.Figure 6.10(b) represents logical AND,wherethe class is 1 ifand only ifboth attributes have value 1.Figure 6.10(c) repre-sents OR,where the class is 0 only ifboth attributes have value 0.Figure 6.10(d)represents NOT,where the class is 0 ifand only ifthe attribute has value 1.Because the last three are linearly separable,a perceptron can represent AND,OR,and NOT.Indeed,perceptrons for the corresponding datasets are shown inFigure 6.10(f) through (h) respectively.However,a simple perceptron cannotrepresent XOR,because that is not linearly separable.To build a classiﬁer forthis type ofproblem a single perceptron is not sufﬁcient:we need several ofthem.Figure 6.10(e) shows a network with three perceptrons,or units,labeled A,B,and C.The ﬁrst two are connected to what is sometimes called the input layerofthe network,representing the attributes in the data.As in a simple percep-224CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 224CAB1("bias")1("bias")attributea1attributea2–1.51111–1–1–0.51.5(e)1–1.51attributea1attributea21(“bias”)(f)1("bias")attributea1attributea211–0.5(g)–1–0.5attributeai1(“bias”)(h)1001(a)1001(b)1001(c)10(d)Figure 6.10Example datasets and corresponding perceptrons.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 225tron,the input layer has an additional constant input called the bias.However,the third unit does not have any connections to the input layer.Its input con-sists ofthe output ofunits A and B (either 0 or 1) and another constant biasunit.These three units make up the hidden layerofthe multilayer perceptron.They are called “hidden”because the units have no direct connection to the envi-ronment.This layer is what enables the system to represent XOR.You can verifythis by trying all four possible combinations ofinput signals.For example,ifattribute a1 has value 1 and a2has value 1,then unit A will output 1 (because1 ¥1 +1 ¥1 -0.5 ¥1 >0),unit B will output 0 (because -1 ¥1 +-1 ¥1 +1.5¥1 <0),and unit C will output 0 (because 1 ¥1 +1 ¥0 +-1.5 ¥1 <0).This is the correct answer.Closer inspection ofthe behavior ofthe three unitsreveals that the ﬁrst one represents OR,the second represents NAND (NOTcombined with AND),and the third represents AND.Together they representthe expression (a1OR a2) AND (a1NAND a3),which is precisely the deﬁnitionofXOR.As this example illustrates,any expression from propositional calculus can beconverted into a multilayer perceptron,because the three connectives AND,OR,and NOT are sufﬁcient for this and we have seen how each can be representedusing a perceptron.Individual units can be connected together to form arbi-trarily complex expressions.Hence,a multilayer perceptron has the sameexpressive power as,say,a decision tree.In fact,it turns out that a two-layer per-ceptron (not counting the input layer) is sufﬁcient.In this case,each unit in thehidden layer corresponds to a variant ofAND—a variant because we assumethat it may negate some ofthe inputs before forming the conjunction—joinedby an OR that is represented by a single unit in the output layer.In other words,each node in the hidden layer has the same role as a leafin a decision tree or asingle rule in a set ofdecision rules.The big question is how to learn a multilayer perceptron.There are twoaspects to the problem:learning the structure ofthe network and learning theconnection weights.It turns out that there is a relatively simple algorithm fordetermining the weights given a ﬁxed network structure.This algorithm is calledbackpropagationand is described in the next section.However,although thereare many algorithms that attempt to identify network structure,this aspect ofthe problem is commonly solved through experimentation—perhaps combinedwith a healthy dose ofexpert knowledge.Sometimes the network can be separated into distinct modules that represent identiﬁable subtasks (e.g.,recog-nizing different components ofan object in an image recognition problem),which opens up a way ofincorporating domain knowledge into the learningprocess.Often a single hidden layer is all that is necessary,and an appropriatenumber ofunits for that layer is determined by maximizing the estimated accuracy.226CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2266.3EXTENDING LINEAR MODELS227BackpropagationSuppose that we have some data and seek a multilayer perceptron that is anaccurate predictor for the underlying classiﬁcation problem.Given a ﬁxednetwork structure,we must determine appropriate weights for the connectionsin the network.In the absence ofhidden layers,the perceptron learning rulefrom Section 4.6 can be used to ﬁnd suitable values.But suppose there arehidden units.We know what the output unit should predict,and could adjustthe weights ofthe connections leading to that unit based on the perceptron rule.But the correct outputs for the hidden units are unknown,so the rule cannotbe applied there.It turns out that,roughly speaking,the solution is to modify the weights ofthe connections leading to the hidden units based on the strength ofeach unit’scontribution to the ﬁnal prediction.There is a standard mathematical opti-mization algorithm,called gradient descent,which achieves exactly that.Unfor-tunately,it requires taking derivatives,and the step function that the simpleperceptron uses to convert the weighted sum ofthe inputs into a 0/1 predictionis not differentiable.We need to see whether the step function can be replacedwith something else.Figure 6.11(a) shows the step function:ifthe input is smaller than zero,itoutputs zero;otherwise,it outputs one.We want a function that is similar inshape but differentiable.A commonly used replacement is shown in Figure6.11(b).In neural networks terminology it is called the sigmoidfunction,and itis deﬁned byWe encountered it in Section 4.6 when we described the logit transform usedin logistic regression.In fact,learning a multilayer perceptron is closely relatedto logistic regression.To apply the gradient descent procedure,the error function—the thing thatis to be minimized by adjusting the weights—must also be differentiable.Thenumber ofmisclassiﬁcations—measured by the discrete 0–1 loss mentioned inSection 5.6—does not fulﬁll this criterion.Instead,multilayer perceptrons areusually trained by minimizing the squared error ofthe network’s output,essentially treating it as an estimate ofthe class probability.(Other loss func-tions are also applicable.For example,ifthe likelihood is used instead ofthe squared error,learning a sigmoid-based perceptron is identical to logisticregression.)We work with the squared-error loss function because it is most widely used.For a single training instance,it isfxex()=+-11.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 227228CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESwhere f(x) is the network’s prediction obtained from the output unit and yisthe instance’s class label (in this case,it is assumed to be either 0 or 1).The factor1/2 is included just for convenience,and will drop out when we start takingderivatives.Eyfx=-()()122,00.20.40.60.81-10-50510(a)00.20.40.60.81-10-50510(b)Figure 6.11Step versus sigmoid:(a) step function and (b) sigmoid function.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2286.3EXTENDING LINEAR MODELS229Gradient descent exploits information given by the derivative ofthe functionthat is to be minimized—in this case,the error function.As an example,con-sider a hypothetical error function that happens to be identical to x2+1,shownin Figure 6.12.The X-axis represents a hypothetical parameter that is to be opti-mized.The derivative ofx2+1 is simply 2x.The crucial observation is that,based on the derivative,we can ﬁgure out the slope ofthe function at any par-ticular point.Ifthe derivative is negative the function slopes downward to theright;ifit is positive,it slopes downward to the left;and the size ofthe deriva-tive determines how steep the decline is.Gradient descent is an iterative optimization procedure that uses this information to adjust a function’s parameters.It takes the value ofthe derivative,multiplies it by a small constantcalled the learning rate,and subtracts the result from the current parametervalue.This is repeated for the new parameter value,and so on,until a minimumis reached.Returning to the example,assume that the learning rate is set to 0.1 and thecurrent parameter value xis 4.The derivative is double this—8 at this point.Multiplying by the learning rate yields 0.8,and subtracting this from 4 gives 3.2,which becomes the new parameter value.Repeating the process for 3.2,we get2.56,then 2.048,and so on.The little crosses in Figure 6.12 show the valuesencountered in this process.The process stops once the change in parametervalue becomes too small.In the example this happens when the valueapproaches 0,the value corresponding to the location on the X-axis where theminimum ofthe hypothetical error function is located.05101520-4-2024Figure 6.12Gradient descent using the error function x2+1.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 229The learning rate determines the step size and hence how quickly the searchconverges.Ifit is too large and the error function has several minima,the searchmay overshoot and miss a minimum entirely,or it may oscillate wildly.Ifit istoo small,progress toward the minimum may be slow.Note that gradientdescent can only ﬁnd a localminimum.Ifthe function has several minima—and error functions for multilayer perceptrons usually have many—it may notﬁnd the best one.This is a signiﬁcant drawback ofstandard multilayer percep-trons compared with,for example,support vector machines.To use gradient descent to ﬁnd the weights ofa multilayer perceptron,thederivative ofthe squared error must be determined with respect to each param-eter—that is,each weight in the network.Let’s start with a simple perceptronwithout a hidden layer.Differentiating the preceding error function with respectto a particular weight wiyieldsHere,f(x) is the perceptron’s output and xis the weighted sum ofthe inputs.To compute the second factor on the right-hand side,the derivative ofthesigmoid function f(x) is needed.It turns out that this has a particularly simpleform that can be written in terms off(x) itself:We use f¢(x) to denote this derivative.But we seek the derivative with respectto wi,not x.Becausethe derivative off(x) with respect to wiisPlugging this back into the derivative ofthe error function yieldsThis expression gives all that is needed to calculate the change ofweight wicaused by a particular example vector a(extended by 1 to represent the bias,asexplained previously).Having repeated this computation for each traininginstance,we add up the changes associated with a particular weight wi,multi-ply by the learning rate,and subtract the result from wi’s current value.dEdwyfxfxaii=-()()¢().dfxdwfxaii()=¢().xwaiii=Â,dfxdxfxfx()=()-()()1.dEdwyfxdfxdwii=-()()().230CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2306.3EXTENDING LINEAR MODELS231So far so good.But all this assumes that there is no hidden layer.With ahidden layer,things get a little trickier.Suppose f(xi) is the output ofthe ithhidden unit,wijis the weight ofthe connection from input j to the ith hiddenunit,and wiis the weight ofthe ith hidden unit to the output unit.The situa-tion is depicted in Figure 6.13.As before,f(x) is the output ofthe single unit inthe output layer.The update rule for the weights wiis essentially the same asabove,except that aiis replaced by the output ofthe ith hidden unit:However,to update the weights wijthe corresponding derivatives must be cal-culated.Applying the chain rule givesThe ﬁrst two factors are the same as in the previous equation.To compute thethird factor,differentiate further.BecausedEdwdEdxdxdwyfxfxdxdwijijij==-()()¢().dEdwyfxfxfxii=-()()¢()().hiddenunit 0input a1input akinput a0w0f(x1)hiddenunit 1hiddenunit loutputunitw1f(x2)wlf(xl)w00w10wl0w01w11wl1wlkw1kw0kf(x)Figure 6.13Multilayer perceptron with a hidden layer.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 231Furthermore,This means that we are ﬁnished.Putting everything together yields an equationfor the derivative ofthe error function with respect to the weights wij:As before,we calculate this value for every training instance,add up the changesassociated with a particular weight wij,multiply by the learning rate,and sub-tract the outcome from the current value ofwij.This derivation applies to a perceptron with one hidden layer.Ifthere are twohidden layers,the same strategy can be applied a second time to update theweights pertaining to the input connections ofthe ﬁrst hidden layer,propagat-ing the error from the output unit through the second hidden layer to the ﬁrstone.Because ofthis error propagation mechanism,this version ofthe genericgradient descent strategy is called backpropagation.We have tacitly assumed that the network’s output layer has just one unit,which is appropriate for two-class problems.For more than two classes,a sep-arate network could be learned for each class that distinguishes it from theremaining classes.A more compact classiﬁer can be obtained from a singlenetwork by creating an output unit for each class,connecting every unit in thehidden layer to every output unit.The squared error for a particular traininginstance is the sum ofsquared errors taken over all output units.The same tech-nique can be applied to predict several targets,or attribute values,simultane-ously by creating a separate output unit for each one.Intuitively,this may givebetter predictive accuracy than building a separate classiﬁer for each class attrib-ute ifthe underlying learning tasks are in some way related.We have assumed that weights are only updated after all training instanceshave been fed through the network and all the corresponding weight changeshave been accumulated.This is batchlearning,because all the training data isprocessed together.But exactly the same formulas can be used to update theweights incrementally after each training instance has been processed.This iscalled stochastic backpropagationbecause the overall error does not necessarilydecrease after every update and there is no guarantee that it will converge to adEdwyfxfxwfxaijiii=-()()¢()¢().dfxdwfxdxdwfxaiijiiijii()=¢()=¢().dxdwwdfxdwijiiij=().xwfxiii=()Â,232CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2326.3EXTENDING LINEAR MODELS233minimum.It can be used for online learning,in which new data arrives in acontinuous stream and every training instance is processed just once.In bothvariants ofbackpropagation,it is often helpful to standardize the attributes tohave zero mean and unit standard deviation.Before learning starts,each weightis initialized to a small,randomly chosen value based on a normal distributionwith zero mean.Like any other learning scheme,multilayer perceptrons trained with back-propagation may suffer from overﬁtting—especially ifthe network is muchlarger than what is actually necessary to represent the structure ofthe underly-ing learning problem.Many modiﬁcations have been proposed to alleviate this.A very simple one,called early stopping,works like reduced-error pruning inrule learners:a holdout set is used to decide when to stop performing furtheriterations ofthe backpropagation algorithm.The error on the holdout set ismeasured and the algorithm is terminated once the error begins to increase,because that indicates overﬁtting to the training data.Another method,called weight decay,adds to the error function a penalty term that consists ofthe squared sum ofall weights in the network.This attempts to limit theinﬂuence ofirrelevant connections on the network’s predictions by penalizinglarge weights that do not contribute a correspondingly large reduction in theerror.Although standard gradient descent is the simplest technique for learning theweights in a multilayer perceptron,it is by no means the most efﬁcient one.Inpractice,it tends to be rather slow.A trick that often improves performance isto include a momentumterm when updating weights:add to the new weightchange a small proportion ofthe update value from the previous iteration.Thissmooths the search process by making changes in direction less abrupt.Moresophisticated methods use information obtained from the second derivative ofthe error function as well;they can converge much more quickly.However,eventhose algorithms can be very slow compared with other methods ofclassiﬁca-tion learning.A serious disadvantage ofmultilayer perceptrons that contain hidden unitsis that they are essentially opaque.There are several techniques that attempt toextract rules from trained neural networks.However,it is unclear whether theyoffer any advantages over standard rule learners that induce rule sets directlyfrom data—especially considering that this can generally be done much morequickly than learning a multilayer perceptron in the ﬁrst place.Although multilayer perceptrons are the most prominent type ofneuralnetwork,many others have been proposed.Multilayer perceptrons belong to aclass ofnetworks called feedforward networksbecause they do not contain anycycles and the network’s output depends only on the current input instance.Recurrentneural networks do have cycles.Computations derived from earlierinput are fed back into the network,which gives them a kind ofmemory.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 233Radial basis function networksAnother popular type offeedforward network is the radial basis function (RBF)network.It has two layers,not counting the input layer,and differs from a multilayer perceptron in the way that the hidden units perform computations.Each hidden unit essentially represents a particular point in input space,and itsoutput,or activation,for a given instance depends on the distance between itspoint and the instance—which is just another point.Intuitively,the closer thesetwo points,the stronger the activation.This is achieved by using a nonlineartransformation function to convert the distance into a similarity measure.Abell-shaped Gaussian activation function,whose width may be different for eachhidden unit,is commonly used for this purpose.The hidden units are calledRBFs because the points in instance space for which a given hidden unit pro-duces the same activation form a hypersphere or hyperellipsoid.(In a multilayerperceptron,this is a hyperplane.)The output layer ofan RBF network is the same as that ofa multilayer per-ceptron:it takes a linear combination ofthe outputs ofthe hidden units and—in classiﬁcation problems—pipes it through the sigmoid function.The parameters that such a network learns are (a) the centers and widths ofthe RBFs and (b) the weights used to form the linear combination ofthe outputsobtained from the hidden layer.A signiﬁcant advantage over multilayer per-ceptrons is that the ﬁrst set ofparameters can be determined independently ofthe second set and still produce accurate classiﬁers.One way to determine the ﬁrst set ofparameters is to use clustering,withoutlooking at the class labels ofthe training instances at all.The simple k-meansclustering algorithm described in Section 4.8 can be applied,clustering eachclass independently to obtain kbasis functions for each class.Intuitively,theresulting RBFs represent prototype instances.Then the second set ofparame-ters can be learned,keeping the ﬁrst parameters ﬁxed.This involves learning alinear model using one ofthe techniques we have discussed (e.g.,linear or logis-tic regression).Ifthere are far fewer hidden units than training instances,thiscan be done very quickly.A disadvantage ofRBF networks is that they give every attribute the sameweight because all are treated equally in the distance computation.Hence theycannot deal effectively with irrelevant attributes—in contrast to multilayer per-ceptrons.Support vector machines share the same problem.In fact,supportvector machines with Gaussian kernels (i.e.,“RBF kernels”) are a particular typeofRBF network,in which one basis function is centered on every traininginstance,and the outputs are combined linearly by computing the maximummargin hyperplane.This has the effect that only some RBFs have a nonzeroweight—the ones that represent the support vectors.234CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  5/3/05  2:29 PM  Page 2346.4INSTANCE-BASED LEARNING235DiscussionSupport vector machines originated from research in statistical learning theory(Vapnik 1999),and a good starting point for exploration is a tutorial by Burges(1998).A general description,including generalization to the case in which thedata is not linearly separable,has been published by Cortes and Vapnik (1995).We have introduced the standard version ofsupport vector regression:Schölkopfet al.(1999) present a different version that has one parameter insteadoftwo.Smola and Schölkopf(2004) provide an extensive tutorial on supportvector regression.The (voted) kernel perceptron is due to Freund and Schapire (1999).Cris-tianini and Shawe-Taylor (2000) provide a nice introduction to support vectormachines and other kernel-based methods,including the optimization theoryunderlying the support vector learning algorithms.We have barely skimmed thesurface ofthese learning schemes,mainly because advanced mathematics liesjust beneath.The idea ofusing kernels to solve nonlinear problems has beenapplied to many algorithms,for example,principal components analysis(described in Section 7.3).A kernel is essentially a similarity function withcertain mathematical properties,and it is possible to deﬁne kernel functionsover all sorts ofstructures—for example,sets,strings,trees,and probability distributions.Shawe-Taylor and Cristianini (2004) cover kernel-based learningin detail.There is extensive literature on neural networks,and Bishop (1995) providesan excellent introduction to both multilayer perceptrons and RBF networks.Interest in neural networks appears to have declined since the arrival ofsupportvector machines,perhaps because the latter generally require fewer parametersto be tuned to achieve the same (or greater) accuracy.However,multilayer per-ceptrons have the advantage that they can learn to ignore irrelevant attributes,and RBF networks trained using k-means can be viewed as a quick-and-dirtymethod for ﬁnding a nonlinear classiﬁer.6.4Instance-based learningIn Section 4.7 we saw how the nearest-neighbor rule can be used to implementa basic form ofinstance-based learning.There are several practical problemswith this simple method.First,it tends to be slow for large training sets,becausethe entire set must be searched for each test instance—unless sophisticated datastructures such as kD-trees or ball trees are used.Second,it performs badly withnoisy data,because the class ofa test instance is determined by its single nearestneighbor without any “averaging”to help to eliminate noise.Third,it performsbadly when different attributes affect the outcome to different extents—in theP088407-Ch006.qxd  5/3/05  5:42 PM  Page 235extreme case,when some attributes are completely irrelevant—because allattributes contribute equally to the distance formula.Fourth,it does notperform explicit generalization,although we intimated in Section 3.8 (and illus-trated in Figure 3.8) that some instance-based learning systems do indeedperform explicit generalization.Reducing the number of exemplarsThe plain nearest-neighbor rule stores a lot ofredundant exemplars:it is almostalways completely unnecessary to save all the examples seen so far.A simplevariant is to classify each example with respect to the examples already seen andto save only ones that are misclassiﬁed.We use the term exemplarsto refer tothe already-seen instances that are used for classiﬁcation.Discarding correctlyclassiﬁed instances reduces the number ofexemplars and proves to be an effec-tive way to prune the exemplar database.Ideally,only a single exemplar is storedfor each important region ofthe instance space.However,early in the learningprocess examples may be discarded that later turn out to be important,possi-bly leading to some decrease in predictive accuracy.As the number ofstoredinstances increases,the accuracy ofthe model improves,and so the systemmakes fewer mistakes.Unfortunately,the strategy ofonly storing misclassiﬁed instances does notwork well in the face ofnoise.Noisy examples are very likely to be misclassiﬁed,and so the set ofstored exemplars tends to accumulate those that are least useful.This effect is easily observed experimentally.Thus this strategy is only a step-ping-stone on the way toward more effective instance-based learners.Pruning noisy exemplarsNoisy exemplars inevitably lower the performance ofany nearest-neighborscheme that does not suppress them because they have the effect ofrepeatedlymisclassifying new instances.There are two ways ofdealing with this.One is tolocate,instead ofthe single nearest neighbor,the knearest neighbors for somepredetermined constant kand assign the majority class to the unknowninstance.The only problem here is determining a suitable value ofk.Plainnearest-neighbor learning corresponds to k =1.The more noise,the greater theoptimal value ofk.One way to proceed is to perform cross-validation tests withdifferent values and choose the best.Although this is expensive in computationtime,it often yields excellent predictive performance.A second solution is to monitor the performance ofeach exemplar that isstored and discard ones that do not perform well.This can be done by keepinga record ofthe number ofcorrect and incorrect classiﬁcation decisions that eachexemplar makes.Two predetermined thresholds are set on the success ratio.When an exemplar’s performance drops below the lower one,it is deleted from236CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2366.4INSTANCE-BASED LEARNING237the exemplar set.Ifits performance exceeds the upper threshold,it is used forpredicting the class ofnew instances.Ifits performance lies between the two,itis not used for prediction but,whenever it is the closest exemplar to the newinstance (and thus would have been used for prediction ifits performancerecord had been good enough),its success statistics are updated as though ithad been used to classify that new instance.To accomplish this,we use the conﬁdence limits on the success probabilityofa Bernoulli process that we derived in Section 5.2.Recall that we took a certainnumber ofsuccesses Sout ofa total number oftrials Nas evidence on whichto base conﬁdence limits on the true underlying success rate p.Given a certainconﬁdence level of,say,5%,we can calculate upper and lower bounds and be95% sure that plies between them.To apply this to the problem ofdeciding when to accept a particular exem-plar,suppose that it has been used ntimes to classify other instances and that softhese have been successes.That allows us to estimate bounds,at a particularconﬁdence level,on the true success rate ofthis exemplar.Now suppose that theexemplar’s class has occurred ctimes out ofa total number Noftraininginstances.This allows us to estimate bounds on the default success rate,that is,the probability ofsuccessfully classifying an instance ofthis class without anyinformation about other instances.We insist that the lower conﬁdence boundon its success rate exceeds the upper conﬁdence bound on the default successrate.We use the same method to devise a criterion for rejecting a poorly per-forming exemplar,requiring that the upper conﬁdence bound on its success ratelies below the lower conﬁdence bound on the default success rate.With a suitable choice ofthresholds,this scheme works well.In a particularimplementation,called IB3for Instance-Based Learner version 3,a conﬁdencelevel of5% is used to determine acceptance,whereas a level of12.5% is usedfor rejection.The lower percentage ﬁgure produces a wider conﬁdence interval,which makes a more stringent criterion because it is harder for the lower boundofone interval to lie above the upper bound ofthe other.The criterion foracceptance is more stringent than that for rejection,making it more difﬁcult foran instance to be accepted.The reason for a less stringent rejection criterion isthat there is little to be lost by dropping instances with only moderately poorclassiﬁcation accuracies:they will probably be replaced by a similar instancelater.Using these thresholds the method has been found to improve the per-formance ofinstance-based learning and,at the same time,dramatically reducethe number ofexemplars—particularly noisy exemplars—that are stored.Weighting attributesThe Euclidean distance function,modiﬁed to scale all attribute values tobetween 0 and 1,works well in domains in which the attributes are equally rel-P088407-Ch006.qxd  4/30/05  11:18 AM  Page 237evant to the outcome.Such domains,however,are the exception rather than therule.In most domains some attributes are irrelevant,and some relevant onesare less important than others.The next improvement in instance-based learn-ing is to learn the relevance ofeach attribute incrementally by dynamicallyupdating feature weights.In some schemes,the weights are class speciﬁc in that an attribute may bemore important to one class than to another.To cater for this,a description isproduced for each class that distinguishes its members from members ofallother classes.This leads to the problem that an unknown test instance may beassigned to several different classes,or to no classes at all—a problem that is alltoo familiar from our description ofrule induction.Heuristic solutions areapplied to resolve these situations.The distance metric incorporates the feature weights w1,w2,...,wnon eachdimension:In the case ofclass-speciﬁc feature weights,there will be a separate set ofweightsfor each class.All attribute weights are updated after each training instance is classiﬁed,andthe most similar exemplar (or the most similar exemplar ofeach class) is usedas the basis for updating.Call the training instance xand the most similar exem-plar y.For each attribute i,the difference |xi-yi|is a measure ofthe contribu-tion ofthat attribute to the decision.Ifthis difference is small then the attributecontributes positively,whereas ifit is large it may contribute negatively.Thebasic idea is to update the ith weight on the basis ofthe size ofthis differenceand whether the classiﬁcation was indeed correct.Ifthe classiﬁcation is correctthe associated weight is increased and ifit is incorrect it is decreased,the amountofincrease or decrease being governed by the size ofthe difference:large ifthedifference is small and vice versa.The weight change is generally followed by arenormalization step.A simpler strategy,which may be equally effective,is toleave the weights alone ifthe decision is correct and ifit is incorrect to increasethe weights for those attributes that differ most greatly,accentuating the differ-ence.Details ofthese weight adaptation algorithms are described by Aha (1992).A good test ofwhether an attribute weighting method works is to add irrel-evant attributes to all examples in a dataset.Ideally,the introduction ofirrele-vant attributes should not affect either the quality ofpredictions or the numberofexemplars stored.Generalizing exemplarsGeneralized exemplars are rectangular regions ofinstance space,called hyper-rectanglesbecause they are high-dimensional.When classifying new instances itwxywxywxynnn121122222222-()+-()++-()....238CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2386.4INSTANCE-BASED LEARNING239is necessary to modify the distance function as described below to allow the dis-tance to a hyperrectangle to be computed.When a new exemplar is classiﬁedcorrectly,it is generalized by simply merging it with the nearest exemplar ofthesame class.The nearest exemplar may be either a single instance or a hyperrec-tangle.In the former case,a new hyperrectangle is created that covers the oldand the new instance.In the latter,the hyperrectangle is enlarged to encompassthe new instance.Finally,ifthe prediction is incorrect and it was a hyperrec-tangle that was responsible for the incorrect prediction,the hyperrectangle’sboundaries are altered so that it shrinks away from the new instance.It is necessary to decide at the outset whether overgeneralization caused bynesting or overlapping hyperrectangles is to be permitted or not.Ifit is to beavoided,a check is made before generalizing a new example to see whether anyregions offeature space conﬂict with the proposed new hyperrectangle.Iftheydo,the generalization is aborted and the example is stored verbatim.Note thatoverlapping hyperrectangles are precisely analogous to situations in which thesame example is covered by two or more rules in a rule set.In some schemes generalized exemplars can be nested in that they may becompletely contained within one another in the same way that,in some repre-sentations,rules may have exceptions.To do this,whenever an example is incor-rectly classiﬁed,a fallback heuristic is tried using the second nearest neighbor ifit would have produced a correct prediction in a further attempt to performgeneralization.This second-chance mechanism promotes nesting ofhyperrec-tangles.Ifan example falls within a rectangle ofthe wrong class that alreadycontains an exemplar ofthe same class,the two are generalized into a new“exception”hyperrectangle nested within the original one.For nested general-ized exemplars,the learning process frequently begins with a small number ofseed instances to prevent all examples ofthe same class from being generalizedinto a single rectangle that covers most ofthe problem space.Distance functions for generalized exemplarsWith generalized exemplars is necessary to generalize the distance function tocompute the distance from an instance to a generalized exemplar,as well as toanother instance.The distance from an instance to a hyperrectangle is deﬁnedto be zero ifthe point lies within the hyperrectangle.The simplest way to gen-eralize the distance function to compute the distance from an exterior point toa hyperrectangle is to choose the closest instance within it and measure the dis-tance to that.However,this reduces the beneﬁt ofgeneralization because it rein-troduces dependence on a particular single example.More precisely,whereasnew instances that happen to lie within a hyperrectangle continue to beneﬁtfrom generalizations,ones that lie outside do not.It might be better to use thedistance from the nearest part ofthe hyperrectangle instead.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 239Figure 6.14 shows the implicit boundaries that are formed between two rec-tangular classes ifthe distance metric is adjusted to measure distance to thenearest point ofa rectangle.Even in two dimensions the boundary contains atotal ofnine regions (they are numbered for easy identiﬁcation);the situationwill be more complex for higher-dimensional hyperrectangles.Proceeding from the lower left,the ﬁrst region,in which the boundary islinear,lies outside the extent ofboth rectangles—to the left ofboth borders ofthe larger one and below both borders ofthe smaller one.The second is withinthe extent ofone rectangle—to the right ofthe leftmost border ofthe largerrectangle—but outside that ofthe other—below both borders ofthe smallerone.In this region the boundary is parabolic,because the locus ofa point thatis the same distance from a given line as from a given point is a parabola.The240CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMES123456789Figure 6.14A boundary between two rectangular classes.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2406.4INSTANCE-BASED LEARNING241third region is where the boundary meets the lower border ofthe larger rec-tangle when projected upward and the left border ofthe smaller one when pro-jected to the right.The boundary is linear in this region,because it is equidistantfrom these two borders.The fourth is where the boundary lies to the right ofthe larger rectangle but below the bottom ofthat rectangle.In this case theboundary is parabolic because it is the locus ofpoints equidistant from the lowerright corner ofthe larger rectangle and the left side ofthe smaller one.The ﬁfthregion lies between the two rectangles:here the boundary is vertical.The patternis repeated in the upper right part ofthe diagram:ﬁrst parabolic,then linear,then parabolic (although this particular parabola is almost indistinguishablefrom a straight line),and ﬁnally linear as the boundary ﬁnally escapes from thescope ofboth rectangles.This simple situation certainly deﬁnes a complex boundary! Ofcourse,it isnot necessary to represent the boundary explicitly;it is generated implicitly bythe nearest-neighbor calculation.Nevertheless,the solution is still not a verygood one.Whereas taking the distance from the nearest instance within a hyper-rectangle is overly dependent on the position ofthat particular instance,takingthe distance to the nearest point ofthe hyperrectangle is overly dependent onthat corner ofthe rectangle—the nearest example might be a long way from thecorner.A ﬁnal problem concerns measuring the distance to hyperrectangles thatoverlap or are nested.This complicates the situation because an instance mayfall within more than one hyperrectangle.A suitable heuristic for use in this caseis to choose the class ofthe most speciﬁc hyperrectangle containing the instance,that is,the one covering the smallest area ofinstance space.Whether or not overlap or nesting is permitted,the distance function shouldbe modiﬁed to take account ofboth the observed prediction accuracy ofexem-plars and the relative importance ofdifferent features,as described in the pre-ceding sections on pruning noisy exemplars and attribute weighting.Generalized distance functionsThere are many different ways ofdeﬁning a distance function,and it is hard toﬁnd rational grounds for any particular choice.An elegant solution is to con-sider one instance being transformed into another through a sequence ofpre-deﬁned elementary operations and to calculate the probability ofsuch asequence occurring ifoperations are chosen randomly.Robustness is improvedifall possible transformation paths are considered,weighted by their probabil-ities,and the scheme generalizes naturally to the problem ofcalculating the distance between an instance and a set ofother instances by considering trans-formations to all instances in the set.Through such a technique it is possible toconsider each instance as exerting a “sphere ofinﬂuence,”but a sphere with softP088407-Ch006.qxd  4/30/05  11:18 AM  Page 241boundaries rather than the hard-edged cutoffimplied by the k-nearest-neighborrule,in which any particular example is either “in”or “out”ofthe decision.With such a measure,given a test instance whose class is unknown,its dis-tance to the set ofall training instances in each class in turn is calculated,andthe closest class is chosen.It turns out that nominal and numeric attributes canbe treated in a uniform manner within this transformation-based approach bydeﬁning different transformation sets,and it is even possible to take account ofunusual attribute types—such as degrees ofarc or days ofthe week,which aremeasured on a circular scale.DiscussionNearest-neighbor methods gained popularity in machine learning through thework ofAha (1992),who showed that,when combined with noisy exemplarpruning and attribute weighting,instance-based learning performs well in com-parison with other methods.It is worth noting that although we have describedit solely in the context ofclassiﬁcation rather than numeric prediction prob-lems,it applies to these equally well:predictions can be obtained by combiningthe predicted values ofthe knearest neighbors and weighting them by distance.Viewed in instance space,the standard rule- and tree-based representationsare only capable ofrepresenting class boundaries that are parallel to the axesdeﬁned by the attributes.This is not a handicap for nominal attributes,but itis for numeric ones.Non-axis-parallel class boundaries can only be approxi-mated by covering the region above or below the boundary with several axis-parallel rectangles,the number ofrectangles determining the degree ofapproximation.In contrast,the instance-based method can easily representarbitrary linear boundaries.Even with just one example ofeach oftwo classes,the boundary implied by the nearest-neighbor rule is a straight line ofarbitraryorientation,namely the perpendicular bisector ofthe line joining the examples.Plain instance-based learning does not produce explicit knowledge repre-sentations except by selecting representative exemplars.However,when com-bined with exemplar generalization,a set ofrules can be obtained that may becompared with those produced by other machine learning schemes.The rulestend to be more conservative because the distance metric,modiﬁed to incor-porate generalized exemplars,can be used to process examples that do not fallwithin the rules.This reduces the pressure to produce rules that cover the wholeexample space or even all ofthe training examples.On the other hand,the incre-mental nature ofmost instance-based learning methods means that rules areformed eagerly,after only part ofthe training set has been seen;and thisinevitably reduces their quality.We have not given precise algorithms for variants ofinstance-based learningthat involve generalization because it is not clear what the best way to do gen-242CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2426.5NUMERIC PREDICTION243eralization is.Salzberg (1991) suggested that generalization with nested exem-plars can achieve a high degree ofclassiﬁcation ofaccuracy on a variety ofdif-ferent problems,a conclusion disputed by Wettschereck and Dietterich (1995),who argued that these results were fortuitous and did not hold in otherdomains.Martin (1995) explored the idea that it is not the generalization butthe overgeneralization that occurs when hyperrectangles nest or overlap that isresponsible for poor performance and demonstrated that ifnesting and over-lapping are avoided excellent results are achieved in a large number ofdomains.The generalized distance function based on transformations is described byCleary and Trigg (1995).Exemplar generalization is a rare example ofa learning strategy in which thesearch proceeds from speciﬁc to general rather than from general to speciﬁc asin the case oftree or rule induction.There is no particular reason why speciﬁc-to-general searching should necessarily be handicapped by forcing the examplesto be considered in a strictly incremental fashion,and batch-orientedapproaches exist that generate rules using a basic instance-based approach.Moreover,it seems that the idea ofproducing conservative generalizations andcoping with instances that are not covered by choosing the “closest”generaliza-tion is an excellent one that will eventually be extended to ordinary tree andrule inducers.6.5Numeric predictionTrees that are used for numeric prediction are just like ordinary decision trees except that at each leafthey store either a class value that represents theaverage value ofinstances that reach the leaf,in which case the tree is called aregression tree,or a linear regression model that predicts the class value ofinstances that reach the leaf,in which case it is called a model tree.In whatfollows we will describe model trees because regression trees are really a specialcase.Regression and model trees are constructed by ﬁrst using a decision treeinduction algorithm to build an initial tree.However,whereas most decisiontree algorithms choose the splitting attribute to maximize the information gain,it is appropriate for numeric prediction to instead minimize the intrasubsetvariation in the class values down each branch.Once the basic tree has beenformed,consideration is given to pruning the tree back from each leaf,just aswith ordinary decision trees.The only difference between regression tree andmodel tree induction is that for the latter,each node is replaced by a regressionplane instead ofa constant value.The attributes that serve to deﬁne that regres-sion are precisely those that participate in decisions in the subtree that will bepruned,that is,in nodes underneath the current one.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 243Following an extensive description ofmodel trees,we brieﬂy explain how togenerate rules from model trees,and then describe another approach to numericprediction—locally weighted linear regression.Whereas model trees derive from the basic divide-and-conquer decision tree methodology,locally weightedregression is inspired by the instance-based methods for classiﬁcation that wedescribed in the previous section.Like instance-based learning,it performs all“learning”at prediction time.Although locally weighted regression resemblesmodel trees in that it uses linear regression to ﬁt models locally to particularareas ofinstance space,it does so in quite a different way.Model treesWhen a model tree is used to predict the value for a test instance,the tree is fol-lowed down to a leafin the normal way,using the instance’s attribute values tomake routing decisions at each node.The leafwill contain a linear model basedon some ofthe attribute values,and this is evaluated for the test instance toyield a raw predicted value.Instead ofusing this raw value directly,however,it turns out to be beneﬁcialto use a smoothing process to compensate for the sharp discontinuities that willinevitably occur between adjacent linear models at the leaves ofthe pruned tree.This is a particular problem for models constructed from a small number oftraining instances.Smoothing can be accomplished by producing linear modelsfor each internal node,as well as for the leaves,at the time the tree is built.Then,once the leafmodel has been used to obtain the raw predicted value for a testinstance,that value is ﬁltered along the path back to the root,smoothing it ateach node by combining it with the value predicted by the linear model for thatnode.An appropriate smoothing calculation iswhere p¢is the prediction passed up to the next higher node,pis the predictionpassed to this node from below,qis the value predicted by the model at thisnode,nis the number oftraining instances that reach the node below,and kisa smoothing constant.Experiments show that smoothing substantially increasesthe accuracy ofpredictions.Exactly the same smoothing process can be accomplished by incorporatingthe interior models into each leafmodel after the tree has been built.Then,during the classiﬁcation process,only the leafmodels are used.The disadvan-tage is that the leafmodels tend to be larger and more difﬁcult to comprehend,because many coefﬁcients that were previously zero become nonzero when theinterior nodes’models are incorporated.pnpkqnk¢=++,244CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2446.5NUMERIC PREDICTION245Building the treeThe splitting criterion is used to determine which attribute is the best to splitthat portion Tofthe training data that reaches a particular node.It is based ontreating the standard deviation ofthe class values in Tas a measure ofthe errorat that node and calculating the expected reduction in error as a result oftestingeach attribute at that node.The attribute that maximizes the expected errorreduction is chosen for splitting at the node.The expected error reduction,which we call SDR for standard deviationreduction,is calculated bywhere T1,T2,...are the sets that result from splitting the node according to thechosen attribute.The splitting process terminates when the class values ofthe instances thatreach a node vary very slightly,that is,when their standard deviation is only asmall fraction (say,less than 5%) ofthe standard deviation ofthe originalinstance set.Splitting also terminates when just a few instances remain,say fouror fewer.Experiments show that the results obtained are not very sensitive tothe exact choice ofthese thresholds.Pruning the treeAs noted previously,a linear model is needed for each interior node ofthe tree,not just at the leaves,for use in the smoothing process.Before pruning,a modelis calculated for each node ofthe unpruned tree.The model takes the formwhere a1,a2,...,akare attribute values.The weights w1,w2,...,wkare calculatedusing standard regression.However,only the attributes that are tested in thesubtree below this node are used in the regression,because the other attributesthat affect the predicted value have been taken into account in the tests that leadto the node.Note that we have tacitly assumed that attributes are numeric:wedescribe the handling ofnominal attributes in the next section.The pruning procedure makes use ofan estimate,at each node,oftheexpected error for test data.First,the absolute difference between the predictedvalue and the actual class value is averaged over each ofthe training instancesthat reach that node.Because the tree has been built expressly for this dataset,this average will underestimate the expected error for unseen cases.To com-pensate,it is multiplied by the factor (n+n)/(n-n),where nis the number oftraining instances that reach the node and nis the number ofparameters in thelinear model that gives the class value at that node.wwawawakk01122++++...,SDR=()-¥()ÂsdTTTsdTiii,P088407-Ch006.qxd  4/30/05  11:18 AM  Page 245The expected error for test data at a node is calculated as described previ-ously,using the linear model for prediction.Because ofthe compensation factor(n+n)/(n-n),it may be that the linear model can be further simpliﬁed bydropping terms to minimize the estimated error.Dropping a term decreases themultiplication factor,which may be enough to offset the inevitable increase inaverage error over the training instances.Terms are dropped one by one,greed-ily,as long as the error estimate decreases.Finally,once a linear model is in place for each interior node,the tree ispruned back from the leaves as long as the expected estimated error decreases.The expected error for the linear model at that node is compared with theexpected error from the subtree below.To calculate the latter,the error fromeach branch is combined into a single,overall value for the node by weightingthe branch by the proportion ofthe training instances that go down it and com-bining the error estimates linearly using those weights.Nominal attributesBefore constructing a model tree,all nominal attributes are transformed intobinary variables that are then treated as numeric.For each nominal attribute,the average class value corresponding to each possible value in the enumerationis calculated from the training instances,and the values in the enumeration aresorted according to these averages.Then,ifthe nominal attribute has kpossi-ble values,it is replaced by k-1 synthetic binary attributes,the ith being 0 ifthe value is one ofthe ﬁrst iin the ordering and 1 otherwise.Thus all splits arebinary:they involve either a numeric attribute or a synthetic binary one,treatedas a numeric attribute.It is possible to prove analytically that the best split at a node for a nominalvariable with kvalues is one ofthe k-1 positions obtained by ordering the average class values for each value ofthe attribute.This sorting operationshould really be repeated at each node;however,there is an inevitable increasein noise because ofsmall numbers ofinstances at lower nodes in the tree (andin some cases nodes may not represent all values for some attributes),and not much is lost by performing the sorting just once,before starting to build amodel tree.Missing valuesTo take account ofmissing values,a modiﬁcation is made to the SDR formula.The ﬁnal formula,including the missing value compensation,isSDR=¥()-¥()ÈÎÍ˘˚˙Œ{}ÂmTsdTTTsdTjjLRj,,246CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2466.5NUMERIC PREDICTION247where mis the number ofinstances without missing values for that attribute,and Tis the set ofinstances that reach this node.TLand TRare sets that resultfrom splitting on this attribute—because all tests on attributes are now binary.When processing both training and test instances,once an attribute is selectedfor splitting it is necessary to divide the instances into subsets according to theirvalue for this attribute.An obvious problem arises when the value is missing.An interesting technique called surrogate splittinghas been developed to handlethis situation.It involves ﬁnding another attribute to split on in place oftheoriginal one and using it instead.The attribute is chosen as the one most highlycorrelated with the original attribute.However,this technique is both complexto implement and time consuming to execute.A simpler heuristic is to use the class value as the surrogate attribute,in thebeliefthat,a priori,this is the attribute most likely to be correlated with the onebeing used for splitting.Ofcourse,this is only possible when processing thetraining set,because for test examples the class is unknown.A simple solutionfor test examples is simply to replace the unknown attribute value with theaverage value ofthat attribute for the training examples that reach the node—which has the effect,for a binary attribute,ofchoosing the most populoussubnode.This simple approach seems to work well in practice.Let’s consider in more detail how to use the class value as a surrogate attrib-ute during the training process.We ﬁrst deal with all instances for which thevalue ofthe splitting attribute is known.We determine a threshold for splittingin the usual way,by sorting the instances according to its value and,for eachpossible split point,calculating the SDR according to the preceding formula,choosing the split point that yields the greatest reduction in error.Only theinstances for which the value ofthe splitting attribute is known are used todetermine the split point.Then we divide these instances into the two sets Land Raccording to thetest.We determine whether the instances in Lor R have the greater average classvalue,and we calculate the average ofthese two averages.Then,an instance forwhich this attribute value is unknown is placed into Lor R according to whetherits class value exceeds this overall average or not.Ifit does,it goes into whicheverofLand Rhas the greater average class value;otherwise,it goes into the onewith the smaller average class value.When the splitting stops,all the missingvalues will be replaced by the average values ofthe corresponding attributes ofthe training instances reaching the leaves.Pseudocode for model tree inductionFigure 6.15 gives pseudocode for the model tree algorithm we have described.The two main parts are creating a tree by successively splitting nodes,performedP088407-Ch006.qxd  4/30/05  11:18 AM  Page 247248CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESMakeModelTree (instances){   SD = sd(instances)  for each k-valued nominal attribute    convert into k-1 synthetic binary attributes  root = newNode  root.instances = instances  split(root)  prune(root)  printTree(root)} split(node){   if sizeof(node.instances) < 4 or sd(node.instances) < 0.05*SD    node.type = LEAF  else    node.type = INTERIOR    for each attribute      for all possible split positions of the attribute        calculate the attribute's SDR    node.attribute = attribute with maximum SDR    split(node.left)    split(node.right)} prune(node){   if node = INTERIOR then    prune(node.leftChild)    prune(node.rightChild)    node.model = linearRegression(node)    if subtreeError(node) > error(node) then      node.type = LEAF} subtreeError(node){   l = node.left; r = node.right  if node = INTERIOR then    return (sizeof(l.instances)*subtreeError(l)          + sizeof(r.instances)*subtreeError(r))/sizeof(node.instances)  else return error(node)} Figure 6.15Pseudocode for model tree induction.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2486.5NUMERIC PREDICTION249by split,and pruning it from the leaves upward,performed by prune.The nodedata structure contains a type ﬂag indicating whether it is an internal node ora leaf,pointers to the left and right child,the set ofinstances that reach thatnode,the attribute that is used for splitting at that node,and a structure repre-senting the linear model for the node.The sdfunction called at the beginning ofthe main program and again at the beginning ofsplitcalculates the standard deviation ofthe class values ofa set ofinstances.Then follows the procedure for obtaining synthetic binary attributes that was described previously.Standard procedures for creat-ing new nodes and printing the ﬁnal tree are not shown.In split,sizeofreturnsthe number ofelements in a set.Missing attribute values are dealt with asdescribed earlier.The SDR is calculated according to the equation at the begin-ning ofthe previous subsection.Although not shown in the code,it is set toinﬁnity ifsplitting on the attribute would create a leafwith fewer than twoinstances.In prune,the linearRegressionroutine recursively descends thesubtree collecting attributes,performs a linear regression on the instances at thatnode as a function ofthose attributes,and then greedily drops terms ifdoingso improves the error estimate,as described earlier.Finally,the errorfunctionreturnswhere nis the number ofinstances at the node and nis the number ofparam-eters in the node’s linear model.Figure 6.16 gives an example ofa model tree formed by this algorithm for aproblem with two numeric and two nominal attributes.What is to be predictedis the rise time ofa simulated servo system involving a servo ampliﬁer,motor,lead screw,and sliding carriage.The nominal attributes play important roles.Four synthetic binary attributes have been created for each ofthe ﬁve-valuednominal attributes motorand screw,and they are shown in Table 6.1 in termsofthe two sets ofvalues to which they correspond.The ordering ofthesevalues—D,E,C,B,A for motorand coincidentally D,E,C,B,A for screwalso—is determined from the training data:the rise time averaged over all examplesfor which motor=D is less than that averaged over examples for which motor=E,which is less than when motor=C,and so on.It is apparent from the mag-nitude ofthe coefﬁcients in Table 6.1 that motor=D versus E,C,B,A plays aleading role in the LM2 model,and motor=D,E versus C,B,A plays a leadingrole in LM1.Both motorand screwalso play minor roles in several ofthe models.The decision tree shows a three-way split on a numeric attribute.First a binary-splitting tree was generated in the usual way.It turned out that the root and oneofits descendants tested the same attribute,pgain,and a simple algorithm wasnnn+-¥Ânndeviation from predicted class valueinstances,P088407-Ch006.qxd  4/30/05  11:18 AM  Page 249used to conﬂate these two nodes into the slightly more comprehensible tree thatis shown.Rules from model treesModel trees are essentially decision trees with linear models at the leaves.Likedecision trees,they may suffer from the replicated subtree problem explained250CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESLM3LM4LM5LM1LM2LM6LM7pgainmotorvgainscrewscrew≤ 3.5≤ 2.5> 2.5(3.5,4.5]> 4.5D,E,CD,E,CD,E,C,BAB,AB,AFigure 6.16Model tree for a dataset with nominal attributes.Table 6.1Linear models in the model tree.ModelLM1LM2LM3LM4LM5LM6LM7Constant term-0.442.603.500.180.520.360.23pgainvgain0.820.420.06motor =D vs. E, C, B, A3.300.240.42motor =D, E vs. C, B, A1.80-0.160.150.22motor =D, E, C vs. B, A0.100.090.07motor =D, E, C, Bvs. A0.18screw =D vs. E, C, B, Ascrew =D, E vs. C, B, A0.47screw =D, E, C vs. B, A0.630.280.34screw = D, E, C, B vs. A0.900.160.14P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2506.5NUMERIC PREDICTION251in Section 3.3,and sometimes the structure can be expressed much more con-cisely using a set ofrules instead ofa tree.Can we generate rules for numericprediction? Recall the rule learner described in Section 6.2 that uses separate-and-conquer in conjunction with partial decision trees to extract decision rulesfrom trees.The same strategy can be applied to model trees to generate deci-sion lists for numeric prediction.First build a partial model tree from all the data.Pick one ofthe leaves and make it into a rule.Remove the data covered by that leaf;then repeat the process with the remaining data.The question is,how to build the partialmodel tree,that is,a tree with unexpanded nodes? This boils down to the question ofhow to pick which node to expand next.The algorithm ofFigure 6.5 (Section 6.2) picks the node whose entropy for the class attribute issmallest.For model trees,whose predictions are numeric,simply use the vari-ance instead.This is based on the same rationale:the lower the variance,theshallower the subtree and the shorter the rule.The rest ofthe algorithm staysthe same,with the model tree learner’s split selection method and pruning strategy replacing the decision tree learner’s.Because the model tree’s leaves arelinear models,the corresponding rules will have linear models on the right-handside.There is one caveat when using model trees in this fashion to generate rulesets:the smoothing process that the model tree learner employs.It turns outthat using smoothed model trees does not reduce the error in the ﬁnal rule set’spredictions.This may be because smoothing works best for contiguous data,butthe separate-and-conquer scheme removes data covered by previous rules,leaving holes in the distribution.Smoothing,ifit is done at all,must be per-formed after the rule set has been generated.Locally weighted linear regressionAn alternative approach to numeric prediction is the method oflocally weightedlinear regression.With model trees,the tree structure divides the instance spaceinto regions,and a linear model is found for each ofthem.In effect,the train-ing data determines how the instance space is partitioned.Locally weightedregression,on the other hand,generates local models at prediction time bygiving higher weight to instances in the neighborhood ofthe particular testinstance.More speciﬁcally,it weights the training instances according to their distance to the test instance and performs a linear regression on theweighted data.Training instances close to the test instance receive a high weight;those far away receive a low one.In other words,a linear model is tailormade for the particular test instance at hand and used to predict the instance’sclass value.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 251To use locally weighted regression,you need to decide on a distance-basedweighting scheme for the training instances.A common choice is to weight theinstances according to the inverse oftheir Euclidean distance from the testinstance.Another possibility is to use the Euclidean distance in conjunction witha Gaussian kernel function.However,there is no clear evidence that the choiceofweighting function is critical.More important is the selection ofa “smooth-ing parameter”that is used to scale the distance function—the distance is mul-tiplied by the inverse ofthis parameter.Ifit is set to a small value,only instancesvery close to the test instance will receive signiﬁcant weight;ifit is large,moredistant instances will also have a signiﬁcant impact on the model.One way ofchoosing the smoothing parameter is to set it to the distance ofthe kth-nearesttraining instance so that its value becomes smaller as the volume oftrainingdata increases.The best choice ofkdepends on the amount ofnoise in the data.The more noise there is,the more neighbors should be included in the linearmodel.Generally,an appropriate smoothing parameter is found using cross-validation.Like model trees,locally weighted linear regression is able to approximatenonlinear functions.One ofits main advantages is that it is ideally suited forincremental learning:all training is done at prediction time,so new instancescan be added to the training data at any time.However,like other instance-based methods,it is slow at deriving a prediction for a test instance.First,the training instances must be scanned to compute their weights;then,aweighted linear regression is performed on these instances.Also,like otherinstance-based methods,locally weighted regression provides little informationabout the global structure ofthe training dataset.Note that ifthe smoothingparameter is based on the kth-nearest neighbor and the weighting function gives zero weight to more distant instances,the kD-trees and ball trees describedin Section 4.7 can be used to speed up the process ofﬁnding the relevant neighbors.Locally weighted learning is not restricted to linear regression:it can beapplied with any learning technique that can handle weighted instances.In par-ticular,you can use it for classiﬁcation.Most algorithms can be easily adaptedto deal with weights.The trick is to realize that (integer) weights can be simu-lated by creating several copies ofthe same instance.Whenever the learningalgorithm uses an instance when computing a model,just pretend that it isaccompanied by the appropriate number ofidentical shadow instances.Thisalso works ifthe weight is not an integer.For example,in the Naïve Bayes algo-rithm described in Section 4.2,multiply the counts derived from an instance bythe instance’s weight,and—voilà—you have a version ofNaïve Bayes that canbe used for locally weighted learning.It turns out that locally weighted Naïve Bayes works extremely well in prac-tice,outperforming both Naïve Bayes itselfand the k-nearest-neighbor tech-252CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2526.5NUMERIC PREDICTION253nique.It also compares favorably with far more sophisticated ways ofenhanc-ing Naïve Bayes by relaxing its intrinsic independence assumption.Locallyweighted learning only assumes independence within a neighborhood,notglobally in the whole instance space as standard Naïve Bayes does.In principle,locally weighted learning can also be applied to decision treesand other models that are more complex than linear regression and Naïve Bayes.However,it is beneﬁcial here because it is primarily a way ofallowing simplemodels to become more ﬂexible by allowing them to approximate arbitrarytargets.Ifthe underlying learning algorithm can already do that,there is littlepoint in applying locally weighted learning.Nevertheless it may improve othersimple models—for example,linear support vector machines and logisticregression.DiscussionRegression trees were introduced in the CART system ofBreiman et al.(1984).CART,for “classiﬁcation and regression trees,”incorporated a decision treeinducer for discrete classes much like that ofC4.5,which was developed inde-pendently,and a scheme for inducing regression trees.Many ofthe techniquesdescribed in the preceding section,such as the method ofhandling nominalattributes and the surrogate device for dealing with missing values,wereincluded in CART.However,model trees did not appear until much morerecently,being ﬁrst described by Quinlan (1992).Using model trees for gener-ating rule sets (although not partial trees) has been explored by Hall et al.(1999).Model tree induction is not so commonly used as decision tree induction,partly because comprehensive descriptions (and implementations) ofthe tech-nique have become available only recently (Wang and Witten 1997).Neural netsare more commonly used for predicting numeric quantities,although theysuffer from the disadvantage that the structures they produce are opaque andcannot be used to help us understand the nature ofthe solution.Although thereare techniques for producing understandable insights from the structure ofneural networks,the arbitrary nature ofthe internal representation means thatthere may be dramatic variations between networks ofidentical architecturetrained on the same data.By dividing the function being induced into linearpatches,model trees provide a representation that is reproducible and at leastsomewhat comprehensible.There are many variations oflocally weighted learning.For example,statis-ticians have considered using locally quadratic models instead oflinear ones andhave applied locally weighted logistic regression to classiﬁcation problems.Also,many different potential weighting and distance functions can be found in theliterature.Atkeson et al.(1997) have written an excellent survey on locallyP088407-Ch006.qxd  4/30/05  11:18 AM  Page 253weighted learning,primarily in the context ofregression problems.Frank et al.(2003) evaluated the use oflocally weighted learning in conjunction with NaïveBayes.6.6ClusteringIn Section 4.8 we examined the k-means clustering algorithm in which kinitialpoints are chosen to represent initial cluster centers,all data points are assignedto the nearest one,the mean value ofthe points in each cluster is computed toform its new cluster center,and iteration continues until there are no changesin the clusters.This procedure only works when the number ofclusters is knownin advance,and this section begins by describing what you can do ifit is not.Next we examine two techniques that do not partition instances into disjointclusters as k-means does.The ﬁrst is an incremental clustering method that wasdeveloped in the late 1980s and embodied in a pair ofsystems called Cobweb(for nominal attributes) and Classit (for numeric attributes).Both come up witha hierarchical grouping ofinstances and use a measure ofcluster “quality”calledcategory utility.The second is a statistical clustering method based on a mixturemodel ofdifferent probability distributions,one for each cluster.It assignsinstances to classes probabilistically,not deterministically.We explain the basictechnique and sketch the working ofa comprehensive clustering scheme calledAutoClass.Choosing the number of clustersSuppose you are using k-means but do not know the number ofclusters inadvance.One solution is to try out different possibilities and see which is best—that is,which one minimizes the total squared distance ofall points to theircluster center.A simple strategy is to start from a given minimum,perhaps k =1,and work up to a small ﬁxed maximum,using cross-validation to ﬁnd thebest value.Because k-means is slow,and cross-validation makes it even slower,it will probably not be feasible to try many possible values for k.Note that onthe training data the “best”clustering according to the total squared distancecriterion will always be to choose as many clusters as there are data points! Topenalize solutions with many clusters you have to apply something like the MDLcriterion ofSection 5.10,or use cross-validation.Another possibility is to begin by ﬁnding a few clusters and determiningwhether it is worth splitting them.You could choose k =2,perform k-meansclustering until it terminates,and then consider splitting each cluster.Compu-tation time will be reduced considerably ifthe initial two-way clustering is considered irrevocable and splitting is investigated for each component 254CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2546.6CLUSTERING255independently.One way to split a cluster is to make a new seed,one standarddeviation away from the cluster’s center in the direction ofits greatest variation,and to make a second seed the same distance in the opposite direction.(Alternatively,ifthis is too slow,choose a distance proportional to the cluster’sbounding box and a random direction.) Then apply k-means to the points in the cluster with these two new seeds.Having tentatively split a cluster,is it worthwhile retaining the split or is theoriginal cluster equally plausible by itself? It’s no good looking at the totalsquared distance ofall points to their cluster center—this is bound to be smallerfor two subclusters.A penalty should be incurred for inventing an extra cluster,and this is a job for the MDL criterion.That principle can be applied to seewhether the information required to specify the two new cluster centers,alongwith the information required to specify each point with respect to them,exceeds the information required to specify the original center and all the pointswith respect to it.Ifso,the new clustering is unproductive and should be abandoned.Ifthe split is retained,try splitting each new cluster further.Continue theprocess until no worthwhile splits remain.Additional implementation efﬁciency can be achieved by combining this iterative clustering process with the kD-tree or ball tree data structure advocatedin Section 4.8.Then,the data points are reached by working down the tree from the root.When considering splitting a cluster,there is no need to consider the whole tree;just consider those parts ofit that are needed to coverthe cluster.For example,when deciding whether to split the lower left cluster inFigure 4.16(a) on page 140 (below the thick line),it is only necessary to con-sider nodes A and B ofthe tree in Figure 4.16(b),because node C is irrelevantto that cluster.Incremental clusteringWhereas the k-means algorithm iterates over the whole dataset until convergence is reached,the clustering methods that we examine next workincrementally,instance by instance.At any stage the clustering forms a tree withinstances at the leaves and a root node that represents the entire dataset.In thebeginning the tree consists ofthe root alone.Instances are added one by one,and the tree is updated appropriately at each stage.Updating may merely be acase ofﬁnding the right place to put a leafrepresenting the new instance,or itmay involve radically restructuring the part ofthe tree that is affected by thenew instance.The key to deciding how and where to update is a quantity calledcategory utility,which measures the overall quality ofa partition ofinstancesinto clusters.We defer detailed consideration ofhow this is deﬁned until thenext subsection and look ﬁrst at how the clustering algorithm works.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 255The procedure is best illustrated by an example.We will use the familiarweather data again,but without the playattribute.To track progress the 14instances are labeled a,b,c,...,n(as in Table 4.6),and for interest we includethe class yesor noin the label—although it should be emphasized that for thisartiﬁcial dataset there is little reason to suppose that the two classes ofinstanceshould fall into separate categories.Figure 6.17 shows the situation at salientpoints throughout the clustering procedure.At the beginning,when new instances are absorbed into the structure,theyeach form their own subcluster under the overall top-level cluster.Each newinstance is processed by tentatively placing it into each ofthe existing leaves andevaluating the category utility ofthe resulting set ofthe top-level node’s chil-dren to see whether the leafis a good “host”for the new instance.For each of256CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESFigure 6.17Clustering the weather data.a:noa:nob:noc:yesd:yese:yesa:nob:noc:yesd:yese:yesf:noa:nob:noc:yesd:yese:yesf:nog:yesb:noc:yesa:nod:yesh:noe:yesf:nog:yesa:nod:yesh:noc:yesl:yesb:nok:yesg:yesf:noj:yesm:yesn:noe:yesi:yesP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2566.6CLUSTERING257the ﬁrst ﬁve instances,there is no such host:it is better,in terms ofcategoryutility,to form a new leaffor each instance.With the sixth it ﬁnally becomesbeneﬁcial to form a cluster,joining the new instance fwith the old one—thehost—e.Ifyou look back at Table 4.6 (page 103) you will see that the ﬁfth andsixth instances are indeed very similar,differing only in the windyattribute (andplay,which is being ignored here).The next example,g,is placed in the samecluster (it differs from eonly in outlook).This involves another call to the clus-tering procedure.First,gis evaluated to see which ofthe ﬁve children oftheroot makes the best host;it turns out to be the rightmost,the one that is alreadya cluster.Then the clustering algorithm is invoked with this as the root,and itstwo children are evaluated to see which would make the better host.In this caseit proves best,according to the category utility measure,to add the new instanceas a subcluster in its own right.Ifwe were to continue in this vein,there would be no possibility ofany radicalrestructuring ofthe tree,and the ﬁnal clustering would be excessively depend-ent on the ordering ofexamples.To avoid this,there is provision for restruc-turing,and you can see it come into play when instance his added in the nextstep shown in Figure 6.17.In this case two existing nodes are mergedinto a singlecluster:nodes aand dare merged before the new instance his added.One wayofaccomplishing this would be to consider all pairs ofnodes for merging andevaluate the category utility ofeach pair.However,that would be computa-tionally expensive and would involve a lot ofrepeated work ifit were under-taken whenever a new instance was added.Instead,whenever the nodes at a particular level are scanned for a suitablehost,both the best-matching node—the one that produces the greatest categoryutility for the split at that level—and the runner-up are noted.The best one willform the host for the new instance (unless that new instance is better offin acluster ofits own).However,before setting to work on putting the new instancein with the host,consideration is given to merging the host and the runner-up.In this case,ais the preferred host and dis the runner-up.When a merge ofaand dis evaluated,it turns out that it would improve the category utilitymeasure.Consequently,these two nodes are merged,yielding a version oftheﬁfth hierarchy ofFigure 6.17 before his added.Then,consideration is given tothe placement ofhin the new,merged node;and it turns out to be best to makeit a subcluster in its own right,as shown.An operation converse to merging is also implemented,called splitting,although it does not take place in this particular example.Whenever the besthost is identiﬁed,and merging has not proved beneﬁcial,consideration is givento splitting the host node.Splitting has exactly the opposite effect ofmerging,taking a node and replacing it with its children.For example,splitting the right-most node in the fourth hierarchy ofFigure 6.17 would raise the e,f,and gleavesup a level,making them siblings ofa,b,c,and d.Merging and splitting provideP088407-Ch006.qxd  4/30/05  11:18 AM  Page 257an incremental way ofrestructuring the tree to compensate for incorrect choicescaused by infelicitous ordering ofexamples.The ﬁnal hierarchy for all 14 examples is shown at the end ofFigure 6.17.There are two major clusters,each ofwhich subdivides further into its own subclusters.Ifthe play/don’t playdistinction really represented an inherentfeature ofthe data,a single cluster would be expected for each outcome.Nosuch clean structure is observed,although a (very) generous eye might discerna slight tendency at lower levels for yesinstances to group together,and likewisefor noinstances.Careful analysis ofthe clustering reveals some anomalies.(Table 4.6 will help ifyou want to follow this analysis in detail.) For example,instances aand bare actually very similar to each other,yet they end up in com-pletely different parts ofthe tree.Instance bends up with k,which is a worsematch than a.Instance aends up with dand h,and it is certainly not as similarto das it is to b.The reason why aand bbecome separated is that aand dgetmerged,as described previously,because they form the best and second-besthosts for h.It was unlucky that aand bwere the ﬁrst two examples:ifeither had occurred later,it may well have ended up with the other.Subsequent split-ting and remerging may be able to rectify this anomaly,but in this case theydidn’t.Exactly the same scheme works for numeric attributes.Category utility isdeﬁned for these as well,based on an estimate ofthe mean and standard devi-ation ofthe value ofthat attribute.Details are deferred to the next subsection.However,there is just one problem that we must attend to here:when estimat-ing the standard deviation ofan attribute for a particular node,the result willbe zero ifthe node contains only one instance,as it does more often than not.Unfortunately,zero variances produce inﬁnite values in the category utilityformula.A simple heuristic solution is to impose a minimum variance on eachattribute.It can be argued that because no measurement is completely precise,it is reasonable to impose such a minimum:it represents the measurement errorin a single sample.This parameter is called acuity.Figure 6.18(a) shows,at the top,a hierarchical clustering produced by theincremental algorithm for part ofthe Iris dataset (30 instances,10 from eachclass).At the top level there are two clusters (i.e.,subclusters ofthe single noderepresenting the whole dataset).The ﬁrst contains both Iris virginicasand Irisversicolors,and the second contains only Iris setosas.The Iris setosasthemselvessplit into two subclusters,one with four cultivars and the other with six.Theother top-level cluster splits into three subclusters,each with a fairly complexstructure.Both the ﬁrst and second contain only Iris versicolors,with one excep-tion,a stray Iris virginica,in each case;the third contains only Iris virginicas.This represents a fairly satisfactory clustering ofthe Iris data:it shows that thethree genera are not artiﬁcial at all but reﬂect genuine differences in the data.This is,however,a slightly overoptimistic conclusion,because quite a bit of258CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2586.6CLUSTERING259VersicolorVersicolorVersicolorVirginicaVersicolorVirginicaVersicolorVersicolorVersicolorVersicolorVersicolorVersicolorVirginicaVirginicaVirginicaVirginicaVirginicaVirginicaVirginicaVirginicaSetosaSetosaSetosaSetosaSetosaSetosaSetosaSetosaSetosaSetosa(a)VersicolorVersicolorVersicolorVersicolorVersicolorVersicolorVirginicaVirginicaVersicolorVersicolorVersicolorVersicolorVirginicaVirginicaVirginicaVirginicaVirginicaVirginicaVirginicaVirginicaSetosaSetosaSetosaSetosaSetosaSetosaSetosaSetosaSetosaSetosa(b)Figure 6.18Hierarchical clusterings ofthe iris data.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 259experimentation with the acuity parameter was necessary to obtain such a nicedivision.The clusterings produced by this scheme contain one leaffor every instance.This produces an overwhelmingly large hierarchy for datasets ofany reasonablesize,corresponding,in a sense,to overﬁtting the particular dataset.Conse-quently,a second numeric parameter called cutoffis used to suppress growth.Some instances are deemed to be sufﬁciently similar to others to not warrantformation oftheir own child node,and this parameter governs the similaritythreshold.Cutoffis speciﬁed in terms ofcategory utility:when the increase incategory utility from adding a new node is sufﬁciently small,that node is cutoff.Figure 6.18(b) shows the same Iris data,clustered with cutoffin effect.Manyleafnodes contain several instances:these are children ofthe parent node thathave been cut off.The division into the three types ofiris is a little easier to seefrom this hierarchy because some ofthe detail is suppressed.Again,however,some experimentation with the cutoffparameter was necessary to get this result,and in fact a sharper cutoffleads to much less satisfactory clusters.Similar clusterings are obtained ifthe full Iris dataset of150 instances is used.However,the results depend on the ordering ofexamples:Figure 6.18 wasobtained by alternating the three varieties ofiris in the input ﬁle.Ifall Iris setosasare presented ﬁrst,followed by all Iris versicolorsand then all Iris virginicas,theresulting clusters are quite unsatisfactory.Category utilityNow we look at how the category utility,which measures the overall quality ofa partition ofinstances into clusters,is calculated.In Section 5.9 we learned howthe MDL measure could,in principle,be used to evaluate the quality ofclus-tering.Category utility is not MDL based but rather resembles a kind ofquad-ratic loss function deﬁned on conditional probabilities.The deﬁnition ofcategory utility is rather formidable:where C1,C2,...,Ckare the kclusters;the outer summation is over these clus-ters;the next inner one sums over the attributes;aiis the ith attribute,and ittakes on values vi1,vi2,...which are dealt with by the sum over j.Note that theprobabilities themselves are obtained by summing over all instances:thus thereis a further implied level ofsummation.This expression makes a great deal ofsense ifyou take the time to examineit.The point ofhaving a cluster is that it will give some advantage in predict-CUCCCCavCavkkiijiijji1222,,...,()=[]=[]-=[]()ÂÂÂPrPrPrlll260CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2606.6CLUSTERING261ing the values ofattributes ofinstances in that cluster—that is,Pr[ai=vij|Cᐉ]is a better estimate ofthe probability that attribute aihas value vij,for an instancein cluster Cᐉ,than Pr[ai=vij] because it takes account ofthe cluster the instanceis in.Ifthat information doesn’t help,the clusters aren’t doing much good! Sowhat the preceding measure calculates,inside the multiple summation,is theamount by which that information doeshelp in terms ofthe differences betweensquares ofprobabilities.This is not quite the standard squared-differencemetric,because that sums the squares ofthe differences (which produces a sym-metric result),and the present measure sums the difference ofthe squares(which,appropriately,does not produce a symmetric result).The differencesbetween squares ofprobabilities are summed over all attributes,and all theirpossible values,in the inner double summation.Then it is summed over all clus-ters,weighted by their probabilities,in the outer summation.The overall division by kis a little hard to justify because the squared differ-ences have already been summed over the categories.It essentially provides a“per cluster”ﬁgure for the category utility that discourages overﬁtting.Other-wise,because the probabilities are derived by summing over the appropriateinstances,the very best category utility would be obtained by placing eachinstance in its own cluster.Then,Pr[ai=vij|Cᐉ] would be 1 for the value thatattribute aiactually has for the single instance in category Cᐉand 0 for all othervalues;and the numerator ofthe category utility formula will end up aswhere nis the total number ofattributes.This is the greatest value that thenumerator can have;and so ifit were not for the additional division by kin thecategory utility formula,there would never be any incentive to form clusterscontaining more than one member.This extra factor is best viewed as a rudi-mentary overﬁtting-avoidance heuristic.This category utility formula applies only to nominal attributes.However,itcan easily be extended to numeric attributes by assuming that their distributionis normal with a given (observed) mean mand standard deviation s.The prob-ability density function for an attribute aisThe analog ofsumming the squares ofattribute–value probabilities iswhere siis the standard deviation ofthe attribute ai.Thus for a numeric attrib-ute,we estimate the standard deviation from the data,both within the clusterPravfadaiijjiii=[]¤()=ÂÚ2212ps,faa()=-()ÊËÁˆ¯˜12222psmsexp.naviijji-=[]ÂÂPr2,P088407-Ch006.qxd  4/30/05  11:18 AM  Page 261(siᐉ) and for the data over all clusters (si),and use these in the category utilityformula:Now the problem mentioned previously that occurs when the standard devia-tion estimate is zero becomes apparent:a zero standard deviation produces aninﬁnite value ofthe category utility formula.Imposing a prespeciﬁed minimumvariance on each attribute,the acuity,is a rough-and-ready solution to theproblem.Probability-based clusteringSome ofthe shortcomings ofthe heuristic clustering described previously havealready become apparent:the arbitrary division by kin the category utilityformula that is necessary to prevent overﬁtting,the need to supply an artiﬁcialminimum value for the standard deviation ofclusters,the ad hoc cutoffvalueto prevent every instance from becoming a cluster in its own right.On top ofthis is the uncertainty inherent in incremental algorithms:to what extent is theresult dependent on the order ofexamples? Are the local restructuring opera-tions ofmerging and splitting really enough to reverse the effect ofbad initialdecisions caused by unlucky ordering? Does the ﬁnal result represent even a localminimum ofcategory utility? Add to this the problem that one never knowshow far the ﬁnal conﬁguration is from a globalminimum—and that the stan-dard trick ofrepeating the clustering procedure several times and choosing thebest will destroy the incremental nature ofthe algorithm.Finally,doesn’t thehierarchical nature ofthe result really beg the question ofwhich are the bestclusters? There are so many clusters in Figure 6.18 that it is hard to separate thewheat from the chaff.A more principled statistical approach to the clustering problem can over-come some ofthese shortcomings.From a probabilistic perspective,the goal ofclustering is to ﬁnd the most likely set ofclusters given the data (and,inevitably,prior expectations).Because no ﬁnite amount ofevidence is enough to make acompletely ﬁrm decision on the matter,instances—even training instances—should not be placed categorically in one cluster or the other:instead they have a certain probability ofbelonging to each cluster.This helps to eliminatethe brittleness that is often associated with methods that make hard and fastjudgments.The foundation for statistical clustering is a statistical model called ﬁnite mix-tures.A mixtureis a set ofkprobability distributions,representing kclusters,that govern the attribute values for members ofthat cluster.In other words,eachCUCCCkCkiii1211211,,...,.()=[]-ÊËˆ¯ÂÂPrlllpss262CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2626.6CLUSTERING263distribution gives the probability that a particular instance would have a certainset ofattribute values ifit were knownto be a member ofthat cluster.Eachcluster has a different distribution.Any particular instance “really”belongs toone and only one ofthe clusters,but it is not known which one.Finally,theclusters are not equally likely:there is some probability distribution that reﬂectstheir relative populations.The simplest ﬁnite mixture situation occurs when there is only one numericattribute,which has a Gaussian or normal distribution for each cluster—butwith different means and variances.The clustering problem is to take a set ofinstances—in this case each instance is just a number—and a prespeciﬁednumber ofclusters,and work out each cluster’s mean and variance and the pop-ulation distribution between the clusters.The mixture model combines severalnormal distributions,and its probability density function looks like a mountainrange with a peak for each component.Figure 6.19 shows a simple example.There are two clusters,A and B,and eachhas a normal distribution with means and standard deviations:mAand sAforcluster A,and mBand sBfor cluster B,respectively.Samples are taken from thesedistributions,using cluster A with probability pAand cluster B with probabilitypB(where pA+pB=1) and resulting in a dataset like that shown.Now,imaginebeing given the dataset without the classes—just the numbers—and being askedto determine the ﬁve parameters that characterize the model:mA,sA,mB,sB,andpA(the parameter pBcan be calculated directly from pA).That is the ﬁnitemixture problem.Ifyou knew which ofthe two distributions each instance came from,ﬁndingthe ﬁve parameters would be easy—just estimate the mean and standard devi-ation for the cluster A samples and the cluster B samples separately,using theformulas(The use ofn-1 rather than nas the denominator in the second formula is atechnicality ofsampling:it makes little difference in practice ifnis used instead.)Here,x1,x2,...,xnare the samples from the distribution A or B.To estimatethe ﬁfth parameter pA,just take the proportion ofthe instances that are in theA cluster.Ifyou knew the ﬁve parameters,ﬁnding the probabilities that a given instancecomes from each distribution would be easy.Given an instance x,the probabil-ity that it belongs to cluster A issmmm2122221=-()+-()++-()-xxxnn....m=+++xxxnn12...P088407-Ch006.qxd  4/30/05  11:18 AM  Page 263where f(x;mA,sA) is the normal distribution function for cluster A,that is:The denominator Pr[x] will disappear:we calculate the numerators for bothPr[A |x] and Pr[B |x] and normalize them by dividing by their sum.This wholeprocedure is just the same as the way numeric attributes are treated in the NaïveBayes learning scheme ofSection 4.2.And the caveat explained there applieshere too:strictly speaking,f(x;mA,sA) is not the probability Pr[x|A] becausethe probability ofxbeing any particular real number is zero,but the normal-fxex;,.mspsms()=-()12222PrAPrAPrAPrPrAAAxxxfxpx[]=[]◊[][]=()[];,ms264CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESdataA 51 A 43 B 62 B 64 A 45 A 42 A 46 A 45 A 45 B 62 A 47 A 52 B 64 A 51 B 65 A 48 A 49A 46A 48 B 64 A 51 B 63 A 43 B 65 B 66 B 65 A 46A 51 A 48 B 64 A 42 A 48 A 41B 64 A 51 A 52 B 62 A 49 A 48 B 62 A 43 A 40A 39 B 62 B 64 A 52 B 63 B 64 A 48 B 64 A 48(a)model304050mA = 50, sA = 5, pA = 0.6mB = 65, sB = 2, pB = 0.4AB6070(b)Figure 6.19A two-class mixture model.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2646.6CLUSTERING265ization process makes the ﬁnal result correct.Note that the ﬁnal outcome is nota particular cluster but rather the probabilitieswith which xbelongs to clusterA and cluster B.The EM algorithmThe problem is that we know neither ofthese things:not the distribution thateach training instance came from nor the ﬁve parameters ofthe mixture model.So we adopt the procedure used for the k-means clustering algorithm anditerate.Start with initial guesses for the ﬁve parameters,use them to calculatethe cluster probabilities for each instance,use these probabilities to reestimatethe parameters,and repeat.(Ifyou prefer,you can start with guesses for theclasses ofthe instances instead.) This is called the EM algorithm,for expecta-tion–maximization.The ﬁrst step,calculation ofthe cluster probabilities (whichare the “expected”class values) is “expectation”;the second,calculation ofthedistribution parameters,is “maximization”ofthe likelihood ofthe distributionsgiven the data.A slight adjustment must be made to the parameter estimation equations toaccount for the fact that it is only cluster probabilities,not the clusters them-selves,that are known for each instance.These probabilities just act like weights.Ifwiis the probability that instance ibelongs to cluster A,the mean and stan-dard deviation for cluster A are—where now the xiare allthe instances,not just those belonging to cluster A.(This differs in a small detail from the estimate for the standard deviation givenon page 101.Technically speaking,this is a “maximum likelihood”estimator forthe variance,whereas the formula on page 101 is for an “unbiased”estimator.The difference is not important in practice.)Now consider how to terminate the iteration.The k-means algorithm stops when the classes ofthe instances don’t change from one iteration to thenext—a “ﬁxed point”has been reached.In the EM algorithm things are not quiteso easy:the algorithm converges toward a ﬁxed point but never actually getsthere.But we can see how close it is by calculating the overall likelihood thatthe data came from this dataset,given the values for the ﬁve parameters.Thisoverall likelihood is obtained by multiplying the probabilities ofthe individualinstances i:smmmA2=-()+-()++-()+++wxwxwxwwwnnn112222212......mA=++++++wxwxwxwwwnnn112212......P088407-Ch006.qxd  4/30/05  11:18 AM  Page 265where the probabilities given the clusters A and B are determined from thenormal distribution function f(x;m,s).This overall likelihood is a measure ofthe “goodness”ofthe clustering and increases at each iteration ofthe EM algo-rithm.Again,there is a technical difﬁculty with equating the probability ofaparticular value ofxwith f(x;m,s),and in this case the effect does not disap-pear because no probability normalization operation is applied.The upshot isthat the preceding likelihood expression is not a probability and does not nec-essarily lie between zero and one:nevertheless,its magnitude still reﬂects the quality ofthe clustering.In practical implementations its logarithm is calculated instead:this is done by summing the logarithms ofthe individualcomponents,avoiding all the multiplications.But the overall conclusion stillholds:you should iterate until the increase in log-likelihood becomes negligi-ble.For example,a practical implementation might iterate until the differencebetween successive values oflog-likelihood is less than 10-10for 10 successiveiterations.Typically,the log-likelihood will increase very sharply over the ﬁrstfew iterations and then converge rather quickly to a point that is virtually stationary.Although the EM algorithm is guaranteed to converge to a maximum,this is a localmaximum and may not necessarily be the same as the global max-imum.For a better chance ofobtaining the global maximum,the whole proce-dure should be repeated several times,with different initial guesses for theparameter values.The overall log-likelihood ﬁgure can be used to compare thedifferent ﬁnal conﬁgurations obtained:just choose the largest ofthe localmaxima.Extending the mixture modelNow that we have seen the Gaussian mixture model for two distributions,let’sconsider how to extend it to more realistic situations.The basic method is justthe same,but because the mathematical notation becomes formidable we willnot develop it in full detail.Changing the algorithm from two-class problems to multiclass problems iscompletely straightforward as long as the number kofnormal distributions isgiven in advance.The model can be extended from a single numeric attribute per instance tomultiple attributes as long as independence between attributes is assumed.Theprobabilities for each attribute are multiplied together to obtain the joint prob-ability for the instance,just as in the Naïve Bayes method.pxpxiiiABPrAPrB[]+[]()’,266CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2666.6CLUSTERING267When the dataset is known in advance to contain correlated attributes,theindependence assumption no longer holds.Instead,two attributes can bemodeled jointly using a bivariate normal distribution,in which each has its ownmean value but the two standard deviations are replaced by a “covariancematrix”with four numeric parameters.There are standard statistical techniquesfor estimating the class probabilities ofinstances and for estimating the means and covariance matrix given the instances and their class probabilities.Several correlated attributes can be handled using a multivariate distribution.The number ofparameters increases with the square ofthe number ofjointlyvarying attributes.With nindependent attributes,there are 2nparameters,amean and a standard deviation for each.With ncovariant attributes,there aren+n(n+1)/2 parameters,a mean for each and an n¥ncovariance matrix thatis symmetric and therefore involves n(n+1)/2 different quantities.This escala-tion in the number ofparameters has serious consequences for overﬁtting,aswe will explain later.To cater for nominal attributes,the normal distribution must be abandoned.Instead,a nominal attribute with vpossible values is characterized by vnumbersrepresenting the probability ofeach one.A different set ofnumbers is neededfor every class;kvparameters in all.The situation is very similar to the Naïve Bayes method.The two steps ofexpectation and maximization corre-spond exactly to operations we have studied before.Expectation—estimatingthe cluster to which each instance belongs given the distribution parameters—is just like determining the class ofan unknown instance.Maximization—estimating the parameters from the classiﬁed instances—is just like determin-ing the attribute–value probabilities from the training instances,with the small difference that in the EM algorithm instances are assigned to classes probabilistically rather than categorically.In Section 4.2 we encountered the problem that probability estimates can turn out to be zero,and the sameproblem occurs here too.Fortunately,the solution is just as simple—use theLaplace estimator.Naïve Bayes assumes that attributes are independent—that is why it is called“naïve.”A pair ofcorrelated nominal attributes with v1and v2possible values,respectively,can be replaced with a single covariant attribute with v1v2possiblevalues.Again,the number ofparameters escalates as the number ofdependentattributes increases,and this has implications for probability estimates and over-ﬁtting that we will come to shortly.The presence ofboth numeric and nominal attributes in the data to be clus-tered presents no particular problem.Covariant numeric and nominal attrib-utes are more difﬁcult to handle,and we will not describe them here.Missing values can be accommodated in various different ways.Missingvalues ofnominal attributes can simply be left out ofthe probability calcula-P088407-Ch006.qxd  4/30/05  11:18 AM  Page 267tions,as described in Section 4.2;alternatively they can be treated as an addi-tional value ofthe attribute,to be modeled as any other value.Which is moreappropriate depends on what it means for a value to be “missing.”Exactly thesame possibilities exist for numeric attributes.With all these enhancements,probabilistic clustering becomes quite sophis-ticated.The EM algorithm is used throughout to do the basic work.The usermust specify the number ofclusters to be sought,the type ofeach attribute(numeric or nominal),which attributes are modeled as covarying,and what to do about missing values.Moreover,different distributions than the onesdescribed previously can be used.Although the normal distribution is usuallya good choice for numeric attributes,it is not suitable for attributes (such asweight) that have a predetermined minimum (zero,in the case ofweight) butno upper bound;in this case a “log-normal”distribution is more appropriate.Numeric attributes that are bounded above and below can be modeled by a “log-odds”distribution.Attributes that are integer counts rather than real valuesare best modeled by the “Poisson”distribution.A comprehensive system mightallow these distributions to be speciﬁed individually for each attribute.In eachcase,the distribution involves numeric parameters—probabilities ofall possi-ble values for discrete attributes and mean and standard deviation for continu-ous ones.In this section we have been talking about clustering.But you may be thinking that these enhancements could be applied just as well to the NaïveBayes algorithm too—and you’d be right.A comprehensive probabilisticmodeler could accommodate both clustering and classiﬁcation learning,nominal and numeric attributes with a variety ofdistributions,various possi-bilities ofcovariation,and different ways ofdealing with missing values.Theuser would specify,as part ofthe domain knowledge,which distributions to usefor which attributes.Bayesian clusteringHowever,there is a snag:overﬁtting.You might say that ifwe are not sure whichattributes are dependent on each other,why not be on the safe side and specifythat allthe attributes are covariant? The answer is that the more parametersthere are,the greater the chance that the resulting structure is overﬁtted to thetraining data—and covariance increases the number ofparameters dramati-cally.The problem ofoverﬁtting occurs throughout machine learning,andprobabilistic clustering is no exception.There are two ways that it can occur:through specifying too large a number ofclusters and through specifying dis-tributions with too many parameters.The extreme case oftoo many clusters occurs when there is one for everydata point:clearly,that will be overﬁtted to the training data.In fact,in the268CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2686.6CLUSTERING269mixture model,problems will occur whenever any ofthe normal distributionsbecomes so narrow that it is centered on just one data point.Consequently,implementations generally insist that clusters contain at least two different datavalues.Whenever there are a large number ofparameters,the problem ofoverﬁttingarises.Ifyou were unsure ofwhich attributes were covariant,you might try outdifferent possibilities and choose the one that maximized the overall probabil-ity ofthe data given the clustering that was found.Unfortunately,the moreparameters there are,the larger the overall data probability will tend to be—notnecessarily because ofbetter clustering but because ofoverﬁtting.The moreparameters there are to play with,the easier it is to ﬁnd a clustering that seemsgood.It would be nice ifsomehow you could penalize the model for introducingnew parameters.One principled way ofdoing this is to adopt a fully Bayesianapproach in which every parameter has a prior probability distribution.Then,whenever a new parameter is introduced,its prior probability must be incor-porated into the overall likelihood ﬁgure.Because this will involve multiplyingthe overall likelihood by a number less than one—the prior probability—it willautomatically penalize the addition ofnew parameters.To improve the overalllikelihood,the new parameters will have to yield a beneﬁt that outweighs thepenalty.In a sense,the Laplace estimator that we met in Section 4.2,and whose usewe advocated earlier to counter the problem ofzero probability estimates fornominal values,is just such a device.Whenever observed probabilities are small,the Laplace estimator exacts a penalty because it makes probabilities that arezero,or close to zero,greater,and this will decrease the overall likelihood ofthedata.Making two nominal attributes covariant will exacerbate the problem.Instead ofv1+v2parameters,where v1and v2are the number ofpossible values,there are now v1v2,greatly increasing the chance ofa large number ofsmall esti-mated probabilities.In fact,the Laplace estimator is tantamount to using a par-ticular prior distribution for the introduction ofnew parameters.The same technique can be used to penalize the introduction oflargenumbers ofclusters,just by using a prespeciﬁed prior distribution that decayssharply as the number ofclusters increases.AutoClass is a comprehensive Bayesian clustering scheme that uses the ﬁnitemixture model with prior distributions on all the parameters.It allows bothnumeric and nominal attributes and uses the EM algorithm to estimate theparameters ofthe probability distributions to best ﬁt the data.Because there isno guarantee that the EM algorithm converges to the global optimum,the pro-cedure is repeated for several different sets ofinitial values.But that is not all.AutoClass considers different numbers ofclusters and can consider differentamounts ofcovariance and different underlying probability distribution typesP088407-Ch006.qxd  4/30/05  11:18 AM  Page 269for the numeric attributes.This involves an additional,outer level ofsearch.Forexample,it initially evaluates the log-likelihood for 2,3,5,7,10,15,and 25 clus-ters:after that,it ﬁts a log-normal distribution to the resulting data and ran-domly selects from it more values to try.As you might imagine,the overallalgorithm is extremely computation intensive.In fact,the actual implementa-tion starts with a prespeciﬁed time bound and continues to iterate as long astime allows.Give it longer and the results may be better!DiscussionThe clustering methods that have been described produce different kinds ofoutput.All are capable oftaking new data in the form ofa test set and classify-ing it according to clusters that were discovered by analyzing a training set.However,the incremental clustering method is the only one that generates anexplicit knowledge structure that describes the clustering in a way that can bevisualized and reasoned about.The other algorithms produce clusters that couldbe visualized in instance space ifthe dimensionality were not too high.Ifa clustering method were used to label the instances ofthe training set withcluster numbers,that labeled set could then be used to train a rule or decisiontree learner.The resulting rules or tree would form an explicit description ofthe classes.A probabilistic clustering scheme could be used for the samepurpose,except that each instance would have multiple weighted labels and therule or decision tree learner would have to be able to cope with weightedinstances—as many can.Another application ofclustering is to ﬁll in any values ofthe attributes thatmay be missing.For example,it is possible to make a statistical estimate ofthevalue ofunknown attributes ofa particular instance,based on the class distri-bution for the instance itselfand the values ofthe unknown attributes for otherexamples.All the clustering methods we have examined make a basic assumption ofindependence among the attributes.AutoClass does allow the user to specify in advance that two or more attributes are dependent and should be modeledwith a joint probability distribution.(There are restrictions,however:nominalattributes may vary jointly,as may numeric attributes,but not both together.Moreover,missing values for jointly varying attributes are not catered for.) Itmay be advantageous to preprocess a dataset to make the attributes more inde-pendent,using a statistical technique such as the principal components trans-form described in Section 7.3.Note that joint variation that is speciﬁc toparticular classes will not be removed by such techniques;they only removeoverall joint variation that runs across all classes.Our description ofhow to modify k-means to ﬁnd a good value ofkbyrepeatedly splitting clusters and seeing whether the split is worthwhile follows270CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2706.7BAYESIAN NETWORKS271the X-means algorithm ofMoore and Pelleg (2000).However,instead oftheMDL principle they use a probabilistic scheme called the Bayes Information Criterion (Kass and Wasserman 1995).The incremental clustering procedure,based on the merging and splitting operations,was introduced in systems calledCobweb for nominal attributes (Fisher 1987) and Classit for numeric attributes(Gennari et al.1990).Both are based on a measure ofcategory utility that hadbeen deﬁned previously (Gluck and Corter 1985).The AutoClass program isdescribed by Cheeseman and Stutz (1995).Two implementations are available:the original research implementation,written in LISP,and a follow-up publicimplementation in C that is 10 or 20 times faster but somewhat morerestricted—for example,only the normal-distribution model is implementedfor numeric attributes.6.7Bayesian networksThe Naïve Bayes classiﬁer ofSection 4.2 and the logistic regression models ofSection 4.6 both produce probability estimates rather than predictions.For eachclass value,they estimate the probability that a given instance belongs to thatclass.Most other types ofclassiﬁers can be coerced into yielding this kind ofinformation ifnecessary.For example,probabilities can be obtained from adecision tree by computing the relative frequency ofeach class in a leafand froma decision list by examining the instances that a particular rule covers.Probability estimates are often more useful than plain predictions.They allow predictions to be ranked,and their expected cost to be minimized (seeSection 5.7).In fact,there is a strong argument for treating classiﬁcation learn-ing as the task oflearning class probability estimates from data.What is beingestimated is the conditional probability distribution ofthe values ofthe classattribute given the values ofthe other attributes.The classiﬁcation model rep-resents this conditional distribution in a concise and easily comprehensibleform.Viewed in this way,Naïve Bayes classiﬁers,logistic regression models,deci-sion trees,and so on,are just alternative ways ofrepresenting a conditionalprobability distribution.Ofcourse,they differ in representational power.Naïve Bayes classiﬁers and logistic regression models can only represent simpledistributions,whereas decision trees can represent—or at least approximate—arbitrary distributions.However,decision trees have their drawbacks:they frag-ment the training set into smaller and smaller pieces,which inevitably yield lessreliable probability estimates,and they suffer from the replicated subtreeproblem described in Section 3.2.Rule sets go some way toward addressing theseshortcomings,but the design ofa good rule learner is guided by heuristics withscant theoretical justiﬁcation.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 271Does this mean that we have to accept our fate and live with these shortcom-ings? No! There is a statistically based alternative:a theoretically well-foundedway ofrepresenting probability distributions concisely and comprehensibly ina graphical manner.The structures are called Bayesian networks.They are drawnas a network ofnodes,one for each attribute,connected by directed edges insuch a way that there are no cycles—a directed acyclic graph.In our explanation ofhow to interpret Bayesian networks and how to learnthem from data,we will make some simplifying assumptions.We assume thatall attributes are nominal and that there are no missing values.Some advancedlearning algorithms can create new attributes in addition to the ones present inthe data—so-called hidden attributes whose values cannot be observed.Thesecan support better models ifthey represent salient features ofthe underlyingproblem,and Bayesian networks provide a good way ofusing them at predic-tion time.However,they make both learning and prediction far more complexand time consuming,so we will not consider them here.Making predictionsFigure 6.20 shows a simple Bayesian network for the weather data.It has a nodefor each ofthe four attributes outlook,temperature,humidity,and windyandone for the class attribute play.An edge leads from the playnode to each oftheother nodes.But in Bayesian networks the structure ofthe graph is only halfthe story.Figure 6.20 shows a table inside each node.The information in thetables deﬁnes a probability distribution that is used to predict the class proba-bilities for any given instance.Before looking at how to compute this probability distribution,consider theinformation in the tables.The lower four tables (for outlook,temperature,humidity,and windy) have two parts separated by a vertical line.On the left arethe values ofplay,and on the right are the corresponding probabilities for eachvalue ofthe attribute represented by the node.In general,the left side containsa column for every edge pointing to the node,in this case just the playattrib-ute.That is why the table associated with playitselfdoes not have a left side:ithas no parents.In general,each row ofprobabilities corresponds to one com-bination ofvalues ofthe parent attributes,and the entries in the row show theprobability ofeach value ofthe node’s attribute given this combination.Ineffect,each row deﬁnes a probability distribution over the values ofthe node’sattribute.The entries in a row always sum to 1.Figure 6.21 shows a more complex network for the same problem,wherethree nodes (windy,temperature,and humidity) have two parents.Again,thereis one column on the left for each parent and as many columns on the right asthe attribute has values.Consider the ﬁrst row ofthe table associated with thetemperaturenode.The left side gives a value for each parent attribute,playand272CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2726.7BAYESIAN NETWORKS273outlook;the right gives a probability for each value oftemperature.For example,the ﬁrst number (0.143) is the probability oftemperaturetaking on the valuehot,given that playand outlookhave values yesand sunny,respectively.How are the tables used to predict the probability ofeach class value for agiven instance? This turns out to be very easy,because we are assuming thatthere are no missing values.The instance speciﬁes a value for each attribute.Foreach node in the network,look up the probability ofthe node’s attribute valuebased on the row determined by its parents’attribute values.Then just multi-ply all these probabilities together.playplayyes.633no.367outlookplayyesnooutlooksunny.238.538overcast.429.077rainy.333.385temperatureplayyesnotemperaturehot.238.385mild.429.385cool.333.231windyplayyesnowindyfalse.350.583true.650.417humidityplayyesnohumidityhigh.350.750normal.650.250Figure 6.20A simple Bayesian network for the weather data.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 273274CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESplayplayyes.633no.367outlookplayyesnooutlooksunny.238.538overcast.429.077rainy.333.385temperatureplayyesyesyesnononotemperaturehot.143.455.111.556.333.143mild.429.273.556.333.333.429cool.429.273.333.111.333.429humidityplayyesyesyesnononohumidityhigh.500.500.125.833.833.250normal.500.500.875.167.167.750temperat.hotmildcoolhotmildcooloutlooksunnyovercastrainysunnyovercastrainywindyplayyesyesyesnononowindyfalse.500.500.125.375.500.833true.500.500.875.625.500.167outlooksunnyovercastrainysunnyovercastrainyFigure 6.21Another Bayesian network for the weather data.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 2746.7BAYESIAN NETWORKS275For example,consider an instance with values outlook =rainy,temperature =cool,humidity =high,and windy =true.To calculate the probability for play =no,observe that the network in Figure 6.21 gives probability 0.367 from nodeplay,0.385 from outlook,0.429from temperature,0.250 from humidity,and0.167 from windy.The product is 0.0025.The same calculation for play =yesyields 0.0077.However,these are clearly not the ﬁnal answer:the ﬁnal proba-bilities must sum to 1,whereas 0.0025 and 0.0077 don’t.They are actually thejoint probabilities Pr[play =no,E] and Pr[play =yes,E],where Edenotes all theevidence given by the instance’s attribute values.Joint probabilities measure thelikelihood ofobserving an instance that exhibits the attribute values in Eas wellas the respective class value.They only sum to 1 ifthey exhaust the space ofallpossible attribute–value combinations,including the class attribute.This is cer-tainly not the case in our example.The solution is quite simple (we already encountered it in Section 4.2).To obtain the conditional probabilities Pr [play =no|E] and Pr [play =yes|E],normalize the joint probabilities by dividing them by their sum.This gives probability 0.245 for play =noand 0.755 forplay =yes.Just one mystery remains:why multiply all those probabilities together? Itturns out that the validity ofthe multiplication step hinges on a single assump-tion—namely that,given values for each ofa node’s parents,knowing the valuesfor any other ancestors does not change the probability associated with each ofits possible values.In other words,ancestors do not provide any informationabout the likelihood ofthe node’s values over and above the information pro-vided by the parents.This can be writtenwhich must hold for all values ofthe nodes and attributes involved.In statisticsthis property is called conditional independence.Multiplication is valid pro-vided that each node is conditionally independent ofits grandparents,great-grandparents,and so on,given its parents.The multiplication step resultsdirectly from the chain rule in probability theory,which states that the jointprobability ofnattributes aican be decomposed into this product:The decomposition holds for any order ofthe attributes.Because our Bayesiannetwork is an acyclic graph,its nodes can be ordered to give all ancestors ofanode aiindices smaller than i.Then,because ofthe conditional independenceassumption,PrPrPr parentsaaaaaaaasniiiniiin121111,,...,,...,’,[]=[]=[]-==’’PrPraaaaaaniiin12111,,...,,...,[]=[]-=’PrnodeancestorsPrnodeparents[]=[],P088407-Ch006.qxd  4/30/05  11:18 AM  Page 275which is exactly the multiplication rule that we applied previously.The two Bayesian networks in Figure 6.20 and Figure 6.21 are fundamentallydifferent.The ﬁrst (Figure 6.20) makes stronger independence assumptionsbecause for each ofits nodes the set ofparents is a subset ofthe correspondingset ofparents in the second (Figure 6.21).In fact,Figure 6.20 is almost identi-cal to the simple Naïve Bayes classiﬁer ofSection 4.2.(The probabilities areslightly different but only because each count has been initialized to 0.5 to avoidthe zero-frequency problem.) The network in Figure 6.21 has more rows in theconditional probability tables and hence more parameters;it may be a moreaccurate representation ofthe underlying domain.It is tempting to assume that the directed edges in a Bayesian network rep-resent causal effects.But be careful! In our case,a particular value ofplaymayenhance the prospects ofa particular value ofoutlook,but it certainly doesn’tcause it—it is more likely to be the other way round.Different Bayesian net-works can be constructed for the same problem,representing exactly the sameprobability distribution.This is done by altering the way in which the jointprobability distribution is factorized to exploit conditional independencies.Thenetwork whose directed edges model causal effects is often the simplest one withthe fewest parameters.Hence,human experts who construct Bayesian networksfor a particular domain often beneﬁt by representing causal effects by directededges.However,when machine learning techniques are applied to inducemodels from data whose causal structure is unknown,all they can do is con-struct a network based on the correlations that are observed in the data.Infer-ring causality from correlation is always a dangerous business.Learning Bayesian networksThe way to construct a learning algorithm for Bayesian networks is to deﬁnetwo components:a function for evaluating a given network based on the dataand a method for searching through the space ofpossible networks.The qualityofa given network is measured by the probability ofthe data given the network.We calculate the probability that the network accords to each instance and multiply these probabilities together over all instances.In practice,this quicklyyields numbers too small to be represented properly (called arithmetic underﬂow),so we use the sum ofthe logarithms ofthe probabilities rather than their product.The resulting quantity is the log-likelihood ofthe network given the data.Assume that the structure ofthe network—the set ofedges—is given.It’s easyto estimate the numbers in the conditional probability tables:just compute therelative frequencies ofthe associated combinations ofattribute values in thetraining data.To avoid the zero-frequency problem each count is initialized witha constant as described in Section 4.2.For example,to ﬁnd the probability thathumidity =normalgiven that play =yesand temperature =cool(the last number276CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2766.7BAYESIAN NETWORKS277ofthe third row ofthe humiditynode’s table in Figure 6.21),observe from Table1.2 (page 11) that there are three instances with this combination ofattributevalues in the weather data,and no instances with humidity =highand the same values for playand temperature.Initializing the counts for the two values ofhumidityto 0.5 yields the probability (3 +0.5) / (3 +0 +1) =0.875 for humidity =normal.The nodes in the network are predetermined,one for each attribute (includ-ing the class).Learning the network structure amounts to searching through thespace ofpossible sets ofedges,estimating the conditional probability tables foreach set,and computing the log-likelihood ofthe resulting network based onthe data as a measure ofthe network’s quality.Bayesian network learning algorithms differ mainly in the way in which they search through the space ofnetwork structures.Some algorithms are introduced below.There is one caveat.Ifthe log-likelihood is maximized based on the trainingdata,it will always be better to add more edges:the resulting network will simplyoverﬁt.Various methods can be employed to combat this problem.One possi-bility is to use cross-validation to estimate the goodness ofﬁt.A second is toadd a penalty for the complexity ofthe network based on the number ofparam-eters,that is,the total number ofindependent estimates in all the probabilitytables.For each table,the number ofindependent probabilities is the totalnumber ofentries minus the number ofentries in the last column,which canbe determined from the other columns because all rows must sum to 1.Let Kbe the number ofparameters,LLthe log-likelihood,and Nthe number ofinstances in the data.Two popular measures for evaluating the quality ofanetwork are the Akaike Information Criterion(AIC),and the following MDL metricbased on the MDL principle:In both cases the log-likelihood is negated,so the aim is to minimize thesescores.A third possibility is to assign a prior distribution over network structuresand ﬁnd the most likely network by combining its prior probability with theprobability accorded to the network by the data.This is the “Bayesian”approachto network scoring.Depending on the priordistribution used,it can takevarious forms.However,true Bayesians would average over all possible networkstructures rather than singling out a particular network for prediction.Unfor-tunately,this generally requires a great deal ofcomputation.A simpliﬁedapproach is to average over all network structures that are substructures ofaMDL score=-+LLKN2log.AIC score=-+LLK,P088407-Ch006.qxd  4/30/05  11:18 AM  Page 277given network.It turns out that this can be implemented very efﬁciently bychanging the method for calculating the conditional probability tables so thatthe resulting probability estimates implicitly contain information from all sub-networks.The details ofthis approach are rather complex and will not bedescribed here.The task ofsearching for a good network structure can be greatly simpliﬁedifthe right metric is used for scoring.Recall that the probability ofa singleinstance based on a network is the product ofall the individual probabilitiesfrom the various conditional probability tables.The overall probability ofthedataset is the product ofthese products for all instances.Because terms in aproduct are interchangeable,the product can be rewritten to group together allfactors relating to the same table.The same holds for the log-likelihood,usingsums instead ofproducts.This means that the likelihood can be optimized sep-arately for each node ofthe network.This can be done by adding,or removing,edges from other nodes to the node that is being optimized—the only constraintis that cycles must not be introduced.The same trick also works ifa local scoringmetric such as AIC or MDL is used instead ofplain log-likelihood because thepenalty term splits into several components,one for each node,and each nodecan be optimized independently.Speciﬁc algorithmsNow we move on to actual algorithms for learning Bayesian networks.Onesimple and very fast learning algorithm,called K2,starts with a given orderingofthe attributes (i.e.,nodes).Then it processes each node in turn and greedilyconsiders adding edges from previously processed nodes to the current one.Ineach step it adds the edge that maximizes the network’s score.When there is nofurther improvement,attention turns to the next node.As an additional mech-anism for overﬁtting avoidance,the number ofparents for each node can berestricted to a predeﬁned maximum.Because only edges from previously pro-cessed nodes are considered and there is a ﬁxed ordering,this procedure cannotintroduce cycles.However,the result depends on the initial ordering,so it makessense to run the algorithm several times with different random orderings.The Naïve Bayes classiﬁer is a network with an edge leading from the classattribute to each ofthe other attributes.When building networks for classiﬁca-tion,it sometimes helps to use this network as a starting point for the search.This can be done in K2 by forcing the class variable to be the ﬁrst one in theordering and initializing the set ofedges appropriately.Another potentially helpful trick is to ensure that every attribute in the datais in the Markov blanketofthe node that represents the class attribute.A node’sMarkov blanket includes all its parents,children,and children’s parents.It canbe shown that a node is conditionally independent ofall other nodes given278CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2786.7BAYESIAN NETWORKS279values for the nodes in its Markov blanket.Hence,ifa node is absent from theclass attribute’s Markov blanket,its value is completely irrelevant to the classi-ﬁcation.Conversely,ifK2 ﬁnds a network that does not include a relevant attrib-ute in the class node’s Markov blanket,it might help to add an edge that rectiﬁesthis shortcoming.A simple way ofdoing this is to add an edge from theattribute’s node to the class node or from the class node to the attribute’s node,depending on which option avoids a cycle.A more sophisticated but slower version ofK2 is not to order the nodes butto greedily consider adding or deleting edges between arbitrary pairs ofnodes(all the while ensuring acyclicity,ofcourse).A further step is to consider invert-ing the direction ofexisting edges as well.As with any greedy algorithm,theresulting network only represents a localmaximum ofthe scoring function:itis always advisable to run such algorithms several times with different randominitial conﬁgurations.More sophisticated optimization strategies such as simu-lated annealing,tabu search,or genetic algorithms can also be used.Another good learning algorithm for Bayesian network classiﬁers is called treeaugmented Naïve Bayes(TAN).As the name implies,it takes the Naïve Bayesclassiﬁer and adds edges to it.The class attribute is the single parent ofeachnode ofa Naïve Bayes network:TAN considers adding a second parent to eachnode.Ifthe class node and all corresponding edges are excluded from consid-eration,and assuming that there is exactly one node to which a second parentis not added,the resulting classiﬁer has a tree structure rooted at the parentlessnode—this is where the name comes from.For this restricted type ofnetworkthere is an efﬁcient algorithm for ﬁnding the set ofedges that maximizes thenetwork’s likelihood based on computing the network’s maximum weightedspanning tree.This algorithm is linear in the number ofinstances and quad-ratic in the number ofattributes.All the scoring metrics that we have described so far are likelihood based in the sense that they are designed to maximize the joint probability Pr[a1,a2,...,an] for each instance.However,in classiﬁcation,what we reallywant to maximize is the conditional probability ofthe class given the values ofthe other attributes—in other words,the conditional likelihood.Unfortunately,there is no closed-form solution for the maximum conditional-likelihood prob-ability estimates that are needed for the tables in a Bayesian network.On theother hand,computing the conditional likelihood for a given network anddataset is straightforward—after all,this is what logistic regression does.Henceit has been proposed to use standard maximum likelihood probability estimatesin the network,but the conditional likelihood to evaluate a particular networkstructure.Another way ofusing Bayesian networks for classiﬁcation is to build a sepa-rate network for each class value,based on the data pertaining to that class,andcombine the predictions using Bayes’s rule.The set ofnetworks is called aP088407-Ch006.qxd  4/30/05  11:18 AM  Page 279Bayesian multinet.To obtain a prediction for a particular class value,take thecorresponding network’s probability and multiply it by the class’s prior proba-bility.Do this for each class and normalize the result as we did previously.Inthis case we would not use the conditional likelihood to learn the network foreach class value.All the network learning algorithms introduced previously are score based.A different strategy,which we will not explain here,is to piece a networktogether by testing individual conditional independence assertions based onsubsets ofthe attributes.This is known as structure learning by conditional inde-pendence tests.Data structures for fast learningLearning Bayesian networks involves a lot ofcounting.For each network struc-ture considered in the search,the data must be scanned afresh to obtain thecounts needed to ﬁll out the conditional probability tables.Instead,could theybe stored in a data structure that eliminated the need for scanning the data overand over again? An obvious way is to precompute the counts and store thenonzero ones in a table—say,the hash table mentioned in Section 4.5.Even so,any nontrivial dataset will have a huge number ofnonzero counts.Again,consider the weather data from Table 1.2 (page 11).There are ﬁve attributes,two with three values and three with two values.This gives 4 ¥4 ¥3 ¥3 ¥3 =432 possible counts.Each component ofthe product corre-sponds to an attribute,and its contribution to the product is one more than thenumber ofits values because the attribute may be missing from the count.Allthese counts can be calculated by treating them as item sets,as explained inSection 4.5,and setting the minimum coverage to one.But even without storingcounts that are zero,this simple scheme runs into memory problems very quickly.It turns out that the counts can be stored effectively in a structure called anall-dimensions (AD) tree,which is analogous to the kD-trees used for nearest-neighbor search described in Section 4.7.For simplicity,we illustrate this usinga reduced version ofthe weather data that only has the attributes humidity,windy,and play.Figure 6.22(a) summarizes the data.The number ofpossiblecounts is 3 ¥3 ¥3 =27,although only 8 ofthem are shown.For example,thecount for play =nois 5 (count them!).Figure 6.22(b) shows an AD tree for this data.Each node says how manyinstances exhibit the attribute values that are tested along the path from the rootto that node.For example,the leftmost leafsays that there is one instance withvalues humidity =normal,windy =true,and play =no,and the rightmost leafsays that there are ﬁve instances with play =no.It would be trivial to construct a tree that enumerates all 27 counts explic-itly.However,that would gain nothing over a plain table and is obviously not280CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2806.7BAYESIAN NETWORKS281what the tree in Figure 6.22(b) does,because it contains only 8 counts.Thereis,for example,no branch that tests humidity =high.How was the tree con-structed,and how can all counts be obtained from it?Assume that each attribute in the data has been assigned an index.In thereduced version ofthe weather data we give humidityindex 1,windyindex 2,and playindex 3.An AD tree is generated by expanding each node correspon-ding to an attribute iwith the values ofall attributes that have indices j>i,withtwo important restrictions:the most populous expansion for each attribute isomitted (breaking ties arbitrarily) as are expansions with counts that are zero.The root node is given index 0,so for it all attributes are expanded,subject tothe same restrictions.humidityhighhighhighhighnormalnormalnormalnormaltruetruefalsefalsetruetruefalsefalseyesnoyesnoyesnoyesno1 2 2 2 2 1 4 0windyplaycount(a)any value14 instanceswindy = true6 instancesplay = no5 instanceshumidity = normal7 instanceswindy = true3 instancesplay = no1 instanceplay = no3 instancesplay = no1 instance(b)Figure 6.22The weather data:(a) reduced version and (b) corresponding AD tree.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 281For example,Figure 6.22(b) contains no expansion for windy =falsefrom theroot node because with eight instances it is the most populous expansion:the value falseoccurs more often in the data than the value true.Similarly,fromthe node labeled humidity =normalthere is no expansion for windy =falsebecause falseis the most common value for windyamong all instances withhumidity =normal.In fact,in our example the second restriction—namely,thatexpansions with zero counts are omitted—never kicks in because the ﬁrstrestriction precludes any path that starts with the tests humidity =normalandwindy =false,which is the only way to reach the solitary zero in Figure 6.22(a).Each node ofthe tree represents the occurrence ofa particular combina-tion ofattribute values.It is straightforward to retrieve the count for a combination that occurs in the tree.However,the tree does not explicitly repre-sent many nonzero counts because the most populous expansion for each attribute is omitted.For example,the combination humidity =highand play =yesoccurs three times in the data but has no node in the tree.Neverthe-less,it turns out that any count can be calculated from those that the tree storesexplicitly.Here’s a simple example.Figure 6.22(b) contains no node for humidity =normal,windy =true,and play =yes.However,it shows three instances withhumidity =normaland windy =true,and one ofthem has a value for playthatis different from yes.It follows that there must be two instances for play =yes.Now for a trickier case:how many times does humidity =high,windy =true,and play =nooccur? At ﬁrst glance it seems impossible to tell because there isno branch for humidity =high.However,we can deduce the number by calcu-lating the count for windy =trueand play =no(3) and subtracting the countfor humidity =normal,windy =true,and play =no(1).This gives 2,the correctvalue.This idea works for any subset ofattributes and any combination ofattrib-ute values,but it may have to be applied recursively.For example,to obtain thecount for humidity =high,windy =false,and play =no,we need the count forwindy =false and play =noand the count for humidity =normal,windy =false,and play =no.We obtain the former by subtracting the count for windy =trueand play =no(3) from the count for play =no(5),giving 2,and the latter bysubtracting the count for humidity =normal,windy =true,and play =no(1)from the count for humidity =normaland play =no(1),giving 0.Thus theremust be 2 -0 =2 instances with humidity =high,windy =false,and play =no,which is correct.AD trees only pay offifthe data contains many thousands ofinstances.It ispretty obvious that they do not help on the weather data.The fact that theyyield no beneﬁt on small datasets means that,in practice,it makes little senseto expand the tree all the way down to the leafnodes.Usually,a cutoffparam-eter kis employed,and nodes covering fewer than kinstances hold a list ofpoint-282CHAPTER 6|IMPLEMENTATIONS:REAL MACHINE LEARNING SCHEMESP088407-Ch006.qxd  4/30/05  11:18 AM  Page 2826.7BAYESIAN NETWORKS283ers to these instances rather than a list ofpointers to other nodes.This makesthe trees smaller and more efﬁcient to use.DiscussionThe K2 algorithm for learning Bayesian networks was introduced by Cooperand Herskovits (1992).Bayesian scoring metrics are covered by Heckerman etal.(1995).The TAN algorithm was introduced by Friedman et al.(1997),whoalso describes multinets.Grossman and Domingos (2004) show how to use theconditional likelihood for scoring networks.Guo and Greiner (2004) present anextensive comparison ofscoring metrics for Bayesian network classiﬁers.Bouck-aert (1995) describes averaging over subnetworks.AD trees were introduced andanalyzed by Moore and Lee (1998)—the same Andrew Moore whose work onkD-trees and ball trees was mentioned in Section 4.9.In a more recent paper,Komarek and Moore (2000) introduce AD trees for incremental learning thatare also more efﬁcient for datasets with many attributes.We have only skimmed the surface ofthe subject oflearning Bayesian net-works.We left open questions ofmissing values,numeric attributes,and hiddenattributes.We did not describe how to use Bayesian networks for regressiontasks.Bayesian networks are a special case ofa wider class ofstatistical modelscalled graphical models,which include networks with undirected edges (called Markov networks).Graphical models are attracting great attention in themachine learning community today.P088407-Ch006.qxd  4/30/05  11:18 AM  Page 283P088407-Ch006.qxd  4/30/05  11:18 AM  Page 284In the previous chapter we examined a vast array ofmachine learning methods:decision trees,decision rules,linear models,instance-based schemes,numeric pre-diction techniques,clustering algorithms,and Bayesian networks.All are sound,robust techniques that are eminently applicable to practical data mining problems.But successful data mining involves far more than selecting a learning algo-rithm and running it over your data.For one thing,many learning methodshave various parameters,and suitable values must be chosen for these.In mostcases,results can be improved markedly by suitable choice ofparameter values,and the appropriate choice depends on the data at hand.For example,decisiontrees can be pruned or unpruned,and in the former case a pruning parametermay have to be chosen.In the k-nearest-neighbor method ofinstance-basedlearning,a value for kwill have to be chosen.More generally,the learningscheme itselfwill have to be chosen from the range ofschemes that are avail-able.In all cases,the right choices depend on the data itself.It is tempting to try out several learning schemes,and several parametervalues,on your data and see which works best.But be careful! The best choicechapter7Transformations:Engineering the input and output285P088407-Ch007.qxd  4/30/05  11:16 AM  Page 285is not necessarily the one that performs best on the training data.We haverepeatedly cautioned about the problem ofoverﬁtting,where a learned modelis too closely tied to the particular training data from which it was built.It isincorrect to assume that performance on the training data faithfully representsthe level ofperformance that can be expected on the fresh data to which thelearned model will be applied in practice.Fortunately,we have already encountered the solution to this problem inChapter 5.There are two good methods for estimating the expected true per-formance ofa learning scheme:the use ofa large dataset that is quite separatefrom the training data,in the case ofplentiful data,and cross-validation(Section 5.3),ifdata is scarce.In the latter case,a single 10-fold cross-validation is typically used in practice,although to obtain a more reliable estimate the entire procedure should be repeated 10 times.Once suitable param-eters have been chosen for the learning scheme,use the whole training set—allthe available training instances—to produce the ﬁnal learned model that is tobe applied to fresh data.Note that the performance obtained with the chosen parameter value duringthe tuning process is nota reliable estimate ofthe ﬁnal model’s performance,because the ﬁnal model potentially overﬁts the data that was used for tuning.To ascertain how well it will perform,you need yet another large dataset that isquite separate from any data used during learning and tuning.The same is truefor cross-validation:you need an “inner”cross-validation for parameter tuningand an “outer”cross-validation for error estimation.With 10-fold cross-validation,this involves running the learning scheme 100 times.To summarize:when assessing the performance ofa learning scheme,any parameter tuningthat goes on should be treated as though it were an integral part ofthe train-ing process.There are other important processes that can materially improve successwhen applying machine learning techniques to practical data mining problems,and these are the subject ofthis chapter.They constitute a kind ofdata engi-neering:engineering the input data into a form suitable for the learning schemechosen and engineering the output model to make it more effective.You canlook on them as a bag oftricks that you can apply to practical data mining prob-lems to enhance the chance ofsuccess.Sometimes they work;other times theydon’t—and at the present state ofthe art,it’s hard to say in advance whetherthey will or not.In an area such as this where trial and error is the most reli-able guide,it is particularly important to be resourceful and understand whatthe tricks are.We begin by examining four different ways in which the input can be mas-saged to make it more amenable for learning methods:attribute selection,attribute discretization,data transformation,and data cleansing.Consider theﬁrst,attribute selection.In many practical situations there are far too many286CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 286attributes for learning schemes to handle,and some ofthem—perhaps the over-whelming majority—are clearly irrelevant or redundant.Consequently,the datamust be preprocessed to select a subset ofthe attributes to use in learning.Ofcourse,learning methods themselves try to select attributes appropriately andignore irrelevant or redundant ones,but in practice their performance can fre-quently be improved by preselection.For example,experiments show thatadding useless attributes causes the performance oflearning schemes such asdecision trees and rules,linear regression,instance-based learners,and cluster-ing methods to deteriorate.Discretization ofnumeric attributes is absolutely essential ifthe task involvesnumeric attributes but the chosen learning method can only handle categoricalones.Even methods that can handle numeric attributes often produce betterresults,or work faster,ifthe attributes are prediscretized.The converse situa-tion,in which categorical attributes must be represented numerically,alsooccurs (although less often);and we describe techniques for this case,too.Data transformation covers a variety oftechniques.One transformation,which we have encountered before when looking at relational data in Chapter2 and support vector machines in Chapter 6,is to add new,synthetic attributeswhose purpose is to present existing information in a form that is suitable forthe machine learning scheme to pick up on.More general techniques that donot depend so intimately on the semantics ofthe particular data mining pro-blem at hand include principal components analysis and random projections.Unclean data plagues data mining.We emphasized in Chapter 2 the neces-sity ofgetting to know your data:understanding the meaning ofall the differ-ent attributes,the conventions used in coding them,the signiﬁcance ofmissingvalues and duplicate data,measurement noise,typographical errors,and thepresence ofsystematic errors—even deliberate ones.Various simple visualiza-tions often help with this task.There are also automatic methods ofcleansingdata,ofdetecting outliers,and ofspotting anomalies,which we describe.Having studied how to massage the input,we turn to the question ofengi-neering the output from machine learning schemes.In particular,we examinetechniques for combining different models learned from the data.There aresome surprises in store.For example,it is often advantageous to take the train-ing data and derive several different training sets from it,learn a model fromeach,and combine the resulting models! Indeed,techniques for doing this canbe very powerful.It is,for example,possible to transform a relatively weak learning method into an extremely strong one (in a precise sense that we willexplain).Moreover,ifseveral learning schemes are available,it may be advan-tageous not to choose the best-performing one for your dataset (using cross-validation) but to use them all and combine the results.Finally,the standard,obvious way ofmodeling a multiclass learning situation as a two-class one canbe improved using a simple but subtle technique.7.1ATTRIBUTE SELECTION287P088407-Ch007.qxd  4/30/05  11:16 AM  Page 287Many ofthese results are counterintuitive,at least at ﬁrst blush.How can itbe a good idea to use many different models together? How can you possiblydo better than choose the model that performs best? Surely all this runs counterto Occam’s razor,which advocates simplicity.How can you possibly obtain ﬁrst-class performance by combining indifferent models,as one ofthese techniquesappears to do? But consider committees ofhumans,which often come up withwiser decisions than individual experts.Recall Epicurus’s view that,faced withalternative explanations,one should retain them all.Imagine a group ofspe-cialists each ofwhom excels in a limited domain even though none is competentacross the board.In struggling to understand how these methods work,researchers have exposed all sorts ofconnections and links that have led to evengreater improvements.Another extraordinary fact is that classiﬁcation performance can often beimproved by the addition ofa substantial amount ofdata that is unlabeled,inother words,the class values are unknown.Again,this seems to ﬂy directly inthe face ofcommon sense,rather like a river ﬂowing uphill or a perpetualmotion machine.But ifit were true—and it is,as we will show you in Section7.6—it would have great practical importance because there are many situationsin which labeled data is scarce but unlabeled data is plentiful.Read on—andprepare to be surprised.7.1Attribute selectionMost machine learning algorithms are designed to learn which are the mostappropriate attributes to use for making their decisions.For example,decision tree methods choose the most promising attribute to split on at each point and should—in theory—never select irrelevant or unhelpful attributes.Having more features should surely—in theory—result in more discriminating power,never less.“What’s the difference between theory and practice?”an old question asks.“There is no difference,”the answer goes,“—in theory.But in practice,there is.”Here there is,too:in practice,addingirrelevant or distracting attributes to a dataset often “confuses”machine learn-ing systems.Experiments with a decision tree learner (C4.5) have shown that adding tostandard datasets a random binary attribute generated by tossing an unbiasedcoin affects classiﬁcation performance,causing it to deteriorate (typically by 5%to 10% in the situations tested).This happens because at some point in the treesthat are learned the irrelevant attribute is invariably chosen to branch on,causing random errors when test data is processed.How can this be,when deci-sion tree learners are cleverly designed to choose the best attribute for splittingat each node? The reason is subtle.As you proceed further down the tree,less288CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 288and less data is available to help make the selection decision.At some point,with little data,the random attribute will look good just by chance.Because thenumber ofnodes at each level increases exponentially with depth,the chance ofthe rogue attribute looking good somewhere along the frontier multiplies up asthe tree deepens.The real problem is that you inevitably reach depths at whichonly a small amount ofdata is available for attribute selection.Ifthe datasetwere bigger it wouldn’t necessarily help—you’d probably just go deeper.Divide-and-conquer tree learners and separate-and-conquer rule learnersboth suffer from this effect because they inexorably reduce the amount ofdataon which they base judgments.Instance-based learners are very susceptible toirrelevant attributes because they always work in local neighborhoods,takingjust a few training instances into account for each decision.Indeed,it has beenshown that the number oftraining instances needed to produce a predeter-mined level ofperformance for instance-based learning increases exponentiallywith the number ofirrelevant attributes present.Naïve Bayes,by contrast,doesnot fragment the instance space and robustly ignores irrelevant attributes.Itassumes by design that all attributes are independent ofone another,an assump-tion that is just right for random “distracter”attributes.But through this verysame assumption,Naïve Bayes pays a heavy price in other ways because its oper-ation is damaged by adding redundant attributes.The fact that irrelevant distracters degrade the performance ofstate-of-the-art decision tree and rule learners is,at ﬁrst,surprising.Even more surprisingis that relevantattributes can also be harmful.For example,suppose that in atwo-class dataset a new attribute were added which had the same value as theclass to be predicted most ofthe time (65%) and the opposite value the rest ofthe time,randomly distributed among the instances.Experiments with standarddatasets have shown that this can cause classiﬁcation accuracy to deteriorate (by1% to 5% in the situations tested).The problem is that the new attribute is (nat-urally) chosen for splitting high up in the tree.This has the effect offragment-ing the set ofinstances available at the nodes below so that other choices arebased on sparser data.Because ofthe negative effect ofirrelevant attributes on most machine learn-ing schemes,it is common to precede learning with an attribute selection stagethat strives to eliminate all but the most relevant attributes.The best way toselect relevant attributes is manually,based on a deep understanding ofthelearning problem and what the attributes actually mean.However,automaticmethods can also be useful.Reducing the dimensionality ofthe data by delet-ing unsuitable attributes improves the performance oflearning algorithms.Italso speeds them up,although this may be outweighed by the computationinvolved in attribute selection.More importantly,dimensionality reductionyields a more compact,more easily interpretable representation ofthe targetconcept,focusing the user’s attention on the most relevant variables.7.1ATTRIBUTE SELECTION289P088407-Ch007.qxd  4/30/05  11:16 AM  Page 289Scheme-independent selectionWhen selecting a good attribute subset,there are two fundamentally differentapproaches.One is to make an independent assessment based on general char-acteristics ofthe data;the other is to evaluate the subset using the machinelearning algorithm that will ultimately be employed for learning.The ﬁrst iscalled the ﬁltermethod,because the attribute set is ﬁltered to produce the most promising subset before learning commences.The second is the wrappermethod,because the learning algorithm is wrapped into the selection proce-dure.Making an independent assessment ofan attribute subset would be easyifthere were a good way ofdetermining when an attribute was relevant tochoosing the class.However,there is no universally accepted measure of“rele-vance,”although several different ones have been proposed.One simple scheme-independent method ofattribute selection is to use justenough attributes to divide up the instance space in a way that separates all thetraining instances.For example,ifjust one or two attributes are used,there willgenerally be several instances that have the same combination ofattributevalues.At the other extreme,the full set ofattributes will likely distinguish theinstances uniquely so that no two instances have the same values for all attrib-utes.(This will not necessarily be the case,however;datasets sometimes containinstances with the same attribute values but different classes.) It makes intuitivesense to select the smallest attribute subset that distinguishes all instancesuniquely.This can easily be found using exhaustive search,although at consid-erable computational expense.Unfortunately,this strong bias toward consis-tency ofthe attribute set on the training data is statistically unwarranted andcan lead to overﬁtting—the algorithm may go to unnecessary lengths to repairan inconsistency that was in fact merely caused by noise.Machine learning algorithms can be used for attribute selection.For instance,you might ﬁrst apply a decision tree algorithm to the full dataset,and then selectonly those attributes that are actually used in the tree.Although this selectionwould have no effect at all ifthe second stage merely built another tree,it willhave an effect on a different learning algorithm.For example,the nearest-neighbor algorithm is notoriously susceptible to irrelevant attributes,and itsperformance can be improved by using a decision tree builder as a ﬁlter forattribute selection ﬁrst.The resulting nearest-neighbor method can alsoperform better than the decision tree algorithm used for ﬁltering.As anotherexample,the simple 1R scheme described in Chapter 4 has been used to selectthe attributes for a decision tree learner by evaluating the effect ofbranchingon different attributes (although an error-based method such as 1R may not bethe optimal choice for ranking attributes,as we will see later when covering therelated problem ofsupervised discretization).Often the decision tree performsjust as well when only the two or three top attributes are used for its construc-290CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 290tion—and it is much easier to understand.In this approach,the user determineshow many attributes to use for building the decision tree.Another possibility is to use an algorithm that builds a linear model—forexample,a linear support vector machine—and ranks the attributes based on the size ofthe coefﬁcients.A more sophisticated variant applies the learningalgorithm repeatedly.It builds a model,ranks the attributes based on the coefﬁcients,removes the highest-ranked one,and repeats the process until all attributes have been removed.This method ofrecursive feature eliminationhas been found to yield better results on certain datasets (e.g.,when iden-tifying important genes for cancer classiﬁcation) than simply ranking attrib-utes based on a single model.With both methods it is important to ensure that the attributes are measured on the same scale;otherwise,the coefﬁcientsare not comparable.Note that these techniques just produce a ranking;another method must be used to determine the appropriate number ofattrib-utes to use.Attributes can be selected using instance-based learning methods,too.Youcould sample instances randomly from the training set and check neighboringrecords ofthe same and different classes—“near hits”and “near misses.”Ifanear hit has a different value for a certain attribute,that attribute appears to beirrelevant and its weight should be decreased.On the other hand,ifa near misshas a different value,the attribute appears to be relevant and its weight shouldbe increased.Ofcourse,this is the standard kind ofprocedure used for attrib-ute weighting for instance-based learning,described in Section 6.4.After repeat-ing this operation many times,selection takes place:only attributes with positiveweights are chosen.As in the standard incremental formulation ofinstance-based learning,different results will be obtained each time the process isrepeated,because ofthe different ordering ofexamples.This can be avoided byusing all training instances and taking into account all near hits and near missesofeach.A more serious disadvantage is that the method will not detect an attributethat is redundant because it is correlated with another attribute.In the extremecase,two identical attributes would be treated in the same way,either bothselected or both rejected.A modiﬁcation has been suggested that appears to gosome way towards addressing this issue by taking the current attribute weightsinto account when computing the nearest hits and misses.Another way ofeliminating redundant attributes as well as irrelevant ones isto select a subset ofattributes that individually correlate well with the class buthave little intercorrelation.The correlation between two nominal attributes Aand Bcan be measured using the symmetric uncertainty:UABHAHBHABHAHB,,,()=()+()-()()+()27.1ATTRIBUTE SELECTION291P088407-Ch007.qxd  4/30/05  11:16 AM  Page 291where His the entropy function described in Section 4.3.The entropies arebased on the probability associated with each attribute value;H(A,B),the jointentropy ofAand B,is calculated from the joint probabilities ofall combina-tions ofvalues ofAand B.The symmetric uncertainty always lies between 0 and 1.Correlation-based feature selection determines the goodness ofa set ofattributes usingwhere C is the class attribute and the indices i and j range over all attributes inthe set.Ifall mattributes in the subset correlate perfectly with the class and withone another,the numerator becomes mand the denominator becomes ,which is also m.Hence,the measure is 1,which turns out to be the maximumvalue it can attain (the minimum is 0).Clearly this is not ideal,because we wantto avoid redundant attributes.However,any subset ofthis set will also have value1.When using this criterion to search for a good subset ofattributes it makessense to break ties in favor ofthe smallest subset.Searching the attribute spaceMost methods for attribute selection involve searching the space ofattributesfor the subset that is most likely to predict the class best.Figure 7.1 illustratesthe attribute space for the—by now all-too-familiar—weather dataset.Thenumber ofpossible attribute subsets increases exponentially with the numberofattributes,making exhaustive search impractical on all but the simplest problems.Typically,the space is searched greedily in one oftwo directions,top tobottom or bottom to top in the ﬁgure.At each stage,a local change is made tothe current attribute subset by either adding or deleting a single attribute.Thedownward direction,where you start with no attributes and add them one at atime,is called forward selection.The upward one,where you start with the fullset and delete attributes one at a time,is backward elimination.In forward selection,each attribute that is not already in the current subsetis tentatively added to it and the resulting set ofattributes is evaluated—using,for example,cross-validation as described in the following section.This evalu-ation produces a numeric measure ofthe expected performance ofthe subset.The effect ofadding each attribute in turn is quantiﬁed by this measure,the bestone is chosen,and the procedure continues.However,ifno attribute producesan improvement when added to the current subset,the search ends.This is astandard greedy search procedure and guarantees to ﬁnd a locally—but not nec-essarily globally—optimal set ofattributes.Backward elimination operates inan entirely analogous fashion.In both cases a slight bias is often introducedm2UACUAAjjijji,,,()()ÂÂÂ292CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 292toward smaller attribute sets.This can be done for forward selection by insist-ing that ifthe search is to continue,the evaluation measure must not onlyincrease but also must increase by at least a small predetermined quantity.Asimilar modiﬁcation works for backward elimination.More sophisticated search methods exist.Forward selection and backwardelimination can be combined into a bidirectional search;again one can eitherbegin with all the attributes or with none ofthem.Best-ﬁrst search is a methodthat does not just terminate when the performance starts to drop but keeps alist ofall attribute subsets evaluated so far,sorted in order ofthe performancemeasure,so that it can revisit an earlier conﬁguration instead.Given enoughtime it will explore the entire space,unless this is prevented by some kind ofstopping criterion.Beam search is similar but truncates its list ofattributesubsets at each stage so that it only contains a ﬁxed number—the beam width—7.1ATTRIBUTE SELECTION293outlooktemperaturetemperaturehumiditytemperaturewindyoutlooktemperaturehumidityoutlooktemperaturehumidityoutlookhumiditywindyhumiditywindytemperaturehumiditywindyoutlookwindyoutlookhumidityoutlookhumiditywindytemperatureoutlooktemperaturehumiditywindyFigure 7.1Attribute space for the weather dataset.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 293ofmost promising candidates.Genetic algorithm search procedures are looselybased on the principal ofnatural selection:they “evolve”good feature subsetsby using random perturbations ofa current list ofcandidate subsets.Scheme-speciﬁc selectionThe performance ofan attribute subset with scheme-speciﬁc selection is meas-ured in terms ofthe learning scheme’s classiﬁcation performance using justthose attributes.Given a subset ofattributes,accuracy is estimated using thenormal procedure ofcross-validation described in Section 5.3.Ofcourse,otherevaluation methods such as performance on a holdout set (Section 5.3) or thebootstrap estimator (Section 5.4) could equally well be used.The entire attribute selection process is computation intensive.Ifeach eval-uation involves a 10-fold cross-validation,the learning procedure must be exe-cuted 10 times.With kattributes,the heuristic forward selection or backwardelimination multiplies evaluation time by a factor ofup to k2—and for moresophisticated searches,the penalty will be far greater,up to 2kfor an exhaustivealgorithm that examines each ofthe 2kpossible subsets.Good results have been demonstrated on many datasets.In general terms,backward elimination produces larger attribute sets,and better classiﬁcationaccuracy,than forward selection.The reason is that the performance measureis only an estimate,and a single optimistic estimate will cause both ofthesesearch procedures to halt prematurely—backward elimination with too manyattributes and forward selection with not enough.But forward selection is usefulifthe focus is on understanding the decision structures involved,because it oftenreduces the number ofattributes with only a very small effect on classiﬁcationaccuracy.Experience seems to show that more sophisticated search techniquesare not generally justiﬁed—although they can produce much better results incertain cases.One way to accelerate the search process is to stop evaluating a subset ofattributes as soon as it becomes apparent that it is unlikely to lead to higheraccuracy than another candidate subset.This is a job for a paired statistical sig-niﬁcance test,performed between the classiﬁer based on this subset and all theother candidate classiﬁers based on other subsets.The performance differencebetween two classiﬁers on a particular test instance can be taken to be -1,0,or1 depending on whether the ﬁrst classiﬁer is worse,the same as,or better thanthe second on that instance.A paired t-test (described in Section 5.5) can beapplied to these ﬁgures over the entire test set,effectively treating the results foreach instance as an independent estimate ofthe difference in performance.Thenthe cross-validation for a classiﬁer can be prematurely terminated as soon as itturns out to be signiﬁcantly worse than another—which,ofcourse,may neverhappen.We might want to discard classiﬁers more aggressively by modifying294CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 294the t-test to compute the probability that one classiﬁer is better than anotherclassiﬁer by at least a small user-speciﬁed threshold.Ifthis probability becomesvery small,we can discard the former classiﬁer on the basis that it is very unlikelyto perform substantially better than the latter.This methodology is called race searchand can be implemented with differ-ent underlying search strategies.When used with forward selection,we race allpossible single-attribute additions simultaneously and drop those that do notperform well enough.In backward elimination,we race all single-attribute dele-tions.Schemata searchis a more complicated method speciﬁcally designed forracing;it runs an iterative series ofraces that each determine whether or not aparticular attribute should be included.The other attributes for this race areincluded or excluded randomly at each point in the evaluation.As soon as onerace has a clear winner,the next iteration ofraces begins,using the winner asthe starting point.Another search strategy is to rank the attributes ﬁrst,using,for example,their information gain (assuming they are discrete),and then racethe ranking.In this case the race includes no attributes,the top-ranked attrib-ute,the top two attributes,the top three,and so on.Whatever way you do it,scheme-speciﬁc attribute selection by no meansyields a uniform improvement in performance.Because ofthe complexity ofthe process,which is greatly increased by the feedback effect ofincluding a targetmachine learning algorithm in the attribution selection loop,it is quite hard topredict the conditions under which it will turn out to be worthwhile.As in manymachine learning situations,trial and error using your own particular source ofdata is the ﬁnal arbiter.There is one type ofclassiﬁer for which scheme-speciﬁc attribute selection isan essential part ofthe learning process:the decision table.As mentioned inSection 3.1,the entire problem oflearning decision tables consists ofselectingthe right attributes to include.Usually this is done by measuring the table’scross-validation performance for different subsets ofattributes and choosingthe best-performing subset.Fortunately,leave-one-out cross-validation is verycheap for this kind ofclassiﬁer.Obtaining the cross-validation error from a deci-sion table derived from the training data is just a matter ofmanipulating theclass counts associated with each ofthe table’s entries,because the table’s struc-ture doesn’t change when instances are added or deleted.The attribute space isgenerally searched by best-ﬁrst search because this strategy is less likely tobecome stuck in a local maximum than others,such as forward selection.Let’s end our discussion with a success story.One learning method for whicha simple scheme-speciﬁc attribute selection approach has shown good results isNaïve Bayes.Although this method deals well with random attributes,it has thepotential to be misled when there are dependencies among attributes,and par-ticularly when redundant ones are added.However,good results have beenreported using the forward selection algorithm—which is better able to detect7.1ATTRIBUTE SELECTION295P088407-Ch007.qxd  4/30/05  11:16 AM  Page 295when a redundant attribute is about to be added than the backward elimina-tion approach—in conjunction with a very simple,almost “naïve,”metric thatdetermines the quality ofan attribute subset to be simply the performance ofthe learned algorithm on the trainingset.As was emphasized in Chapter 5,train-ing set performance is certainly not a reliable indicator oftest-set performance.Nevertheless,experiments show that this simple modiﬁcation to Naïve Bayesmarkedly improves its performance on those standard datasets for which it doesnot do so well as tree- or rule-based classiﬁers,and does not have any negativeeffect on results on datasets on which Naïve Bayes already does well.SelectiveNaïve Bayes,as this learning method is called,is a viable machine learning tech-nique that performs reliably and well in practice.7.2Discretizing numeric attributesSome classiﬁcation and clustering algorithms deal with nominal attributes onlyand cannot handle ones measured on a numeric scale.To use them on generaldatasets,numeric attributes must ﬁrst be “discretized”into a small number ofdistinct ranges.Even learning algorithms that do handle numeric attributessometimes process them in ways that are not altogether satisfactory.Statisticalclustering methods often assume that numeric attributes have a normal distri-bution—often not a very plausible assumption in practice—and the standardextension ofthe Naïve Bayes classiﬁer to handle numeric attributes adopts thesame assumption.Although most decision tree and decision rule learners canhandle numeric attributes,some implementations work much more slowlywhen numeric attributes are present because they repeatedly sort the attributevalues.For all these reasons the question arises:what is a good way to discretizenumeric attributes into ranges before any learning takes place?We have already encountered some methods for discretizing numeric attrib-utes.The 1R learning scheme described in Chapter 4 uses a simple but effectivetechnique:sort the instances by the attribute’s value and assign the value intoranges at the points that the class value changes—except that a certain minimumnumber ofinstances in the majority class (six) must lie in each ofthe ranges,which means that any given range may include a mixture ofclass values.Thisis a “global”method ofdiscretization that is applied to all continuous attributesbefore learning starts.Decision tree learners,on the other hand,deal with numeric attributes on alocal basis,examining attributes at each node ofthe tree when it is being con-structed to see whether they are worth branching on—and only at that pointdeciding on the best place to split continuous attributes.Although the tree-building method we examined in Chapter 6 only considers binary splits ofcon-tinuous attributes,one can imagine a full discretization taking place at that296CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 2967.2DISCRETIZING NUMERIC ATTRIBUTES297point,yielding a multiway split on a numeric attribute.The pros and cons ofthe local versus the global approach are clear.Local discretization is tailored tothe actual context provided by each tree node,and will produce different dis-cretizations ofthe same attribute at different places in the tree ifthat seemsappropriate.However,its decisions are based on less data as tree depth increases,which compromises their reliability.Iftrees are developed all the way out tosingle-instance leaves before being pruned back,as with the normal techniqueofbackward pruning,it is clear that many discretization decisions will be basedon data that is grossly inadequate.When using global discretization before applying a learning method,thereare two possible ways ofpresenting the discretized data to the learner.The mostobvious is to treat discretized attributes like nominal ones:each discretizationinterval is represented by one value ofthe nominal attribute.However,becausea discretized attribute is derived from a numeric one,its values are ordered,andtreating it as nominal discards this potentially valuable ordering information.Ofcourse,ifa learning scheme can handle ordered attributes directly,the solu-tion is obvious:each discretized attribute is declared to be oftype “ordered.”Ifthe learning method cannot handle ordered attributes,there is still a simpleway ofenabling it to exploit the ordering information:transform each dis-cretized attribute into a set ofbinary attributes before the learning scheme isapplied.Assuming the discretized attribute has kvalues,it is transformed intok-1 binary attributes,the ﬁrst i-1 ofwhich are set to falsewhenever the ithvalue ofthe discretized attribute is present in the data and to trueotherwise.The remaining attributes are set to false.In other words,the (i-1)th binaryattribute represents whether the discretized attribute is less than i.Ifa decisiontree learner splits on this attribute,it implicitly uses the ordering informationit encodes.Note that this transformation is independent ofthe particular dis-cretization method being applied:it is simply a way ofcoding an ordered attrib-ute using a set ofbinary attributes.Unsupervised discretizationThere are two basic approaches to the problem ofdiscretization.One is to quan-tize each attribute in the absence ofany knowledge ofthe classes ofthe instancesin the training set—so-called unsupervised discretization.The other is to takethe classes into account when discretizing—supervised discretization.Theformer is the only possibility when dealing with clustering problems in whichthe classes are unknown or nonexistent.The obvious way ofdiscretizing a numeric attribute is to divide its range intoa predetermined number ofequal intervals:a ﬁxed,data-independent yardstick.This is frequently done at the time when data is collected.But,like any unsu-pervised discretization method,it runs the risk ofdestroying distinctions thatP088407-Ch007.qxd  4/30/05  11:16 AM  Page 297would have turned out to be useful in the learning process by using gradationsthat are too coarse or by unfortunate choices ofboundary that needlessly lumptogether many instances ofdifferent classes.Equal-interval binning often distributes instances very unevenly:some binscontain many instances,and others contain none.This can seriously impair theability ofthe attribute to help to build good decision structures.It is often betterto allow the intervals to be ofdifferent sizes,choosing them so that the samenumber oftraining examples fall into each one.This method,equal-frequencybinning,divides the attribute’s range into a predetermined number ofbins basedon the distribution ofexamples along that axis—sometimes called histogramequalization,because ifyou take a histogram ofthe contents ofthe resultingbins it will be completely ﬂat.Ifyou view the number ofbins as a resource,thismethod makes best use ofit.However,equal-frequency binning is still oblivious to the instances’classes,and this can cause bad boundaries.For example,ifall instances in a bin haveone class,and all instances in the next higher bin have another except for theﬁrst,which has the original class,surely it makes sense to respect the class divisions and include that ﬁrst instance in the previous bin,sacriﬁcing the equal-frequency property for the sake ofhomogeneity.Supervised discretization—taking classes into account during the process—certainly has advantages.Nevertheless,it has been found that equal-frequency binning can yield excellentresults,at least in conjunction with the Naïve Bayes learning scheme,when thenumber ofbins is chosen in a data-dependent fashion by setting it to the squareroot ofthe number ofinstances.This method is called proportional k-intervaldiscretization.Entropy-based discretizationBecause the criterion used for splitting a numeric attribute during the forma-tion ofa decision tree works well in practice,it seems a good idea to extend itto more general discretization by recursively splitting intervals until it is timeto stop.In Chapter 6 we saw how to sort the instances by the attribute’s valueand consider,for each possible splitting point,the information gain oftheresulting split.To discretize the attribute,once the ﬁrst split is determined thesplitting process can be repeated in the upper and lower parts ofthe range,andso on,recursively.To see this working in practice,we revisit the example on page 189 for dis-cretizing the temperature attribute ofthe weather data,whose values are646568697071727580818385noyesyesnoyesyesyesnoyesyesnoyesyesno298CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 2987.2DISCRETIZING NUMERIC ATTRIBUTES299(Repeated values have been collapsed together.) The information gain for eachofthe 11 possible positions for the breakpoint is calculated in the usual way.For example,the information value ofthe test temperature<71.5,which splitsthe range into fouryes’s and two no’s versus ﬁveyes’s and three no’s,isThis represents the amount ofinformation required to specify the individualvalues ofyesand nogiven the split.We seek a discretization that makes thesubintervals as pure as possible;hence,we choose to split at the point where theinformation value is smallest.(This is the same as splitting where the informa-tion gain,deﬁned as the difference between the information value without thesplit and that with the split,is largest.) As before,we place numeric thresholdshalfway between the values that delimit the boundaries ofa concept.The graph labeled A in Figure 7.2 shows the information values at each pos-sible cut point at this ﬁrst stage.The cleanest division—smallest informationvalue—is at a temperature of84 (0.827 bits),which separates offjust the veryﬁnal value,a noinstance,from the preceding list.The instance classes are writtenbelow the horizontal axis to make interpretation easier.Invoking the algorithmagain on the lower range oftemperatures,from 64 to 83,yields the graph labeledB.This has a minimum at 80.5 (0.800 bits),which splits offthe next two values,info4,2info4,2info5,3bits[][]()=()¥[]()+()¥[]()=,,.5361481409390.40.20.600.816570758085ABCDEFyesnoyesyesyesnonoyesyesyesnoyesyesnoFigure 7.2Discretizing the temperatureattribute using the entropy method.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 299both yesinstances.Again invoking the algorithm on the lower range,now from64 to 80,produces the graph labeled C (shown dotted to help distinguish it fromthe others).The minimum is at 77.5 (0.801 bits),splitting offanother noinstance.Graph D has a minimum at 73.5 (0.764 bits),splitting offtwo yesinstances.Graph E (again dashed,purely to make it more easily visible),for thetemperature range 64 to 72,has a minimum at 70.5 (0.796 bits),which splitsofftwo nos and a yes.Finally,graph F,for the range 64 to 70,has a minimumat 66.5 (0.4 bits).The ﬁnal discretization ofthe temperatureattribute is shown in Figure 7.3.The fact that recursion only ever occurs in the ﬁrst interval ofeach split is anartifact ofthis example:in general,both the upper and the lower intervals willhave to be split further.Underneath each division is the label ofthe graph inFigure 7.2 that is responsible for it,and below that is the actual value ofthe splitpoint.It can be shown theoretically that a cut point that minimizes the informa-tion value will never occur between two instances ofthe same class.This leadsto a useful optimization:it is only necessary to consider potential divisions thatseparate instances ofdifferent classes.Notice that ifclass labels were assigned tothe intervals based on the majority class in the interval,there would be no guar-antee that adjacent intervals would receive different labels.You might betempted to consider merging intervals with the same majority class (e.g.,theﬁrst two intervals ofFigure 7.3),but as we will see later (pages 302–304) this isnot a good thing to do in general.The only problem left to consider is the stopping criterion.In the tempera-ture example most ofthe intervals that were identiﬁed were “pure”in that alltheir instances had the same class,and there is clearly no point in trying to splitsuch an interval.(Exceptions were the ﬁnal interval,which we tacitly decidednot to split,and the interval from 70.5 to 73.5.) In general,however,things arenot so straightforward.300CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUT646568697071727580818385yesnoyesyesyesnonoyesyesyesnoyesyesnoFEDCBA66.570.573.577.580.584Figure 7.3The result ofdiscretizing the temperatureattribute.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3007.2DISCRETIZING NUMERIC ATTRIBUTES301A good way to stop the entropy-based splitting discretization procedure turnsout to be the MDL principle that we encountered in Chapter 5.In accordancewith that principle,we want to minimize the size ofthe “theory”plus the sizeofthe information necessary to specify all the data given that theory.In thiscase,ifwe do split,the “theory”is the splitting point,and we are comparing thesituation in which we split with that in which we do not.In both cases we assumethat the instances are known but their class labels are not.Ifwe do not split,theclasses can be transmitted by encoding each instance’s label.Ifwe do,we ﬁrstencode the split point (in log2[N-1] bits,where Nis the number ofinstances),then the classes ofthe instances below that point,and then the classes ofthoseabove it.You can imagine that ifthe split is a good one—say,all the classes belowit are yesand all those above are no—then there is much to be gained by split-ting.Ifthere is an equal number ofyesand noinstances,each instance costs 1bit without splitting but hardly more than 0 bits with splitting—it is not quite0 because the class values associated with the split itselfmust be encoded,butthis penalty is amortized across all the instances.In this case,ifthere are manyexamples,the penalty ofhaving to encode the split point will be far outweighedby the information saved by splitting.We emphasized in Section 5.9 that when applying the MDL principle,thedevil is in the details.In the relatively straightforward case ofdiscretization,thesituation is tractable although not simple.The amounts ofinformation can beobtained exactly under certain reasonable assumptions.We will not go into thedetails,but the upshot is that the split dictated by a particular cut point is worth-while ifthe information gain for that split exceeds a certain value that dependson the number ofinstances N,the number ofclasses k,the entropy oftheinstances E,the entropy ofthe instances in each subinterval E1and E2,and thenumber ofclasses represented in each subinterval k1and k2:The ﬁrst component is the information needed to specify the splitting point;the second is a correction due to the need to transmit which classes correspondto the upper and lower subintervals.When applied to the temperature example,this criterion prevents any split-ting at all.The ﬁrst split removes just the ﬁnal example,and as you can imaginevery little actual information is gained by this when transmitting the classes—in fact,the MDL criterion will never create an interval containing just oneexample.Failure to discretize temperatureeffectively disbars it from playing anyrole in the ﬁnal decision structure because the same discretized value will begiven to all instances.In this situation,this is perfectly appropriate:the temper-gainNNkEkEkENk>-()+-()-++loglog.221122132P088407-Ch007.qxd  4/30/05  11:16 AM  Page 301atureattribute does not occur in good decision trees or rules for the weatherdata.In effect,failure to discretize is tantamount to attribute selection.Other discretization methodsThe entropy-based method with the MDL stopping criterion is one ofthe bestgeneral techniques for supervised discretization.However,many other methodshave been investigated.For example,instead ofproceeding top-down by recur-sively splitting intervals until some stopping criterion is satisﬁed,you couldwork bottom-up,ﬁrst placing each instance into its own interval and then con-sidering whether to merge adjacent intervals.You could apply a statistical crite-rion to see which would be the best two intervals to merge,and merge them ifthe statistic exceeds a certain preset conﬁdence level,repeating the operationuntil no potential merge passes the test.The c2test is a suitable one and hasbeen used for this purpose.Instead ofspecifying a preset signiﬁcance threshold,more complex techniques are available to determine an appropriate level automatically.A rather different approach is to count the number oferrors that a dis-cretization makes when predicting each training instance’s class,assuming thateach interval receives the majority class.For example,the 1R method describedearlier is error based—it focuses on errors rather than the entropy.However,the best possible discretization in terms oferror count is obtained by using thelargest possible number ofintervals,and this degenerate case should be avoidedby restricting the number ofintervals in advance.For example,you might ask,what is the best way to discretize an attribute into kintervals in a way that min-imizes the number oferrors?The brute-force method ofﬁnding the best way ofpartitioning an attributeinto kintervals in a way that minimizes the error count is exponential in kandhence infeasible.However,there are much more efﬁcient schemes that are basedon the idea ofdynamic programming.Dynamic programming applies not justto the error count measure but also to any given additive impurity function,andit can ﬁnd the partitioning ofNinstances into kintervals in a way that mini-mizes the impurity in time proportional to kN2.This gives a way ofﬁnding thebest entropy-based discretization,yielding a potential improvement in thequality ofthe discretization (but in practice a negligible one) over the recursiveentropy-based method described previously.The news for error-based dis-cretization is even better,because there is a method that minimizes the errorcount in time linear in N.Entropy-based versus error-based discretizationWhy not use error-based discretization,since the optimal discretization can befound very quickly? The answer is that there is a serious drawback to error-based302CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3027.2DISCRETIZING NUMERIC ATTRIBUTES303discretization:it cannot produce adjacent intervals with the same label (such asthe ﬁrst two ofFigure 7.3).The reason is that merging two such intervals willnot affect the error count but it will free up an interval that can be used else-where to reduce the error count.Why would anyone want to generate adjacent intervals with the same label?The reason is best illustrated with an example.Figure 7.4 shows the instancespace for a simple two-class problem with two numeric attributes ranging from0 to 1.Instances belong to one class (the dots) iftheir ﬁrst attribute (a1) is lessthan 0.3 or ifit is less than 0.7 andtheir second attribute (a2) is less than 0.5.Otherwise,they belong to the other class (triangles).The data in Figure 7.4 hasbeen artiﬁcially generated according to this rule.Now suppose we are trying to discretize both attributes with a view to learn-ing the classes from the discretized attributes.The very best discretization splitsa1 into three intervals (0 through 0.3,0.3 through 0.7,and 0.7 through 1.0) anda2 into two intervals (0 through 0.5 and 0.5 through 1.0).Given these nominal00.20.40.60.81a100.20.40.60.81a2Figure 7.4Class distribution for a two-class,two-attribute problem.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 303attributes,it will be easy to learn how to tell the classes apart with a simple deci-sion tree or rule algorithm.Discretizing a2 is no problem.For a1,however,theﬁrst and last intervals will have opposite labels (dotand triangle,respectively).The second will have whichever label happens to occur most in the region from0.3 through 0.7 (it is in fact dotfor the data in Figure 7.4).Either way,this labelmust inevitably be the same as one ofthe adjacent labels—ofcourse this is truewhatever the class probability happens to be in the middle region.Thus this dis-cretization will not be achieved by any method that minimizes the error counts,because such a method cannot produce adjacent intervals with the same label.The point is that what changes as the value ofa1 crosses the boundary at 0.3is not the majority class but the class distribution.The majority class remainsdot.The distribution,however,changes markedly,from 100% before the bound-ary to just over 50% after it.And the distribution changes again as the bound-ary at 0.7 is crossed,from 50% to 0%.Entropy-based discretization methodsare sensitive to changes in the distribution even though the majority class doesnot change.Error-based methods are not.Converting discrete to numeric attributesThere is a converse problem to discretization.Some learning algorithms—notably the nearest-neighbor instance-based method and numeric predictiontechniques involving regression—naturally handle only attributes that arenumeric.How can they be extended to nominal attributes?In instance-based learning,as described in Section 4.7,discrete attributes canbe treated as numeric by deﬁning the “distance”between two nominal valuesthat are the same as 0 and between two values that are different as 1—regard-less ofthe actual values involved.Rather than modifying the distance function,this can be achieved using an attribute transformation:replace a k-valuednominal attribute with ksynthetic binary attributes,one for each value indi-cating whether the attribute has that value or not.Ifthe attributes have equalweight,this achieves the same effect on the distance function.The distance isinsensitive to the attribute values because only “same”or “different”informa-tion is encoded,not the shades ofdifference that may be associated with thevarious possible values ofthe attribute.More subtle distinctions can be made ifthe attributes have weights reﬂecting their relative importance.Ifthe values ofthe attribute can be ordered,more possibilities arise.For anumeric prediction problem,the average class value corresponding to eachvalue ofa nominal attribute can be calculated from the training instances andused to determine an ordering—this technique was introduced for model trees in Section 6.5.(It is hard to come up with an analogous way oforderingattribute values for a classiﬁcation problem.) An ordered nominal attribute can be replaced with an integer in the obvious way—but this implies not just304CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3047.3SOME USEFUL TRANSFORMATIONS305an ordering but also a metric on the attribute’s values.The implication ofa metric can be avoided by creating k-1 synthetic binary attributes for a k-valued nominal attribute,in the manner described on page 297.This encod-ing still implies an ordering among different values ofthe attribute—adjacentvalues differ in just one ofthe synthetic attributes,whereas distant ones differ in several—but it does not imply an equal distance between the attributevalues.7.3Some useful transformationsResourceful data miners have a toolbox full oftechniques,such as discretiza-tion,for transforming data.As we emphasized in Section 2.4,data mining ishardly ever a matter ofsimply taking a dataset and applying a learning algo-rithm to it.Every problem is different.You need to think about the data andwhat it means,and examine it from diverse points ofview—creatively!—toarrive at a suitable perspective.Transforming it in different ways can help youget started.You don’t have to make your own toolbox by implementing the techniquesyourself.Comprehensive environments for data mining,such as the onedescribed in Part II ofthis book,contain a wide range ofsuitable tools for youto use.You do not necessarily need a detailed understanding ofhow they areimplemented.What you do need is to understand what the tools do and howthey can be applied.In Part II we list,and brieﬂy describe,all the transforma-tions in the Weka data mining workbench.Data often calls for general mathematical transformations ofa set ofattrib-utes.It might be useful to deﬁne new attributes by applying speciﬁed mathe-matical functions to existing ones.Two dateattributes might be subtracted togive a third attribute representing age—an example ofa semantic transforma-tion driven by the meaning ofthe original attributes.Other transformationsmight be suggested by known properties ofthe learning algorithm.Ifa linearrelationship involving two attributes,A and B,is suspected,and the algorithmis only capable ofaxis-parallel splits (as most decision tree and rule learnersare),the ratio A/B might be deﬁned as a new attribute.The transformations arenot necessarily mathematical ones but may involve world knowledge such asdays ofthe week,civic holidays,or chemical atomic numbers.They could beexpressed as operations in a spreadsheet or as functions that are implementedby arbitrary computer programs.Or you can reduce several nominal attributesto one by concatenating their values,producing a single k1¥k2-valued attrib-ute from attributes with k1andk2values,respectively.Discretization converts anumeric attribute to nominal,and we saw earlier how to convert in the otherdirection too.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 305As another kind oftransformation,you might apply a clustering procedureto the dataset and then deﬁne a new attribute whose value for any given instanceis the cluster that contains it using an arbitrary labeling for clusters.Alterna-tively,with probabilistic clustering,you could augment each instance with itsmembership probabilities for each cluster,including as many new attributes asthere are clusters.Sometimes it is useful to add noise to data,perhaps to test the robustness ofa learning algorithm.To take a nominal attribute and change a given percent-age ofits values.To obfuscate data by renaming the relation,attribute names,and nominal and string attribute values—because it is often necessary toanonymize sensitive datasets.To randomize the order ofinstances or produce arandom sample ofthe dataset by resampling it.To reduce a dataset by remov-ing a given percentage ofinstances,or all instances that have certain values fornominal attributes,or numeric values above or below a certain threshold.Or toremove outliers by applying a classiﬁcation method to the dataset and deletingmisclassiﬁed instances.Different types ofinput call for their own transformations.Ifyou can inputsparse data ﬁles (see Section 2.4),you may need to be able to convert datasetsto a nonsparse form,and vice versa.Textual input and time series input call fortheir own specialized conversions,described in the subsections that follow.Butﬁrst we look at two general techniques for transforming data with numericattributes into a lower-dimensional form that may be more useful for datamining.Principal components analysisIn a dataset with knumeric attributes,you can visualize the data as a cloud ofpoints in k-dimensional space—the stars in the sky,a swarm ofﬂies frozen intime,a two-dimensional scatter plot on paper.The attributes represent the co-ordinates ofthe space.But the axes you use,the coordinate system itself,is arbi-trary.You can place horizontal and vertical axes on the paper and represent thepoints ofthe scatter plot using those coordinates,or you could draw an arbi-trary straight line to represent the X-axis and one perpendicular to it to repre-sent Y.To record the positions ofthe ﬂies you could use a conventionalcoordinate system with a north–south axis,an east–west axis,and an up–downaxis.But other coordinate systems would do equally well.Creatures such as ﬂiesdon’t know about north,south,east,and west—although,being subject togravity,they may perceive up–down as being something special.As for the starsin the sky,who’s to say what the “right”coordinate system is? Over the centuriesour ancestors moved from a geocentric perspective to a heliocentric one to apurely relativistic one,each shift ofperspective being accompanied by turbu-306CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3067.3SOME USEFUL TRANSFORMATIONS307lent religious–scientiﬁc upheavals and painful reexamination ofhumankind’srole in God’s universe.Back to the dataset.Just as in these examples,there is nothing to stop youtransforming all the data points into a different coordinate system.But unlikethese examples,in data mining there often isa preferred coordinate system,deﬁned not by some external convention but by the very data itself.Whatevercoordinates you use,the cloud ofpoints has a certain variance in each direc-tion,indicating the degree ofspread around the mean value in that direction.It is a curious fact that ifyou add up the variances along each axis and thentransform the points into a different coordinate system and do the same there,you get the same total variance in both cases.This is always true provided thatthe coordinate systems are orthogonal,that is,each axis is at right angles to theothers.The idea ofprincipal components analysis is to use a special coordinatesystem that depends on the cloud ofpoints as follows:place the ﬁrst axis in thedirection ofgreatest variance ofthe points to maximize the variance along thataxis.The second axis is perpendicular to it.In two dimensions there is nochoice—its direction is determined by the ﬁrst axis—but in three dimensionsit can lie anywhere in the plane perpendicular to the ﬁrst axis,and in higherdimensions there is even more choice,although it is always constrained to beperpendicular to the ﬁrst axis.Subject to this constraint,choose the second axisin the way that maximizes the variance along it.Continue,choosing each axisto maximize its share ofthe remaining variance.How do you do this? It’s not hard,given an appropriate computer program,and it’s not hard to understand,given the appropriate mathematical tools.Tech-nically—for those who understand the italicized terms—you calculate thecovariance matrixofthe original coordinates ofthe points and diagonalizeit toﬁnd the eigenvectors.These are the axes ofthe transformed space,sorted in orderofeigenvalue—because each eigenvalue gives the variance along its axis.Figure 7.5 shows the result oftransforming a particular dataset with 10numeric attributes,corresponding to points in 10-dimensional space.Imaginethe original dataset as a cloud ofpoints in 10 dimensions—we can’t draw it!Choose the ﬁrst axis along the direction ofgreatest variance,the second per-pendicular to it along the direction ofnext greatest variance,and so on.Thetable gives the variance along each new coordinate axis in the order in whichthe axes were chosen.Because the sum ofthe variances is constant regardless ofthe coordinate system,they are expressed as percentages ofthat total.We callaxes componentsand say that each one “accounts for”its share ofthe variance.Figure 7.5(b) plots the variance that each component accounts for against thecomponent’s number.You can use all the components as new attributes for datamining,or you might want to choose just the ﬁrst few,the principal components,P088407-Ch007.qxd  4/30/05  11:16 AM  Page 307and discard the rest.In this case,three principal components account for 84%ofthe variance in the dataset;seven account for more than 95%.On numeric datasets it is common to use principal components analysisbefore data mining as a form ofdata cleanup and attribute generation.Forexample,you might want to replace the numeric attributes with the principalcomponent axes or with a subset ofthem that accounts for a given proportion—say,95%—ofthe variance.Note that the scale ofthe attributes affects the308CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTAxis1234567891061.2%18.0%4.7%4.0%3.2%2.9%2.0%1.7%1.4%0.9%61.2%79.2%83.9%87.9%91.1%94.0%96.0%97.7%99.1%100%VarianceCumulative(a)percentage of variance70%60% 50% 40% 30% 20% 10% 0%98765432110component number(b)Figure 7.5Principal components transform ofa dataset:(a) variance ofeach compo-nent and (b) variance plot.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3087.3SOME USEFUL TRANSFORMATIONS309outcome ofprincipal components analysis,and it is common practice to stan-dardize all attributes to zero mean and unit variance ﬁrst.Another possibility is to apply principal components analysis recursively ina decision tree learner.At each stage an ordinary decision tree learner choosesto split in a direction that is parallel to one ofthe axes.However,suppose a prin-cipal components transform is performed ﬁrst,and the learner chooses an axisin the transformed space.This equates to a split along an oblique line in theoriginal space.Ifthe transform is performed afresh before each split,the resultwill be a multivariate decision tree whose splits are in directions that are notparallel with the axes or with one another.Random projectionsPrincipal components analysis transforms the data linearly into a lower-dimensional space.But it’s expensive.The time taken to ﬁnd the trans-formation (which is a matrix comprising the eigenvectors ofthe covariancematrix) is cubic in the number ofdimensions.This makes it infeasible fordatasets with a large number ofattributes.A far simpler alternative is to use arandom projection ofthe data into a subspace with a predetermined numberofdimensions.It’s very easy to ﬁnd a random projection matrix.But will it beany good?In fact,theory shows that random projections preserve distance relationshipsquite well on average.This means that they could be used in conjunction withkD-trees or ball trees to do approximate nearest-neighbor search in spaces witha huge number ofdimensions.First transform the data to reduce the numberofattributes;then build a tree for the transformed space.In the case ofnearest-neighbor classiﬁcation you could make the result more stable,and less depend-ent on the choice ofrandom projection,by building an ensemble classiﬁer thatuses multiple random matrices.Not surprisingly,random projections perform worse than ones carefullychosen by principal components analysis when used to preprocess data for arange ofstandard classiﬁers.However,experimental results have shown that thedifference is not too great—and that it tends to decrease as the number ofdimensions increase.And ofcourse,random projections are far cheaper computationally.Text to attribute vectorsIn Section 2.4 we introduced string attributes that contain pieces oftext andremarked that the value ofa string attribute is often an entire document.Stringattributes are basically nominal,with an unspeciﬁed number ofvalues.Iftheyare treated simply as nominal attributes,models can be built that depend onwhether the values oftwo string attributes are equal or not.But that does notP088407-Ch007.qxd  4/30/05  11:16 AM  Page 309capture any internal structure ofthe string or bring out any interesting aspectsofthe text it represents.You could imagine decomposing the text in a string attribute into paragraphs,sentences,or phrases.Generally,however,the word is the most useful unit.Thetext in a string attribute is usually a sequence ofwords,and is often best repre-sented in terms ofthe words it contains.For example,you might transform thestring attribute into a set ofnumeric attributes,one for each word,that repre-sent how often the word appears.The set ofwords—that is,the set ofnew attrib-utes—is determined from the dataset and is typically quite large.Ifthere areseveral string attributes whose properties should be treated separately,the newattribute names must be distinguished,perhaps by a user-determined preﬁx.Conversion into words—tokenization—is not such a simple operation as itsounds.Tokens may be formed from contiguous alphabetic sequences with non-alphabetic characters discarded.Ifnumbers are present,numeric sequences maybe retained too.Numbers may involve +or -signs,may contain decimal points,and may have exponential notation—in other words,they must be parsedaccording to a deﬁned number syntax.An alphanumeric sequence may beregarded as a single token.Perhaps the space character is the token delimiter;perhaps white space (including the tab and new-line characters) is the delim-iter,and perhaps punctuation is,too.Periods can be difﬁcult:sometimes theyshould be considered part ofthe word (e.g.,with initials,titles,abbreviations,and numbers),but sometimes they should not (e.g.,ifthey are sentence delim-iters).Hyphens and apostrophes are similarly problematic.All words may be converted to lowercase before being added to the diction-ary.Words on a ﬁxed,predetermined list offunction words or stopwords—suchas the,and,and but—could be ignored.Note that stopword lists are languagedependent.In fact,so are capitalization conventions (German capitalizes allnouns),number syntax (Europeans use the comma for a decimal point),punc-tuation conventions (Spanish has an initial question mark),and,ofcourse,char-acter sets.Text is complicated!Low-frequency words such as hapax legomena3are often discarded,too.Sometimes it is found beneﬁcial to keep the most frequent kwords after stop-words have been removed—or perhaps the top kwords for each class.Along with all these tokenization options,there is also the question ofwhat the value ofeach word attribute should be.The value may be the wordcount—the number oftimes the word appears in the string—or it may simplyindicate the word’s presence or absence.Word frequencies could be normalized togive each document’s attribute vector the same Euclidean length.Alternatively,310CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUT3A hapax legomenais a word that only occurs once in a given corpus oftext.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3107.3SOME USEFUL TRANSFORMATIONS311the frequencies fijfor word i in document j can be transformed in various stan-dard ways.One standard logarithmic term frequency measure is log (1 +fij).Ameasure that is widely used in information retrieval is TF ¥IDF,or “term fre-quency times inverse document frequency.”Here,the term frequency is modu-lated by a factor that depends on how commonly the word is used in otherdocuments.The TF ¥IDF metric is typically deﬁned asThe idea is that a document is basically characterized by the words that appearoften in it,which accounts for the ﬁrst factor,except that words used in everydocument or almost every document are useless as discriminators,whichaccounts for the second.TF ¥IDF is used to refer not just to this particularformula but also to a general class ofmeasures ofthe same type.For example,the frequency factor fijmay be replaced by a logarithmic term such as log (1 +fij).Time seriesIn time series data,each instance represents a different time step and the attrib-utes give values associated with that time—such as in weather forecasting or stock market prediction.You sometimes need to be able to replace anattribute’s value in the current instance with the corresponding value in some other instance in the past or the future.It is even more common to replacean attribute’s value with the difference between the current value and the value in some previous instance.For example,the difference—often called theDelta—between the current value and the preceding one is often more informative than the value itself.The ﬁrst instance,in which the time-shiftedvalue is unknown,may be removed,or replaced with a missing value.The Deltavalue is essentially the ﬁrst derivative scaled by some constant that depends on the size ofthe time step.Successive Delta transformations take higher derivatives.In some time series,instances do not represent regular samples,but the timeofeach instance is given by a timestampattribute.The difference between time-stamps is the step size for that instance,and ifsuccessive differences are takenfor other attributes they should be divided by the step size to normalize thederivative.In other cases each attribute may represent a different time,ratherthan each instance,so that the time series is from one attribute to the next ratherthan from one instance to the next.Then,ifdifferences are needed,they mustbe taken between one attribute’s value and the next attribute’s value for eachinstance.fiijlog.number of documentsnumber of documents that include wordP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3117.4Automatic data cleansingA problem that plagues practical data mining is poor quality ofthe data.Errorsin large databases are extremely common.Attribute values,and class values too,are frequently unreliable and corrupted.Although one way ofaddressing thisproblem is to painstakingly check through the data,data mining techniquesthemselves can sometimes help to solve the problem.Improving decision treesIt is a surprising fact that decision trees induced from training data can oftenbe simpliﬁed,without loss ofaccuracy,by discarding misclassiﬁed instancesfrom the training set,relearning,and then repeating until there are no misclas-siﬁed instances.Experiments on standard datasets have shown that this hardlyaffects the classiﬁcation accuracy ofC4.5,a standard decision tree inductionscheme.In some cases it improves slightly;in others it deteriorates slightly.Thedifference is rarely statistically signiﬁcant—and even when it is,the advantagecan go either way.What the technique does affect is decision tree size.The result-ing trees are invariably much smaller than the original ones,even though theyperform about the same.What is the reason for this? When a decision tree induction method prunesaway a subtree,it applies a statistical test that decides whether that subtree is“justiﬁed”by the data.The decision to prune accepts a small sacriﬁce in classi-ﬁcation accuracy on the training set in the beliefthat this will improve test-setperformance.Some training instances that were classiﬁed correctly by theunpruned tree will now be misclassiﬁed by the pruned one.In effect,the deci-sion has been taken to ignore these training instances.But that decision has only been applied locally,in the pruned subtree.Itseffect has not been allowed to percolate further up the tree,perhaps resultingin different choices being made ofattributes to branch on.Removing the mis-classiﬁed instances from the training set and relearning the decision tree is justtaking the pruning decisions to their logical conclusion.Ifthe pruning strategyis a good one,this should not harm performance.It may even improve it byallowing better attribute choices to be made.It would no doubt be even better to consult a human expert.Misclassiﬁedtraining instances could be presented for veriﬁcation,and those that were foundto be wrong could be deleted—or better still,corrected.Notice that we are assuming that the instances are not misclassiﬁed in anysystematic way.Ifinstances are systematically corrupted in both training andtest sets—for example,one class value might be substituted for another—it isonly to be expected that training on the erroneous training set would yield betterperformance on the (also erroneous) test set.312CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3127.4AUTOMATIC DATA CLEANSING313Interestingly enough,it has been shown that when artiﬁcial noise is added toattributes (rather than to classes),test-set performance is improved ifthe samenoise is added in the same way to the training set.In other words,when attrib-ute noise is the problem it is not a good idea to train on a “clean”set ifper-formance is to be assessed on a “dirty”one.A learning method can learn tocompensate for attribute noise,in some measure,ifgiven a chance.In essence,it can learn which attributes are unreliable and,ifthey are all unreliable,howbest to use them together to yield a more reliable result.To remove noise fromattributes for the training set denies the opportunity to learn how best to combatthat noise.But with class noise (rather than attribute noise),it is best to trainon noise-free instances ifpossible.Robust regressionThe problems caused by noisy data have been known in linear regression foryears.Statisticians often check data for outliers and remove them manually.Inthe case oflinear regression,outliers can be identiﬁed visually—although it isnever completely clear whether an outlier is an error or just a surprising,butcorrect,value.Outliers dramatically affect the usual least-squares regressionbecause the squared distance measure accentuates the inﬂuence ofpoints faraway from the regression line.Statistical methods that address the problem ofoutliers are called robust.Oneway ofmaking regression more robust is to use an absolute-value distancemeasure instead ofthe usual squared one.This weakens the effect ofoutliers.Another possibility is to try to identify outliers automatically and remove themfrom consideration.For example,one could form a regression line and thenremove from consideration those 10% ofpoints that lie furthest from the line.A third possibility is to minimize the median(rather than the mean) ofthesquares ofthe divergences from the regression line.It turns out that this esti-mator is very robust and actually copes with outliers in the X-direction as well as outliers in the Y-direction—which is the normal direction one thinks ofoutliers.A dataset that is often used to illustrate robust regression is the graph ofinter-national telephone calls made from Belgium from 1950 to 1973,shown in Figure7.6.This data is taken from the Belgian Statistical Survey published by the Min-istry ofEconomy.The plot seems to show an upward trend over the years,butthere is an anomalous group ofpoints from 1964 to 1969.It turns out thatduring this period,results were mistakenly recorded in the total number ofminutesofthe calls.The years 1963 and 1970 are also partially affected.Thiserror causes a large fraction ofoutliers in the Y-direction.Not surprisingly,the usual least-squares regression line is seriously affectedby this anomalous data.However,the least medianofsquares line remainsP088407-Ch007.qxd  4/30/05  11:16 AM  Page 313remarkably unperturbed.This line has a simple and natural interpretation.Geo-metrically,it corresponds to ﬁnding the narrowest strip covering halfoftheobservations,where the thickness ofthe strip is measured in the vertical direc-tion—this strip is marked gray in Figure 7.6;you need to look closely to see it.The least median ofsquares line lies at the exact center ofthis band.Note thatthis notion is often easier to explain and visualize than the normal least-squaresdeﬁnition ofregression.Unfortunately,there is a serious disadvantage tomedian-based regression techniques:they incur a high computational cost,which often makes them infeasible for practical problems.Detecting anomaliesA serious problem with any form ofautomatic detection ofapparently incor-rect data is that the baby may be thrown out with the bathwater.Short ofcon-sulting a human expert,there is really no way oftelling whether a particularinstance really is an error or whether it just does not ﬁt the type ofmodel thatis being applied.In statistical regression,visualizations help.It will usually bevisually apparent,even to the nonexpert,ifthe wrong kind ofcurve is beingﬁtted—a straight line is being ﬁtted to data that lies on a parabola,for example.The outliers in Figure 7.6 certainly stand out to the eye.But most problemscannot be so easily visualized:the notion of“model type”is more subtle than aregression line.And although it is known that good results are obtained on moststandard datasets by discarding instances that do not ﬁt a decision tree model,this is not necessarily ofgreat comfort when dealing with a particular new314CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUT-50510152025195019551960196519701975least squaresleast median of squaresyearphone calls (tens of millions)Figure 7.6Number ofinternational phone calls from Belgium,1950–1973.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3147.5COMBINING MULTIPLE MODELS315dataset.The suspicion will remain that perhaps the new dataset is simplyunsuited to decision tree modeling.One solution that has been tried is to use several different learning schemes—such as a decision tree,and a nearest-neighbor learner,and a linear discrimi-nant function—to ﬁlter the data.A conservative approach is to ask that all threeschemes fail to classify an instance correctly before it is deemed erroneous andremoved from the data.In some cases,ﬁltering the data in this way and usingthe ﬁltered data as input to a ﬁnal learning scheme gives better performancethan simply using the three learning schemes and letting them vote on theoutcome.Training all three schemes on the ﬁltereddata and letting them votecan yield even better results.However,there is a danger to voting techniques:some learning algorithms are better suited to certain types ofdata than others,and the most appropriate method may simply get out-voted! We will examinea more subtle method ofcombining the output from different classiﬁers,calledstacking,in the next section.The lesson,as usual,is to get to know your dataand look at it in many different ways.One possible danger with ﬁltering approaches is that they might con-ceivably just be sacriﬁcing instances ofa particular class (or group ofclasses) to improve accuracy on the remaining classes.Although there are no generalways to guard against this,it has not been found to be a common problem inpractice.Finally,it is worth noting once again that automatic ﬁltering is a poor sub-stitute for getting the data right in the ﬁrst place.Ifthis is too time consumingand expensive to be practical,human inspection could be limited to thoseinstances that are identiﬁed by the ﬁlter as suspect.7.5Combining multiple modelsWhen wise people make critical decisions,they usually take into account theopinions ofseveral experts rather than relying on their own judgment or thatofa solitary trusted adviser.For example,before choosing an important newpolicy direction,a benign dictator consults widely:he or she would be ill advisedto follow just one expert’s opinion blindly.In a democratic setting,discussionofdifferent viewpoints may produce a consensus;ifnot,a vote may be calledfor.In either case,different expert opinions are being combined.In data mining,a model generated by machine learning can be regarded asan expert.Expertis probably too strong a word!—depending on the amountand quality ofthe training data,and whether the learning algorithm is appro-priate to the problem at hand,the expert may in truth be regrettably ignorant—but we use the term nevertheless.An obvious approach to making decisionsmore reliable is to combine the output ofdifferent models.Several machineP088407-Ch007.qxd  4/30/05  11:16 AM  Page 315learning techniques do this by learning an ensemble ofmodels and using themin combination:prominent among these are schemes called bagging,boosting,and stacking.They can all,more often than not,increase predictive performanceover a single model.And they are general techniques that can be applied tonumeric prediction problems and to classiﬁcation tasks.Bagging,boosting,and stacking have only been developed over the pastdecade,and their performance is often astonishingly good.Machine learningresearchers have struggled to understand why.And during that struggle,newmethods have emerged that are sometimes even better.For example,whereashuman committees rarely beneﬁt from noisy distractions,shaking up baggingby adding random variants ofclassiﬁers can improve performance.Closeranalysis revealed that boosting—perhaps the most powerful ofthe threemethods—is closely related to the established statistical technique ofadditivemodels,and this realization has led to improved procedures.These combined models share the disadvantage ofbeing difﬁcult to analyze:they can comprise dozens or even hundreds ofindividual models,and althoughthey perform well it is not easy to understand in intuitive terms what factorsare contributing to the improved decisions.In the last few years methods havebeen developed that combine the performance beneﬁts ofcommittees withcomprehensible models.Some produce standard decision tree models;othersintroduce new variants oftrees that provide optional paths.We close by introducing a further technique ofcombining models usingerror-correcting output codes.This is more specialized than the other three tech-niques:it applies only to classiﬁcation problems,and even then only to ones thathave more than three classes.BaggingCombining the decisions ofdifferent models means amalgamating the variousoutputs into a single prediction.The simplest way to do this in the case ofclas-siﬁcation is to take a vote (perhaps a weighted vote);in the case ofnumeric pre-diction,it is to calculate the average (perhaps a weighted average).Bagging andboosting both adopt this approach,but they derive the individual models in dif-ferent ways.In bagging,the models receive equal weight,whereas in boosting,weighting is used to give more inﬂuence to the more successful ones—just asan executive might place different values on the advice ofdifferent expertsdepending on how experienced they are.To introduce bagging,suppose that several training datasets ofthe same sizeare chosen at random from the problem domain.Imagine using a particularmachine learning technique to build a decision tree for each dataset.You mightexpect these trees to be practically identical and to make the same predictionfor each new test instance.Surprisingly,this assumption is usually quite wrong,316CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3167.5COMBINING MULTIPLE MODELS317particularly ifthe training datasets are fairly small.This is a rather disturbingfact and seems to cast a shadow over the whole enterprise! The reason for it isthat decision tree induction (at least,the standard top-down method describedin Chapter 4) is an unstable process:slight changes to the training data mayeasily result in a different attribute being chosen at a particular node,with sig-niﬁcant ramiﬁcations for the structure ofthe subtree beneath that node.Thisautomatically implies that there are test instances for which some ofthe deci-sion trees produce correct predictions and others do not.Returning to the preceding experts analogy,consider the experts to be theindividual decision trees.We can combine the trees by having them vote on eachtest instance.Ifone class receives more votes than any other,it is taken as thecorrect one.Generally,the more the merrier:predictions made by votingbecome more reliable as more votes are taken into account.Decisions rarelydeteriorate ifnew training sets are discovered,trees are built for them,and theirpredictions participate in the vote as well.In particular,the combined classiﬁerwill seldom be less accurate than a decision tree constructed from just one ofthe datasets.(Improvement is not guaranteed,however.It can be shown theo-retically that pathological situations exist in which the combined decisions areworse.)The effect ofcombining multiple hypotheses can be viewed through a theo-retical device known as the bias–variance decomposition.Suppose that we couldhave an inﬁnite number ofindependent training sets ofthe same size and usethem to make an inﬁnite number ofclassiﬁers.A test instance is processed byall classiﬁers,and a single answer is determined by majority vote.In this ideal-ized situation,errors will still occur because no learning scheme is perfect:theerror rate will depend on how well the machine learning method matches theproblem at hand,and there is also the effect ofnoise in the data,which cannotpossibly be learned.Suppose the expected error rate were evaluated by averag-ing the error ofthe combined classiﬁer over an inﬁnite number ofindepend-ently chosen test examples.The error rate for a particular learning algorithm iscalled its biasfor the learning problem and measures how well the learningmethod matches the problem.This technical deﬁnition is a way ofquantifyingthe vaguer notion ofbias that was introduced in Section 1.5:it measures the“persistent”error ofa learning algorithm that can’t be eliminated even by takingan inﬁnite number oftraining sets into account.Ofcourse,it cannot be calcu-lated exactly in practical situations;it can only be approximated.A second source oferror in a learned model,in a practical situation,stemsfrom the particular training set used,which is inevitably ﬁnite and therefore notfully representative ofthe actual population ofinstances.The expected value ofthis component ofthe error,over all possible training sets ofthe given size andall possible test sets,is called the varianceofthe learning method for thatproblem.The total expected error ofa classiﬁer is made up ofthe sum ofbiasP088407-Ch007.qxd  4/30/05  11:16 AM  Page 317and variance:this is the bias–variance decomposition.4Combining multipleclassiﬁers decreases the expected error by reducing the variance component.Themore classiﬁers that are included,the greater the reduction in variance.Ofcourse,a difﬁculty arises when putting this voting method into practice:usually there’s only one training set,and obtaining more data is either impos-sible or expensive.Bagging attempts to neutralize the instability oflearning methods by simu-lating the process described previously using a given training set.Instead ofsam-pling a fresh,independent training dataset each time,the original training datais altered by deleting some instances and replicating others.Instances are ran-domly sampled,with replacement,from the original dataset to create a new one ofthe same size.This sampling procedure inevitably replicates some ofthe instances and deletes others.Ifthis idea strikes a chord,it is because wedescribed it in Chapter 5 when explaining the bootstrap method for estimatingthe generalization error ofa learning method (Section 5.4):indeed,the termbaggingstands for bootstrap aggregating.Bagging applies the learning scheme—for example,a decision tree inducer—to each one ofthese artiﬁcially deriveddatasets,and the classiﬁers generated from them vote for the class to be pre-dicted.The algorithm is summarized in Figure 7.7.The difference between bagging and the idealized procedure described pre-viously is the way in which the training datasets are derived.Instead ofobtain-ing independent datasets from the domain,bagging just resamples the originaltraining data.The datasets generated by resampling are different from oneanother but are certainly not independent because they are all based on onedataset.However,it turns out that bagging produces a combined model thatoften performs signiﬁcantly better than the single model built from the origi-nal training data,and is never substantially worse.Bagging can also be applied to learning methods for numeric prediction—for example,model trees.The only difference is that,instead ofvoting on theoutcome,the individual predictions,being real numbers,are averaged.Thebias–variance decomposition can be applied to numeric prediction as well bydecomposing the expected value ofthe mean-squared error ofthe predictionson fresh data.Bias is deﬁned as the mean-squared error expected when averag-ing over models built from all possible training datasets ofthe same size,andvariance is the component ofthe expected error ofa single model that is dueto the particular training data it was built from.It can be shown theoreticallythat averaging over multiple models built from independent training sets always318CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUT4This is a simpliﬁed version ofthe full story.Several different methods for performing thebias–variance decomposition can be found in the literature;there is no agreed way ofdoingthis.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3187.5COMBINING MULTIPLE MODELS319reduces the expected value ofthe mean-squared error.(As we mentioned earlier,the analogous result is not true for classiﬁcation.)Bagging with costsBagging helps most ifthe underlying learning method is unstable in that smallchanges in the input data can lead to quite different classiﬁers.Indeed it canhelp to increase the diversity in the ensemble ofclassiﬁers by making the learn-ing method as unstable as possible.For example,when bagging decision trees,which are already unstable,better performance is often achieved by switchingpruning off,which makes them even more unstable.Another improvement canbe obtained by changing the way that predictions are combined for classiﬁca-tion.As originally formulated,bagging uses voting.But when the models canoutput probability estimates and not just plain classiﬁcations,it makes intuitivesense to average these probabilities instead.Not only does this often improveclassiﬁcation slightly,but the bagged classiﬁer also generates probability estimates—ones that are often more accurate than those produced by the in-dividual models.Implementations ofbagging commonly use this method ofcombining predictions.In Section 5.7 we showed how to make a classiﬁer cost sensitive by mini-mizing the expected cost ofpredictions.Accurate probability estimates are nec-essary because they are used to obtain the expected cost ofeach prediction.Bagging is a prime candidate for cost-sensitive classiﬁcation because it producesvery accurate probability estimates from decision trees and other powerful,yetunstable,classiﬁers.However,a disadvantage is that bagged classiﬁers are hardto analyze.A method called MetaCostcombines the predictive beneﬁts ofbagging witha comprehensible model for cost-sensitive prediction.It builds an ensembleclassiﬁer using bagging and uses it to relabel the training data by giving everymodel generationLet n be the number of instances in the training data.For each of t iterations:  Sample n instances with replacement from training data.  Apply the learning algorithm to the sample.  Store the resulting model.classificationFor each of the t models:  Predict class of instance using model.Return class that has been predicted most often.Figure 7.7Algorithm for bagging.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 319training instance the prediction that minimizes the expected cost,based on theprobability estimates obtained from bagging.MetaCost then discards the orig-inal class labels and learns a single new classiﬁer—for example,a single pruneddecision tree—from the relabeled data.This new model automatically takescosts into account because they have been built into the class labels! The resultis a single cost-sensitive classiﬁer that can be analyzed to see how predictionsare made.In addition to the cost-sensitive classiﬁcationtechnique just mentioned,Section 5.7 also described a cost-sensitive learningmethod that learns a cost-sensitive classiﬁer by changing the proportion ofeach class in the training datato reﬂect the cost matrix.MetaCost seems to produce more accurate results thanthis method,but it requires more computation.Ifthere is no need for a com-prehensible model,MetaCost’s postprocessing step is superﬂuous:it is better touse the bagged classiﬁer directly in conjunction with the minimum expectedcost method.RandomizationBagging generates a diverse ensemble ofclassiﬁers by introducing randomnessinto the learning algorithm’s input,often with excellent results.But there areother ways ofcreating diversity by introducing randomization.Some learningalgorithms already have a built-in random component.For example,whenlearning multilayer perceptrons using the backpropagation algorithm (asdescribed in Section 6.3) the network weights are set to small randomly chosenvalues.The learned classiﬁer depends on the random numbers because the algo-rithm may ﬁnd a different local minimum ofthe error function.One way tomake the outcome ofclassiﬁcation more stable is to run the learner several timeswith different random number seeds and combine the classiﬁers’predictions byvoting or averaging.Almost every learning method is amenable to some kind ofrandomization.Consider an algorithm that greedily picks the best option at every step—suchas a decision tree learner that picks the best attribute to split on at each node.It could be randomized by randomly picking one ofthe Nbest options insteadofa single winner,or by choosing a random subset ofoptions and picking thebest from that.Ofcourse,there is a tradeoff:more randomness generates morevariety in the learner but makes less use ofthe data,probably decreasing theaccuracy ofeach individual model.The best dose ofrandomness can only beprescribed by experiment.Although bagging and randomization yield similar results,it sometimes paysto combine them because they introduce randomness in different,perhaps complementary,ways.A popular algorithm for learning random forests buildsa randomized decision tree in each iteration ofthe bagging algorithm,and oftenproduces excellent predictors.320CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3207.5COMBINING MULTIPLE MODELS321Randomization demands more work than bagging because the learning algo-rithm must be modiﬁed,but it can proﬁtably be applied to a greater variety oflearners.We noted earlier that bagging fails with stable learning algorithmswhose output is insensitive to small changes in the input.For example,it ispointless to bag nearest-neighbor classiﬁers because their output changes verylittle ifthe training data is perturbed by sampling.But randomization can beapplied even to stable learners:the trick is to randomize in a way that makesthe classiﬁers diverse without sacriﬁcing too much performance.A nearest-neighbor classiﬁer’s predictions depend on the distances between instances,which in turn depend heavily on which attributes are used to compute them,so nearest-neighbor classiﬁers can be randomized by using different,randomlychosen subsets ofattributes.BoostingWe have explained that bagging exploits the instability inherent in learning algo-rithms.Intuitively,combining multiple models only helps when these modelsare signiﬁcantly different from one another and when each one treats a reason-able percentage ofthe data correctly.Ideally,the models complement oneanother,each being a specialist in a part ofthe domain where the other modelsdon’t perform very well—just as human executives seek advisers whose skillsand experience complement,rather than duplicate,one another.The boosting method for combining multiple models exploits this insight byexplicitly seeking models that complement one another.First,the similarities:like bagging,boosting uses voting (for classiﬁcation) or averaging (for numericprediction) to combine the output ofindividual models.Again like bagging,itcombines models ofthe same type—for example,decision trees.However,boost-ing is iterative.Whereas in bagging individual models are built separately,inboosting each new model is inﬂuenced by the performance ofthose built previ-ously.Boosting encourages new models to become experts for instances handledincorrectly by earlier ones.A ﬁnal difference is that boosting weights a model’scontribution by its performance rather than giving equal weight to all models.There are many variants on the idea ofboosting.We describe a widely usedmethod called AdaBoost.M1that is designed speciﬁcally for classiﬁcation.Likebagging,it can be applied to any classiﬁcation learning algorithm.To simplifymatters we assume that the learning algorithm can handle weighted instances,where the weight ofan instance is a positive number.(We revisit this assump-tion later.) The presence ofinstance weights changes the way in which a classi-ﬁer’s error is calculated:it is the sum ofthe weights ofthe misclassiﬁed instancesdivided by the total weight ofall instances,instead ofthe fraction ofinstancesthat are misclassiﬁed.By weighting instances,the learning algorithm can beforced to concentrate on a particular set ofinstances,namely,those with highP088407-Ch007.qxd  4/30/05  11:16 AM  Page 321weight.Such instances become particularly important because there is a greaterincentive to classify them correctly.The C4.5 algorithm,described in Section6.1,is an example ofa learning method that can accommodate weightedinstances without modiﬁcation because it already uses the notion offractionalinstances to handle missing values.The boosting algorithm,summarized in Figure 7.8,begins by assigning equalweight to all instances in the training data.It then calls the learning algorithmto form a classiﬁer for this data and reweights each instance according to theclassiﬁer’s output.The weight ofcorrectly classiﬁed instances is decreased,andthat ofmisclassiﬁed ones is increased.This produces a set of“easy”instanceswith low weight and a set of“hard”ones with high weight.In the next itera-tion—and all subsequent ones—a classiﬁer is built for the reweighted data,which consequently focuses on classifying the hard instances correctly.Then theinstances’weights are increased or decreased according to the output ofthis newclassiﬁer.As a result,some hard instances might become even harder and easierones might become even easier;on the other hand,other hard instances mightbecome easier,and easier ones might become harder—all possibilities can occurin practice.After each iteration,the weights reﬂect how often the instances havebeen misclassiﬁed by the classiﬁers produced so far.By maintaining a measureof“hardness”with each instance,this procedure provides an elegant way ofgen-erating a series ofexperts that complement one another.322CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTmodel generationAssign equal weight to each training instance.For each of t iterations:  Apply learning algorithm to weighted dataset and store      resulting model.  Compute error e of model on weighted dataset and store error.  If e equal to zero, or e greater or equal to 0.5:     Terminate model generation.  For each instance in dataset:     If instance classified correctly by model:        Multiply weight of instance by e / (1 – e).  Normalize weight of all instances.classificationAssign weight of zero to all classes.For each of the t (or less) models:  Add –log(e / (1 – e)) to weight of class predicted by model.Return class with highest weight.Figure 7.8Algorithm for boosting.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3227.5COMBINING MULTIPLE MODELS323How much should the weights be altered after each iteration? The answerdepends on the current classiﬁer’s overall error.More speciﬁcally,ifedenotesthe classiﬁer’s error on the weighted data (a fraction between 0 and 1),thenweights are updated byfor correctly classiﬁed instances,and the weights remain unchanged for mis-classiﬁed ones.Ofcourse,this does not increase the weight ofmisclassiﬁedinstances as claimed previously.However,after all weights have been updatedthey are renormalized so that their sum remains the same as it was before.Eachinstance’s weight is divided by the sum ofthe new weights and multiplied bythe sum ofthe old ones.This automatically increases the weight ofeach mis-classiﬁed instance and reduces that ofeach correctly classiﬁed one.Whenever the error on the weighted training data exceeds or equals 0.5,theboosting procedure deletes the current classiﬁer and does not perform any moreiterations.The same thing happens when the error is 0,because then all instanceweights become 0.We have explained how the boosting method generates a series ofclassiﬁers.To form a prediction,their output is combined using a weighted vote.To deter-mine the weights,note that a classiﬁer that performs well on the weighted train-ing data from which it was built (eclose to 0) should receive a high weight,anda classiﬁer that performs badly (eclose to 0.5) should receive a low one.Morespeciﬁcally,which is a positive number between 0 and inﬁnity.Incidentally,this formulaexplains why classiﬁers that perform perfectly on the training data must bedeleted,because when eis 0 the weight is undeﬁned.To make a prediction,theweights ofall classiﬁers that vote for a particular class are summed,and the classwith the greatest total is chosen.We began by assuming that the learning algorithm can cope with weightedinstances.We explained how to adapt learning algorithms to deal with weightedinstances at the end ofSection 6.5 under Locally weighted linear regression.Instead ofchanging the learning algorithm,it is possible to generate anunweighted dataset from the weighted data by resampling—the same techniquethat bagging uses.Whereas for bagging each instance is chosen with equal prob-ability,for boosting instances are chosen with probability proportional to theirweight.As a result,instances with high weight are replicated frequently,and oneswith low weight may never be selected.Once the new dataset becomes as largeas the original one,it is fed into the learning method instead ofthe weighteddata.It’s as simple as that.weightee=--log,1weightweighte1e¨¥-()P088407-Ch007.qxd  4/30/05  11:16 AM  Page 323A disadvantage ofthis procedure is that some instances with low weight don’tmake it into the resampled dataset,so information is lost before the learningmethod is applied.However,this can be turned into an advantage.Ifthe learn-ing method produces a classiﬁer whose error exceeds 0.5,boosting must termi-nate ifthe weighted data is used directly,whereas with resampling it might bepossible to produce a classiﬁer with error below 0.5 by discarding the resam-pled dataset and generating a new one from a different random seed.Sometimesmore boosting iterations can be performed by resampling than when using theoriginal weighted version ofthe algorithm.The idea ofboosting originated in a branch ofmachine learning researchknown as computational learning theory.Theoreticians are interested in boost-ing because it is possible to derive performance guarantees.For example,it canbe shown that the error ofthe combined classiﬁer on the training dataapproaches zero very quickly as more iterations are performed (exponentiallyquickly in the number ofiterations).Unfortunately,as explained in Section 5.1,guarantees for the training error are not very interesting because they do notnecessarily indicate good performance on fresh data.However,it can be showntheoretically that boosting only fails on fresh data ifthe individual classiﬁers aretoo “complex”for the amount oftraining data present or iftheir training errorsbecome too large too quickly (in a precise sense explained by Schapire et al.1997).As usual,the problem lies in ﬁnding the right balance between the indi-vidual models’complexity and their ﬁt to the data.Ifboosting succeeds in reducing the error on fresh test data,it often does soin a spectacular way.One very surprising ﬁnding is that performing more boosting iterations can reduce the error on new data long after the error ofthe combined classiﬁer on the training data has dropped to zero.Researcherswere puzzled by this result because it seems to contradict Occam’s razor,whichdeclares that oftwo hypotheses that explain the empirical evidence equally well the simpler one is to be preferred.Performing more boosting iterationswithout reducing training error does not explain the training data any better,and it certainly adds complexity to the combined classiﬁer.Fortunately,the con-tradiction can be resolved by considering the classiﬁer’s conﬁdence in its pre-dictions.Conﬁdence is measured by the difference between the estimatedprobability ofthe true class and that ofthe most likely predicted class other thanthe true class—a quantity known as the margin.The larger the margin,the moreconﬁdent the classiﬁer is in predicting the true class.It turns out that boostingcan increase the margin long after the training error has dropped to zero.Theeffect can be visualized by plotting the cumulative distribution ofthe marginvalues ofall the training instances for different numbers ofboosting iterations,giving a graph known as the margin curve.Hence,ifthe explanation ofempir-ical evidence takes the margin into account,Occam’s razor remains as sharp asever.324CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3247.5COMBINING MULTIPLE MODELS325The beautiful thing about boosting is that a powerful combined classiﬁer canbe built from very simple ones as long as they achieve less than 50% error onthe reweighted data.Usually,this is easy—certainly for learning problems withtwo classes! Simple learning methods are called weaklearners,and boosting con-verts weak learners into strong ones.For example,good results for two-classproblems can be obtained by boosting extremely simple decision trees that haveonly one level—called decision stumps.Another possibility is to apply boostingto an algorithm that learns a single conjunctive rule—such as a single path in adecision tree—and classiﬁes instances based on whether or not the rule coversthem.Ofcourse,multiclass datasets make it more difﬁcult to achieve error ratesbelow 0.5.Decision trees can still be boosted,but they usually need to be morecomplex than decision stumps.More sophisticated algorithms have been devel-oped that allow very simple models to be boosted successfully in multiclass situations.Boosting often produces classiﬁers that are signiﬁcantly more accurate onfresh data than ones generated by bagging.However,unlike bagging,boostingsometimes fails in practical situations:it can generate a classiﬁer that is signif-icantly less accurate than a single classiﬁer built from the same data.This indi-cates that the combined classiﬁer overﬁts the data.Additive regressionWhen boosting was ﬁrst investigated it sparked intense interest amongresearchers because it could coax ﬁrst-class performance from indifferent learn-ers.Statisticians soon discovered that it could be recast as a greedy algorithmfor ﬁtting an additive model.Additive models have a long history in statistics.Broadly,the term refers to any way ofgenerating predictions by summing upcontributions obtained from other models.Most learning algorithms for addi-tive models do not build the base models independently but ensure that theycomplement one another and try to form an ensemble ofbase models that opti-mizes predictive performance according to some speciﬁed criterion.Boosting implements forward stagewise additive modeling.This class ofalgo-rithms starts with an empty ensemble and incorporates new members sequen-tially.At each stage the model that maximizes the predictive performance oftheensemble as a whole is added,without altering those already in the ensemble.Optimizing the ensemble’s performance implies that the next model shouldfocus on those training instances on which the ensemble performs poorly.Thisis exactly what boosting does by giving those instances larger weights.Here’s a well-known forward stagewise additive modeling method fornumeric prediction.First build a standard regression model,for example,aregression tree.The errors it exhibits on the training data—the differencesbetween predicted and observed values—are called residuals.Then correct forP088407-Ch007.qxd  4/30/05  11:16 AM  Page 325these errors by learning a second model—perhaps another regression tree—thattries to predict the observed residuals.To do this,simply replace the originalclass values by their residuals before learning the second model.Adding the pre-dictions made by the second model to those ofthe ﬁrst one automatically yieldslower error on the training data.Usually some residuals still remain,becausethe second model is not a perfect one,so we continue with a third model thatlearns to predict the residuals ofthe residuals,and so on.The procedure is rem-iniscent ofthe use ofrules with exceptions for classiﬁcation that we met inSection 3.5.Ifthe individual models minimize the squared error ofthe predictions,aslinear regression models do,this algorithm minimizes the squared error ofthe ensemble as a whole.In practice it also works well when the base learneruses a heuristic approximation instead,such as the regression and model treelearners described in Section 6.5.In fact,there is no point in using standardlinear regression as the base learner for additive regression,because the sum oflinear regression models is again a linear regression model and the regressionalgorithm itselfminimizes the squared error.However,it is a different story ifthe base learner is a regression model based on a single attribute,the one thatminimizes the squared error.Statisticians call this simplelinear regression,incontrast to the standard multiattribute method,properly called multiplelinearregression.In fact,using additive regression in conjunction with simple linearregression and iterating until the squared error ofthe ensemble decreases nofurther yields an additive model identical to the least-squares multiple linearregression function.Forward stagewise additive regression is prone to overﬁtting because eachmodel added ﬁts the training data more closely.To decide when to stop,usecross-validation.For example,perform a cross-validation for every number ofiterations up to a user-speciﬁed maximum and choose the one that minimizesthe cross-validated estimate ofsquared error.This is a good stopping criterionbecause cross-validation yields a fairly reliable estimate ofthe error on futuredata.Incidentally,using this method in conjunction with simple linear regres-sion as the base learner effectively combines multiple linear regression withbuilt-in attribute selection,because the next most important attribute’s contri-bution is only included ifit decreases the cross-validated error.For implementation convenience,forward stagewise additive regressionusually begins with a level-0 model that simply predicts the mean ofthe classon the training data so that every subsequent model ﬁts residuals.This suggestsanother possibility for preventing overﬁtting:instead ofsubtracting a model’sentire prediction to generate target values for the next model,shrink the pre-dictions by multiplying them by a user-speciﬁed constant factor between 0 and1 before subtracting.This reduces the model’s ﬁt to the residuals and conse-quently reduces the chance ofoverﬁtting.Ofcourse,it may increase the number326CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3267.5COMBINING MULTIPLE MODELS327ofiterations needed to arrive at a good additive model.Reducing the multipliereffectively damps down the learning process,increasing the chance ofstoppingat just the right moment—but also increasing run time.Additive logistic regressionAdditive regression can also be applied to classiﬁcation just as linear regressioncan.But we know from Section 4.6 that logistic regression outperforms linearregression for classiﬁcation.It turns out that a similar adaptation can be madeto additive models by modifying the forward stagewise modeling method toperform additivelogistic regression.Use the logit transform to translate theprobability estimation problem into a regression problem,as we did in Section4.6,and solve the regression task using an ensemble ofmodels—for example,regression trees—just as for additive regression.At each stage,add the modelthat maximizes the probability ofthe data given the ensemble classiﬁer.Suppose fjis the jth regression model in the ensemble and fj(a) is its predic-tion for instance a.Assuming a two-class problem,use the additive model Sfj(a)to obtain a probability estimate for the ﬁrst class:This closely resembles the expression used in Section 4.6 (page 121),except thathere it is abbreviated by using vector notation for the instance aand the origi-nal weighted sum ofattribute values is replaced by a sum ofarbitrarily complexregression models f.Figure 7.9 shows the two-class version ofthe LogitBoostalgorithm,which per-forms additive logistic regression and generates the individual models fj.Here,yiis 1 for an instance in the ﬁrst class and 0 for an instance in the second.Ineach iteration this algorithm ﬁts a regression model fjto a weighted version ofpefj111aa()=+-()Âmodel generationFor j = 1 to t iterations:    For each instance a[i]:        Set the target value for the regression to            z[i] = (y[i] – p(1 | a[i])) / [p(1 | a[i]) ⫻ (1 – p(1 | a[i])]        Set the weight of instance a[i] to p(1 | a[i]) ⫻ (1 – p(1 | a[i])    Fit a regression model f[j] to the data with class values z[i] and weights w[i].classificationPredict first class if p(1 | a) > 0.5, otherwise predict second class.Figure 7.9Algorithm for additive logistic regression.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 327the original dataset based on dummy class values ziand weights wi.We assumethat p(1 |a) is computed using the fjthat were built in previous iterations.The derivation ofthis algorithm is beyond the scope ofthis book,but it canbe shown that the algorithm maximizes the probability ofthe data with respectto the ensemble ifeach model fjis determined by minimizing the squared erroron the corresponding regression problem.In fact,ifmultiple linear regressionis used to form the fj,the algorithm converges to the maximum likelihood linear-logistic regression model:it is an incarnation ofthe iteratively reweighted least-squares method mentioned in Section 4.6.Superﬁcially,LogitBoost looks quite different to AdaBoost,but the predictorsthey produce differ mainly in that the former optimizes the likelihood directlywhereas the latter optimizes an exponential loss function that can be regardedas an approximation to it.From a practical perspective,the difference is thatLogitBoost uses a regression method as the base learner whereas AdaBoostworks with classiﬁcation algorithms.We have only shown the two-class version ofLogitBoost,but the algorithmcan be generalized to multiclass problems.As with additive regression,thedanger ofoverﬁtting can be reduced by shrinking the predictions ofthe indi-vidual fjby a predetermined multiplier and using cross-validation to determinean appropriate number ofiterations.Option treesBagging,boosting,and randomization all produce ensembles ofclassiﬁers.Thismakes it very difﬁcult to analyze what kind ofinformation has been extractedfrom the data.It would be nice to have a single model with the same predictiveperformance.One possibility is to generate an artiﬁcial dataset,by randomlysampling points from the instance space and assigning them the class labels pre-dicted by the ensemble classiﬁer,and then learn a decision tree or rule set fromthis new dataset.To obtain similar predictive performance from the tree as fromthe ensemble a huge dataset may be required,but in the limit this strategyshould be able to replicate the performance ofthe ensemble classiﬁer—and itcertainly will ifthe ensemble itselfconsists ofdecision trees.Another approach is to derive a single structure that can represent an ensem-ble ofclassiﬁers compactly.This can be done ifthe ensemble consists ofdeci-sion trees;the result is called an option tree.Option trees differ from decisiontrees in that they contain two types ofnode:decision nodes and option nodes.Figure 7.10 shows a simple example for the weather data,with only one optionnode.To classify an instance,ﬁlter it down through the tree.At a decision nodetake just one ofthe branches,as usual,but at an option node take allthebranches.This means that the instance ends up in more than one leaf,and theclassiﬁcations obtained from those leaves must somehow be combined into an328CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3287.5COMBINING MULTIPLE MODELS329overall classiﬁcation.This can be done simply by voting,taking the majorityvote at an option node to be the prediction ofthe node.In that case it makeslittle sense to have option nodes with only two options (as in Figure 7.10)because there will only be a majority ifboth branches agree.Another pos-sibility is to average the probability estimates obtained from the different paths,using either an unweighted average or a more sophisticated Bayesianapproach.Option trees can be generated by modifying an existing decision tree learnerto create an option node ifthere are several splits that look similarly usefulaccording to their information gain.All choices within a certain user-speciﬁedtolerance ofthe best one can be made into options.During pruning,the errorofan option node is the average error ofits options.Another possibility is to grow an option tree by incrementally adding nodesto it.This is commonly done using a boosting algorithm,and the resulting treesare usually called alternating decision treesinstead ofoption trees.In this contextthe decision nodes are called splitter nodesand the option nodes are called pre-diction nodes.Prediction nodes are leaves ifno splitter nodes have been addedto them yet.The standard alternating decision tree applies to two-class prob-lems,and with each prediction node is associated a positive or negative numericvalue.To obtain a prediction for an instance,ﬁlter it down all applicablebranches and sum up the values from any prediction nodes that are encoun-tered;predict one class or the other depending on whether the sum is positiveor negative.≠ overcastoutlook= overcast= false= trueyesnoyeswindyoption node= normal= highnoyeshumidityFigure 7.10Simple option tree for the weather data.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 329A simple example tree for the weather data is shown in Figure 7.11,where apositive value corresponds to class play =noand a negative one to play =yes.To classify an instance with outlook =sunny,temperature =hot,humidity =normal,and windy =false,ﬁlter it down to the corresponding leaves,obtainingthe values -0.255,0.213,-0.430,and -0.331.The sum ofthese values is nega-tive;hence predict play =yes.Alternating decision trees always have a predic-tion node at the root,as in this example.The alternating tree is grown using a boosting algorithm—for example,aboosting algorithm that employs a base learner for numeric prediction,such asthe LogitBoost method described previously.Assume that the base learner pro-duces a single conjunctive rule in each boosting iteration.Then an alternatingdecision tree can be generated by simply adding each rule into the tree.Thenumeric scores associated with the prediction nodes are obtained from the rules.However,the resulting tree would grow large very quickly because the rules fromdifferent boosting iterations are likely to be different.Hence,learning algorithmsfor alternating decision trees consider only those rules that extend one oftheexisting paths in the tree by adding a splitter node and two corresponding pre-diction nodes (assuming binary splits).In the standard version ofthe algorithm,330CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUT≠ overcastoutlook= overcast= false= true–0.7050.437–0.331windy= normal= high0.486–0.43humidity0.213–0.255Figure 7.11Alternating decision tree for the weather data.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 3307.5COMBINING MULTIPLE MODELS331every possible location in the tree is considered for addition,and a node is addedaccording to a performance measure that depends on the particular boostingalgorithm employed.However,heuristics can be used instead ofan exhaustivesearch to speed up the learning process.Logistic model treesOption trees and alternating trees yield very good classiﬁcation performancebased on a single structure,but they may still be difﬁcult to interpret when thereare many options nodes because it becomes difﬁcult to see how a particular pre-diction is derived.However,it turns out that boosting can also be used to buildvery effective decision trees that do not include any options at all.For example,the LogitBoost algorithm has been used to induce trees with linear logisticregression models at the leaves.These are called logistic model treesand are interpreted in the same way as the model trees for regression described inSection 6.5.LogitBoost performs additive logistic regression.Suppose that each iterationofthe boosting algorithm ﬁts a simple regression function by going through allthe attributes,ﬁnding the simple regression function with the smallest error,and adding it into the additive model.Ifthe LogitBoost algorithm is run untilconvergence,the result is a maximum likelihood multiple-logistic regressionmodel.However,for optimum performance on future data it is usually unnec-essary to wait for convergence—and to do so is often detrimental.An appro-priate number ofboosting iterations can be determined by estimating theexpected performance for a given number ofiterations using cross-validationand stopping the process when performance ceases to increase.A simple extension ofthis algorithm leads to logistic model trees.The boost-ing process terminates when there is no further structure in the data that canbe modeled using a linear logistic regression function.However,there may stillbe a structure that linear models can ﬁt ifattention is restricted to subsets ofthe data,obtained,for example,by a standard decision tree criterion such asinformation gain.Then,once no further improvement can be obtained byadding more simple linear models,the data is split and boosting is resumed sep-arately in each subset.This process takes the logistic model generated so far andreﬁnes it separately for the data in each subset.Again,cross-validation is run ineach subset to determine an appropriate number ofiterations to perform in thatsubset.The process is applied recursively until the subsets become too small.Theresulting tree will surely overﬁt the training data,and one ofthe standardmethods ofdecision tree learning can be used to prune it.Experiments indicatethat the pruning operation is very important.Using a strategy that chooses theright tree size using cross-validation,the algorithm produces small but veryaccurate trees with linear logistic models at the leaves.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 331StackingStacked generalization,or stackingfor short,is a different way ofcombining mul-tiple models.Although developed some years ago,it is less widely used thanbagging and boosting,partly because it is difﬁcult to analyze theoretically andpartly because there is no generally accepted best way ofdoing it—the basic ideacan be applied in many different variations.Unlike bagging and boosting,stacking is not normally used to combinemodels ofthe same type—for example,a set ofdecision trees.Instead it isapplied to models built by different learning algorithms.Suppose you have adecision tree inducer,a Naïve Bayes learner,and an instance-based learningmethod and you want to form a classiﬁer for a given dataset.The usual proce-dure would be to estimate the expected error ofeach algorithm by cross-validation and to choose the best one to form a model for prediction on futuredata.But isn’t there a better way? With three learning algorithms available,can’twe use all three for prediction and combine the outputs together?One way to combine outputs is by voting—the same mechanism used inbagging.However,(unweighted) voting only makes sense ifthe learningschemes perform comparably well.Iftwo ofthe three classiﬁers make predic-tions that are grossly incorrect,we will be in trouble! Instead,stacking intro-duces the concept ofametalearner,which replaces the voting procedure.Theproblem with voting is that it’s not clear which classiﬁer to trust.Stacking triesto learnwhich classiﬁers are the reliable ones,using another learning algo-rithm—the metalearner—to discover how best to combine the output ofthebase learners.The input to the metamodel—also called the level-1 model—are the predic-tions ofthe base models,or level-0 models.A level-1 instance has as many attrib-utes as there are level-0 learners,and the attribute values give the predictions ofthese learners on the corresponding level-0 instance.When the stacked learneris used for classiﬁcation,an instance is ﬁrst fed into the level-0 models,and eachone guesses a class value.These guesses are fed into the level-1 model,whichcombines them into the ﬁnal prediction.There remains the problem oftraining the level-1 learner.To do this,we needto ﬁnd a way oftransforming the level-0 training data (used for training thelevel-0 learners) into level-1 training data (used for training the level-1 learner).This seems straightforward:let each level-0 model classify a training instance,and attach to their predictions the instance’s actual class value to yield a level-1 training instance.Unfortunately,this doesn’t work well.It would allow rulesto be learned such as always believe the output ofclassiﬁer A,and ignore B andC.This rule may well be appropriate for particular base classiﬁers A,B,and C;ifso,it will probably be learned.But just because it seems appropriate on thetraining data doesn’t necessarily mean that it will work well on the test data—332CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  5/3/05  2:31 PM  Page 3327.5COMBINING MULTIPLE MODELS333because it will inevitably learn to prefer classiﬁers that overﬁt the training dataover ones that make decisions more realistically.Consequently,stacking does not simply transform the level-0 training datainto level-1 data in this manner.Recall from Chapter 5 that there are bettermethods ofestimating a classiﬁer’s performance than using the error on thetraining set.One is to hold out some instances and use them for an independ-ent evaluation.Applying this to stacking,we reserve some instances to form thetraining data for the level-1 learner and build level-0 classiﬁers from the remain-ing data.Once the level-0 classiﬁers have been built they are used to classify theinstances in the holdout set,forming the level-1 training data as described pre-viously.Because the level-0 classiﬁers haven’t been trained on these instances,their predictions are unbiased;therefore the level-1 training data accuratelyreﬂects the true performance ofthe level-0 learning algorithms.Once the level-1 data has been generated by this holdout procedure,the level-0 learners can bereapplied to generate classiﬁers from the full training set,making slightly betteruse ofthe data and leading to better predictions.The holdout method inevitably deprives the level-1 model ofsome ofthetraining data.In Chapter 5,cross-validation was introduced as a means ofcir-cumventing this problem for error estimation.This can be applied in conjunc-tion with stacking by performing a cross-validation for every level-0 learner.Each instance in the training data occurs in exactly one ofthe test folds ofthecross-validation,and the predictions ofthe level-0 inducers built from the cor-responding training fold are used to build a level-1 training instance from it.This generates a level-1 training instance for each level-0 training instance.Ofcourse,it is slow because a level-0 classiﬁer has to be trained for each fold ofthecross-validation,but it does allow the level-1 classiﬁer to make full use ofthetraining data.Given a test instance,most learning methods are able to output probabilitiesfor every class label instead ofmaking a single categorical prediction.This canbe exploited to improve the performance ofstacking by using the probabilitiesto form the level-1 data.The only difference to the standard procedure is thateach nominal level-1 attribute—representing the class predicted by a level-0learner—is replaced by several numeric attributes,each representing a classprobability output by the level-0 learner.In other words,the number ofattrib-utes in the level-1 data is multiplied by the number ofclasses.This procedurehas the advantage that the level-1 learner is privy to the conﬁdence that eachlevel-0 learner associates with its predictions,thereby amplifying communica-tion between the two levels oflearning.An outstanding question remains:what algorithms are suitable for the level-1 learner? In principle,any learning scheme can be applied.However,becausemost ofthe work is already done by the level-0 learners,the level-1 classiﬁer isbasically just an arbiter and it makes sense to choose a rather simple algorithmP088407-Ch007.qxd  4/30/05  11:16 AM  Page 333for this purpose.In the words ofDavid Wolpert,the inventor ofstacking,it isreasonable that “relatively global,smooth”level-1 generalizers should performwell.Simple linear models or trees with linear models at the leaves usually workwell.Stacking can also be applied to numeric prediction.In that case,the level-0models and the level-1 model all predict numeric values.The basic mechanismremains the same;the only difference lies in the nature ofthe level-1 data.Inthe numeric case,each level-1 attribute represents the numeric prediction madeby one ofthe level-0 models,and instead ofa class value the numeric targetvalue is attached to level-1 training instances.Error-correcting output codesError-correcting output codes are a technique for improving the performanceofclassiﬁcation algorithms in multiclass learning problems.Recall from Chapter6 that some learning algorithms—for example,standard support vectormachines—only work with two-class problems.To apply such algorithms tomulticlass datasets,the dataset is decomposed into several independent two-class problems,the algorithm is run on each one,and the outputs ofthe result-ing classiﬁers are combined.Error-correcting output codes are a method formaking the most ofthis transformation.In fact,the method works so well thatit is often advantageous to apply it even when the learning algorithm can handlemulticlass datasets directly.In Section 4.6 (page 123) we learned how to transform a multiclass datasetinto several two-class ones.For each class,a dataset is generated containing acopy ofeach instance in the original data,but with a modiﬁed class value.Iftheinstance has the class associated with the corresponding dataset it is tagged yes;otherwise no.Then classiﬁers are built for each ofthese binary datasets,classi-ﬁers that output a conﬁdence ﬁgure with their predictions—for example,theestimated probability that the class is yes.During classiﬁcation,a test instanceis fed into each binary classiﬁer,and the ﬁnal class is the one associated with theclassiﬁer that predicts yesmost conﬁdently.Ofcourse,this method is sensitiveto the accuracy ofthe conﬁdence ﬁgures produced by the classiﬁers:ifsomeclassiﬁers have an exaggerated opinion oftheir own predictions,the overallresult will suffer.Consider a multiclass problem with the four classes a,b,c,and d.The trans-formation can be visualized as shown in Table 7.1(a),where yesand noaremapped to 1 and 0,respectively.Each ofthe original class values is convertedinto a 4-bit code word,1 bit per class,and the four classiﬁers predict the bits independently.Interpreting the classiﬁcation process in terms ofthese code words,errors occur when the wrong binary bit receives the highest conﬁdence.334CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3347.5COMBINING MULTIPLE MODELS335However,we do not have to use the particular code words shown.Indeed,there is no reason why each class must be represented by 4 bits.Look instead atthe code ofTable 7.1(b),where classes are represented by 7 bits.When appliedto a dataset,seven classiﬁers must be built instead offour.To see what that mightbuy,consider the classiﬁcation ofa particular instance.Suppose it belongs toclass a,and that the predictions ofthe individual classiﬁers are 1 0 1 1 1 1 1(respectively).Obviously,comparing this code word with those in Table 7.1(b),the second classiﬁer has made a mistake:it predicted 0 instead of1,noinsteadofyes.However,comparing the predicted bits with the code word associatedwith each class,the instance is clearly closer to athan to any other class.Thiscan be quantiﬁed by the number ofbits that must be changed to convert thepredicted code word into those ofTable 7.1(b):the Hamming distance,or thediscrepancy between the bit strings,is 1,3,3,and 5 for the classes a,b,c,and d,respectively.We can safely conclude that the second classiﬁer made a mistakeand correctly identify aas the instance’s true class.The same kind oferror correction is not possible with the code words ofTable7.1(a),because any predicted string of4 bits other than these four 4-bit wordshas the same distance to at least two ofthem.The output codes are not “errorcorrecting.”What determines whether a code is error correcting or not? Consider theHamming distance between the code words representing different classes.Thenumber oferrors that can possibly be corrected depends on the minimum dis-tance between any pair ofcode words,say d.The code can guarantee to correctup to (d-1)/2 1-bit errors,because ifthis number ofbits ofthe correct code word are ﬂipped,it will still be the closest and will therefore be identiﬁedcorrectly.In Table 7.1(a) the Hamming distance for each pair ofcode words is2.Hence,the minimum distance dis also 2,and we can correct no more than0 errors! However,in the code ofTable 7.1(b) the minimum distance is 4 (infact,the distance is 4 for all pairs).That means it is guaranteed to correct 1-biterrors.Table 7.1Transforming a multiclass problem into a two-class one: (a) standard method and (b) error-correcting code.ClassClass vectorClassClass vectora1 0 0 0a1 1 1 1 1 1 1b0 1 0 0b0 0 0 0 1 1 1c0 0 1 0c0 0 1 1 0 0 1d0 0 0 1d0 1 0 1 0 1 0(a)(b)P088407-Ch007.qxd  4/30/05  11:16 AM  Page 335We have identiﬁed one property ofa good error-correcting code:the codewords must be well separated in terms oftheir Hamming distance.Because theycomprise the rows ofthe code table,this property is called row separation.Thereis a second requirement that a good error-correcting code should fulﬁll:columnseparation.The Hamming distance between every pair ofcolumns must belarge,as must the distance between each column and the complement ofeveryother column.In Table 7.1(b),the seven columns are separated from oneanother (and their complements) by at least 1 bit.Column separation is necessary because iftwo columns are identical (or ifone is the complement ofanother),the corresponding classiﬁers will make the same errors.Error correction is weakened ifthe errors are correlated—in other words,ifmany bit positions are simultaneously incorrect.The greater the distance between columns,the more errors are likely to be corrected.With fewer than four classes it is impossible to construct an effective error-correcting code because good row separation and good column separa-tion cannot be achieved simultaneously.For example,with three classes there are only eight possible columns (23),four ofwhich are complements ofthe other four.Moreover,columns with all zeroes or all ones provide no discrimination.This leaves just three possible columns,and the resultingcode is not error correcting at all.(In fact,it is the standard “one-per-class”encoding.)Ifthere are few classes,an exhaustive error-correcting code such as the onein Table 7.1(b) can be built.In an exhaustive code for kclasses,the columnscomprise every possible k-bit string,except for complements and the trivial all-zero or all-one strings.Each code word contains 2k-1-1 bits.The code is con-structed as follows:the code word for the ﬁrst class consists ofall ones;that forthe second class has 2k-2zeroes followed by 2k-2-1 ones;the third has 2k-3zeroesfollowed by 2k-3ones followed by 2k-3zeroes followed by 2k-3-1 ones;and soon.The ith code word consists ofalternating runs of2k-izeroes and ones,thelast run being one short.With more classes,exhaustive codes are infeasible because the number ofcolumns increases exponentially and too many classiﬁers have to be built.Inthat case more sophisticated methods are employed,which can build a code withgood error-correcting properties from a smaller number ofcolumns.Error-correcting output codes do not work for local learning algorithms suchas instance-based learners,which predict the class ofan instance by looking atnearby training instances.In the case ofa nearest-neighbor classiﬁer,all outputbits would be predicted using the same training instance.The problem can becircumvented by using different attribute subsets to predict each output bit,decorrelating the predictions.336CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3367.6USING UNLABELED DATA3377.6Using unlabeled dataWhen introducing the machine learning process in Chapter 2 we drew a sharpdistinction between supervised and unsupervised learning—classiﬁcation andclustering.Recently researchers have begun to explore territory between the two,sometimes called semisupervised learning,in which the goal is classiﬁcation butthe input contains both unlabeled and labeled data.You can’t do classiﬁcationwithout labeled data,ofcourse,because only the labels tell what the classes are.But it is sometimes attractive to augment a small amount oflabeled data witha large pool ofunlabeled data.It turns out that the unlabeled data can help youlearn the classes.How can this be?First,why would you want it? Many situations present huge volumes ofrawdata,but assigning classes is expensive because it requires human insight.Textmining provides some classic examples.Suppose you want to classify Web pagesinto predeﬁned groups.In an academic setting you might be interested in facultypages,graduate student pages,course information pages,research group pages,and department pages.You can easily download thousands,or millions,ofrel-evant pages from university Web sites.But labeling the training data is a labo-rious manual process.Or suppose your job is to use machine learning to spotnames in text,differentiating among personal names,company names,andplace names.You can easily download megabytes,or gigabytes,oftext,butmaking this into training data by picking out the names and categorizing themcan only be done manually.Cataloging news articles,sorting electronic mail,learning users’reading interests—applications are legion.Leaving text aside,suppose you want to learn to recognize certain famous people in televisionbroadcast news.You can easily record hundreds or thousands ofhours ofnews-casts,but again labeling is manual.In any ofthese scenarios it would be enor-mously attractive to be able to leverage a large pool ofunlabeled data to obtainexcellent performance from just a few labeled examples—particularly ifyouwere the graduate student who had to do the labeling!Clustering for classiﬁcationHow can unlabeled data be used to improve classiﬁcation? Here’s a simple idea.Use Naïve Bayes to learn classes from a small labeled dataset,and then extendit to a large unlabeled dataset using the EM (expectation–maximization) itera-tive clustering algorithm ofSection 6.6.The procedure is this.First,train a clas-siﬁer using the labeled data.Second,apply it to the unlabeled data to label itwith class probabilities (the “expectation”step).Third,train a new classiﬁerusing the labels for all the data (the “maximization”step).Fourth,iterate untilconvergence.You could think ofthis as iterative clustering,where starting pointsP088407-Ch007.qxd  4/30/05  11:16 AM  Page 337and cluster labels are gleaned from the labeled data.The EM procedure guar-antees to ﬁnd model parameters that have equal or greater likelihood at eachiteration.The key question,which can only be answered empirically,is whetherthese higher likelihood parameter estimates will improve classiﬁcation accuracy.Intuitively,this might work well.Consider document classiﬁcation.Certainphrases are indicative ofthe classes.Some occur in labeled documents,whereasothers only occur in unlabeled ones.But there are probably some documentsthat contain both,and the EM procedure uses these to generalize the learnedmodel to utilize phrases that do not appear in the labeled dataset.For example,both supervisorand PhD topicmight indicate a graduate student’s home page.Suppose that only the former phrase occurs in the labeled documents.EM iter-atively generalizes the model to correctly classify documents that contain justthe latter.This might work with any classiﬁer and any iterative clustering algorithm.But it is basically a bootstrapping procedure,and you must take care to ensurethat the feedback loop is a positive one.Using probabilities rather than harddecisions seems beneﬁcial because it allows the procedure to converge slowlyinstead ofjumping to conclusions that may be wrong.Naïve Bayes and the prob-abilistic EM procedure described in Section 6.6 are particularly apt choicesbecause they share the same fundamental assumption:independence betweenattributes—or,more precisely,conditional independence between attributesgiven the class.Ofcourse,the independence assumption is universally violated.Even ourlittle example used the two-word phrase PhD topic,whereas actual implemen-tations would likely use individual words as attributes—and the example wouldhave been far less compelling ifwe had substituted either ofthe single termsPhDor topic.The phrase PhD studentsis probably more indicative offacultythan graduate student home pages;the phrase research topicis probably less dis-criminating.It is the very fact that PhDand topicare notconditionally inde-pendent given the class that makes the example work:it is their combinationthat characterizes graduate student pages.Nevertheless,coupling Naïve Bayes and EM in this manner works well in thedomain ofdocument classiﬁcation.In a particular classiﬁcation task it attainedthe performance ofa traditional learner using fewer than one-third ofthelabeled training instances,as well as ﬁve times as many unlabeled ones.This isa good tradeoffwhen labeled instances are expensive but unlabeled ones are vir-tually free.With a small number oflabeled documents,classiﬁcation accuracycan be improved dramatically by incorporating many unlabeled ones.Two reﬁnements to the procedure have been shown to improve performance.The ﬁrst is motivated by experimental evidence that when there are manylabeled documents the incorporation ofunlabeled data may reduce rather thanincrease accuracy.Hand-labeled data is (or should be) inherently less noisy than338CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 3387.6USING UNLABELED DATA339automatically labeled data.The solution is to introduce a weighting parameterthat reduces the contribution ofthe unlabeled data.This can be incorporatedinto the maximization step ofEM by maximizing the weighted likelihood ofthelabeled and unlabeled instances.When the parameter is close to zero,unlabeleddocuments have little inﬂuence on the shape ofEM’s hill-climbing surface;when close to one,the algorithm reverts to the original version in which thesurface is equally affected by both kinds ofdocument.The second reﬁnement is to allow each class to have several clusters.Asexplained in Section 6.6,the EM clustering algorithm assumes that the data isgenerated randomly from a mixture ofdifferent probability distributions,oneper cluster.Until now,a one-to-one correspondence between mixture compo-nents and classes has been assumed.In many circumstances this is unrealistic—including document classiﬁcation,because most documents address multipletopics.With several clusters per class,each labeled document is initially assignedrandomly to each ofits components in a probabilistic fashion.The maximiza-tion step ofthe EM algorithm remains as before,but the expectation step ismodiﬁed to not only probabilistically label each example with the classes,butto probabilistically assign it to the components within the class.The number ofclusters per class is a parameter that depends on the domain and can be set bycross-validation.Co-trainingAnother situation in which unlabeled data can improve classiﬁcation perform-ance is when there are two different and independent perspectives on the clas-siﬁcation task.The classic example again involves documents,this time Webdocuments,in which the two perspectives are the contentofa Web page and thelinksto it from other pages.These two perspectives are well known to be bothuseful and different:successful Web search engines capitalize on them both,using secret recipes.The text that labels a link to another Web page gives arevealing clue as to what that page is about—perhaps even more revealing thanthe page’s own content,particularly ifthe link is an independent one.Intuitively,a link labeled my adviseris strong evidence that the target page is a facultymember’s home page.The idea,called co-training,is this.Given a few labeled examples,ﬁrst learna different model for each perspective—in this case a content-based and ahyperlink-based model.Then use each one separately to label the unlabeledexamples.For each model,select the example it most conﬁdently labels as pos-itive and the one it most conﬁdently labels as negative,and add these to the pooloflabeled examples.Better yet,maintain the ratio ofpositive and negative exam-ples in the labeled pool by choosing more ofone kind than the other.In eithercase,repeat the whole procedure,training both models on the augmented pooloflabeled examples,until the unlabeled pool is exhausted.P088407-Ch007.qxd  4/30/05  11:16 AM  Page 339There is some experimental evidence,using Naïve Bayes throughout as thelearner,that this bootstrapping procedure outperforms one that employs all thefeatures from both perspectives to learn a single model from the labeled data.It relies on having two different views ofan instance that are redundant but notcompletely correlated.Various domains have been proposed,from spottingcelebrities in televised newscasts using video and audio separately to mobilerobots with vision,sonar,and range sensors.The independence ofthe viewsreduces the likelihood ofboth hypotheses agreeing on an erroneous label.EM and co-trainingOn datasets with two feature sets that are truly independent,experiments haveshown that co-training gives better results than using EM as described previ-ously.Even better performance,however,can be achieved by combining the twointo a modiﬁed version ofco-training called co-EM.Co-training trains two clas-siﬁers representing different perspectives,A and B,and uses both to add newexamples to the training pool by choosing whichever unlabeled examples theyclassify most positively or negatively.The new examples are few in number anddeterministically labeled.Co-EM,on the other hand,trains perspective A on thelabeled data and uses it to probabilistically label all unlabeled data.Next it trainsclassiﬁer B on both the labeled data and the unlabeled data with classiﬁer A’stentative labels,and then it probabilistically relabels all the data for use by clas-siﬁer A.The process iterates until the classiﬁers converge.This procedure seemsto perform consistently better than co-training because it does not commit tothe class labels that are generated by classiﬁers A and B but rather reestimatestheir probabilities at each iteration.The range ofapplicability ofco-EM,like co-training,is still limited by therequirement for multiple independent perspectives.But there is some experi-mental evidence to suggest that even when there is no natural split offeaturesinto independent perspectives,beneﬁts can be achieved by manufacturing sucha split and using co-training—or,better yet,co-EM—on the split data.Thisseems to work even when the split is made randomly;performance could surelybe improved by engineering the split so that the feature sets are maximally independent.Why does this work? Researchers have hypothesized that thesealgorithms succeed partly because the split makes them more robust to theassumptions that their underlying classiﬁers make.There is no particular reason to restrict the base classiﬁer to Naïve Bayes.Support vector machines probably represent the most successful technology fortext categorization today.However,for the EM iteration to work it is necessarythat the classiﬁer labels the data probabilistically;it must also be able to useprobabilistically weighted examples for training.Support vector machines caneasily be adapted to do both.We explained how to adapt learning algorithms to340CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 340deal with weighted instances in Section 6.5 under Locally weighted linear regres-sion(page 252).One way ofobtaining probability estimates from support vectormachines is to ﬁt a one-dimensional logistic model to the output,effectivelyperforming logistic regression as described in Section 4.6 on the output.Excel-lent results have been reported for text classiﬁcation using co-EM with thesupport vector machine (SVM) classiﬁer.It outperforms other variants ofSVMand seems quite robust to varying proportions oflabeled and unlabeled data.The ideas ofco-training and EM—and particularly their combination in the co-EM algorithm—are interesting,thought provoking,and have striking poten-tial.But just what makes them work is still controversial and poorly understood.These techniques are the subject ofcurrent research:they have not yet enteredthe mainstream ofmachine learning and been harnessed for practical datamining.7.7Further readingAttribute selection,under the term feature selection,has been investigated in theﬁeld ofpattern recognition for decades.Backward elimination,for example,wasintroduced in the early 1960s (Marill and Green 1963).Kittler (1978) surveysthe feature selection algorithms that have been developed for pattern recogni-tion.Best-ﬁrst search and genetic algorithms are standard artiﬁcial intelligencetechniques (Winston 1992,Goldberg 1989).The experiments that show the performance ofdecision tree learners deteri-orating when new attributes are added are reported by John (1997),who givesa nice explanation ofattribute selection.The idea ofﬁnding the smallest attrib-ute set that carves up the instances uniquely is from Almuallin and Dietterich(1991,1992) and was further developed by Liu and Setiono (1996).Kibler andAha (1987) and Cardie (1993) both investigated the use ofdecision tree algo-rithms to identify features for nearest-neighbor learning;Holmes and Nevill-Manning (1995) used 1R to order features for selection.Kira and Rendell (1992)used instance-based methods to select features,leading to a scheme calledRELIEFfor Recursive Elimination ofFeatures.Gilad-Bachrach et al.(2004) showhow this scheme can be modiﬁed to work better with redundant attributes.Thecorrelation-based feature selection method was developed by Hall (2000).The use ofwrapper methods for feature selection is due to John et al.(1994)and Kohavi and John (1997),and genetic algorithms have been applied withina wrapper framework by Vafaie and DeJong (1992) and Cherkauer and Shavlik(1996).The selective Naïve Bayes learning method is due to Langley and Sage(1994).Guyon et al.(2002) present and evaluate the recursive feature elimina-tion scheme in conjunction with support vector machines.The method ofracedsearch was developed by Moore and Lee (1994).7.7FURTHER READING341P088407-Ch007.qxd  4/30/05  11:16 AM  Page 341Dougherty et al.(1995) give a briefaccount ofsupervised and unsuperviseddiscretization,along with experimental results comparing the entropy-basedmethod with equal-width binning and the 1R method.Frank and Witten (1999)describe the effect ofusing the ordering information in discretized attributes.Proportional k-interval discretization for Naïve Bayes was proposed by Yang andWebb (2001).The entropy-based method for discretization,including the useofthe MDL stopping criterion,was developed by Fayyad and Irani (1993).Thebottom-up statistical method using the c2test is due to Kerber (1992),and itsextension to an automatically determined signiﬁcance level is described by Liuand Setiono (1997).Fulton et al.(1995) investigate the use ofdynamic pro-gramming for discretization and derive the quadratic time bound for a generalimpurity function (e.g.,entropy) and the linear one for error-based discretiza-tion.The example used for showing the weakness oferror-based discretizationis adapted from Kohavi and Sahami (1996),who were the ﬁrst to clearly iden-tify this phenomenon.Principal components analysis is a standard technique that can be found inmost statistics textbooks.Fradkin and Madigan (2003) analyze the performanceofrandom projections.The TF ¥IDF metric is described by Witten et al.(1999b).The experiments on using C4.5 to ﬁlter its own training data were reportedby John (1995).The more conservative approach ofa consensus ﬁlter involvingseveral learning algorithms has been investigated by Brodley and Friedl (1996).Rousseeuw and Leroy (1987) describe the detection ofoutliers in sta-tistical regression,including the least median ofsquares method;they alsopresent the telephone data ofFigure 7.6.It was Quinlan (1986) who noticedthat removing noise from the training instance’s attributes can decrease a classiﬁer’s performance on similarly noisy test instances,particularly at highernoise levels.Combining multiple models is a popular research topic in machine learningresearch,with many related publications.The term bagging(for “bootstrapaggregating”) was coined by Breiman (1996b),who investigated the propertiesofbagging theoretically and empirically for both classiﬁcation and numeric pre-diction.Domingos (1999) introduced the MetaCost algorithm.Randomizationwas evaluated by Dietterich (2000) and compared with bagging and boosting.Bay (1999) suggests using randomization for ensemble learning with nearest-neighbor classiﬁers.Random forests were introduced by Breiman (2001).Freund and Schapire (1996) developed the AdaBoost.M1 boosting algorithmand derived theoretical bounds for its performance.Later,they improved thesebounds using the concept ofmargins (Freund and Schapire 1999).Drucker(1997) adapted AdaBoost.M1 for numeric prediction.The LogitBoost algorithmwas developed by Friedman et al.(2000).Friedman (2001) describes how tomake boosting more resilient in the presence ofnoisy data.342CHAPTER 7|TRANSFORMATIONS:ENGINEERING THE INPUT AND OUTPUTP088407-Ch007.qxd  4/30/05  11:16 AM  Page 342Domingos (1997) describes how to derive a single interpretable model froman ensemble using artiﬁcial training examples.Bayesian option trees were intro-duced by Buntine (1992),and majority voting was incorporated into optiontrees by Kohavi and Kunz (1997).Freund and Mason (1999) introduced alter-nating decision trees;experiments with multiclass alternating decision treeswere reported by Holmes et al.(2002).Landwehr et al.(2003) developed logis-tic model trees using the LogitBoost algorithm.Stacked generalization originated with Wolpert (1992),who presented theidea in the neural network literature,and was applied to numeric prediction byBreiman (1996a).Ting and Witten (1997a) compared different level-1 modelsempirically and found that a simple linear model performs best;they alsodemonstrated the advantage ofusing probabilities as level-1 data.A combina-tion ofstacking and bagging has also been investigated (Ting and Witten1997b).The idea ofusing error-correcting output codes for classiﬁcation gained wideacceptance after a paper by Dietterich and Bakiri (1995);Ricci and Aha (1998)showed how to apply such codes to nearest-neighbor classiﬁers.Blum and Mitchell (1998) pioneered the use ofco-training and developed atheoretical model for the use oflabeled and unlabeled data from different inde-pendent perspectives.Nigam and Ghani (2000) analyzed the effectiveness andapplicability ofco-training,relating it to the traditional use ofstandard EM toﬁll in missing values.They also introduced the co-EM algorithm.Nigam et al.(2000) thoroughly explored how the EM clustering algorithm can use unlabeleddata to improve an initial classiﬁer built by Naïve Bayes,as reported in the Clustering for classiﬁcationsection.Up to this point,co-training and co-EM were applied mainly to small two-class problems;Ghani (2002) used error-correcting output codes to address multiclass situations with many classes.Brefeld and Scheffer (2004) extended co-EM to use a support vector machinerather than Naïve Bayes.Seeger (2001) casts some doubt on whether these newalgorithms really do have anything to offer over traditional ones,properly used.7.7FURTHER READING343P088407-Ch007.qxd  4/30/05  11:16 AM  Page 343P088407-Ch007.qxd  4/30/05  11:16 AM  Page 344Machine learning is a burgeoning new technology for mining knowledge fromdata,a technology that a lot ofpeople are beginning to take seriously.We don’twant to oversell it.The kind ofmachine learning we know is not about the big problems:futuristic visions ofautonomous robot servants,philosophicalconundrums ofconsciousness,metaphysical issues offree will,evolutionary—or theological—questions ofwhere intelligence comes from,linguistic debatesover language learning,psychological theories ofchild development,or cogni-tive explanations ofwhat intelligence is and how it works.For us,it’s far moreprosaic:machine learning is about algorithms for inferring structure from dataand ways ofvalidating that structure.These algorithms are not abstruse andcomplicated,but they’re not completely obvious and trivial either.Looking forward,the main challenge ahead is applications.Opportunitiesabound.Wherever there is data,things can be learned from it.Whenever there is too much data for people to pore over themselves,the mechanics oflearning will have to be automatic.But the inspiration will certainly not be auto-matic! Applications will come not from computer programs,nor from machinechapter8Moving on:Extensions and Applications345P088407-Ch008.qxd  4/30/05  11:14 AM  Page 345learning experts,nor from the data itself,but from the people who work withthe data and the problems from which it arises.That is why we have written this book,and the Weka system described in Part II—to empower those whoare not machine learning experts to apply these techniques to the problems thatarise in daily working life.The ideas are simple.The algorithms are here.Therest is really up to you!Ofcourse,development ofthe technology is certainly not ﬁnished.Machinelearning is a hot research topic,and new ideas and techniques continuallyemerge.To give a ﬂavor ofthe scope and variety ofresearch fronts,we close PartI by looking at some topical areas in the world ofdata mining.8.1Learning from massive datasetsThe enormous proliferation ofvery large databases in today’s companies andscientiﬁc institutions makes it necessary for machine learning algorithms tooperate on massive datasets.Two separate dimensions become critical when anyalgorithm is applied to very large datasets:space and time.Suppose the data is so large that it cannot be held in main memory.Thiscauses no difﬁculty ifthe learning scheme works in an incremental fashion,processing one instance at a time when generating the model.An instance canbe read from the input ﬁle,the model can be updated,the next instance can beread,and so on—without ever holding more than one training instance in mainmemory.Normally,the resulting model is small compared with the dataset size,and the amount ofavailable memory does not impose any serious constrainton it.The Naïve Bayes method is an excellent example ofthis kind ofalgorithm;there are also incremental versions ofdecision tree inducers and rule learningschemes.However,incremental algorithms for some ofthe learning methodsdescribed in this book have not yet been developed.Other methods,such asbasic instance-based schemes and locally weighted regression,need access to allthe training instances at prediction time.In that case,sophisticated caching andindexing mechanisms have to be employed to keep only the most frequentlyused parts ofa dataset in memory and to provide rapid access to relevantinstances in the ﬁle.The other critical dimension when applying learning algorithms to massivedatasets is time.Ifthe learning time does not scale linearly (or almost linearly)with the number oftraining instances,it will eventually become infeasible toprocess very large datasets.In some applications the number ofattributes is acritical factor,and only methods that scale linearly in the number ofattributesare acceptable.Alternatively,prediction time might be the crucial issue.Fortu-nately,there are many learning algorithms that scale gracefully during bothtraining and testing.For example,the training time for Naïve Bayes is linear in346CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 346both the number ofinstances and the number ofattributes.For top-down deci-sion tree inducers,we saw in Section 6.1 (pages 196–198) that training time islinear in the number ofattributes and,ifthe tree is uniformly bushy,log-linearin the number ofinstances (ifsubtree raising is not used or,ifit is,with a further log factor).When a dataset is too large for a particular learning algorithm to be applied,there are three ways to make learning feasible.The ﬁrst is trivial:instead ofapplying the scheme to the full dataset,use just a small subset for training.Ofcourse,information is lost when subsampling is employed.However,the lossmay be negligible because the predictive performance ofa learned model oftenﬂattens out long before all the training data is incorporated into it.Ifthis is thecase,it can easily be veriﬁed by observing the model’s performance on a holdouttest set for training sets ofdifferent size.This kind ofbehavior,called the law ofdiminishing returns,may arise becausethe learning problem is a simple one,so that a small volume oftraining data issufﬁcient to learn an accurate model.Alternatively,the learning algorithm mightbe incapable ofgrasping the detailed structure ofthe underlying domain.Thisis often observed when Naïve Bayes is employed in a complex domain:addi-tional training data may not improve the performance ofthe model,whereas a decision tree’s accuracy may continue to climb.In this case,ofcourse,ifpredictive performance is the main objective you should switch to the morecomplex learning algorithm.But beware ofoverﬁtting! Take care not to assessperformance on the training data.Parallelization is another way ofreducing the time complexity oflearning.The idea is to split the problem into smaller parts,solve each using a separateprocessor,and combine the results together.To do this,a parallelized version ofthe learning algorithm must be created.Some algorithms lend themselvesnaturally to parallelization.Nearest-neighbor methods,for example,can easilybe distributed among several processors by splitting the data into several partsand letting each processor ﬁnd the nearest neighbor in its part ofthe trainingset.Decision tree learners can be parallelized by letting each processor build asubtree ofthe complete tree.Bagging and stacking (although not boosting) arenaturally parallel algorithms.However,parallelization is only a partial remedybecause with a ﬁxed number ofprocessors,the algorithm’s asymptotic timecomplexity cannot be improved.A simple way to apply any algorithm to a large dataset is to split the data intochunks oflimited size and learn models separately for each one,combining theresult using voting or averaging.Either a parallel bagging-like scheme or asequential boosting-like scheme can be employed for this purpose.Boosting hasthe advantage that new chunks can be weighted based on the classiﬁers learnedfrom previous chunks,thus transferring knowledge between chunks.In bothcases memory consumption increases linearly with dataset size;hence some8.1LEARNING FROM MASSIVE DATASETS347P088407-Ch008.qxd  4/30/05  11:14 AM  Page 347form ofpruning is necessary for very large datasets.This can be done by settingaside some validation data and only adding a model from a new chunk to thecommittee classiﬁer ifit increases the committee’s performance on the valida-tion set.The validation set can also be used to identify an appropriate chunksize by running the method with several different chunk sizes in parallel andmonitoring performance on the validation set.The best but most challenging way to enable a learning paradigm to deal withvery large datasets would be to develop new algorithms with lower computa-tional complexity.In some cases,it is provably impossible to derive exact algo-rithms with lower complexity.Decision tree learners that deal with numericattributes fall into this category.Their asymptotic time complexity is dominatedby the sorting process for the numeric attribute values,a procedure that mustbe performed at least once for any given dataset.However,stochastic algorithmscan sometimes be derived that approximate the true solution but require a muchsmaller amount oftime.Background knowledge can make it possible to vastly reduce the amount ofdata that needs to be processed by a learning algorithm.Depending on whichattribute is the class,most ofthe attributes in a huge dataset might turn out tobe irrelevant when background knowledge is taken into account.As usual,itpays to carefully engineer the data that is passed to the learning scheme andmake the greatest possible use ofany prior information about the learningproblem at hand.Ifinsufﬁcient background knowledge is available,the attrib-ute ﬁltering algorithms described in Section 7.1 can often drastically reduce theamount ofdata—possibly at the expense ofa minor loss in predictive per-formance.Some ofthese—for example,attribute selection using decision treesor the 1R learning scheme—are linear in the number ofattributes.Just to give you a feeling for the amount ofdata that can be handled bystraightforward implementations ofmachine learning algorithms on ordinarymicrocomputers,we ran the decision tree learner J4.8 on a dataset with 600,000instances,54 attributes (10 numeric and 44 binary),and a class with sevenvalues.We used a Pentium 4 processor with a 2.8-GHz clock and a Java virtualmachine with a “just-in-time compiler.”It took 40 minutes to load the data ﬁle,build the tree using reduced-error pruning,and classify all the traininginstances.The tree had 20,000 nodes.Note that this implementation is writtenin Java,and executing a Java program is often several times slower than runninga corresponding program written in C because the Java byte-code must be trans-lated into machine code before it can be executed.(In our experience the difference is usually a factor ofthree to ﬁve ifthe virtual machine uses a just-in-time compiler.)There are datasets today that truly deserve the adjective massive.Scientiﬁcdatasets from astrophysics,nuclear physics,earth science,and molecular biologyare measured in hundreds ofgigabytes—or even terabytes.So are datasets 348CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 348containing records ofﬁnancial transactions.Application ofstandard programsfor machine learning to such datasets in their entirety is a very challengingproposition.8.2Incorporating domain knowledgeThroughout this book we have emphasized the importance ofgetting to knowyour data when undertaking practical data mining.Knowledge ofthe domainis absolutely essential for success.Data about data is often called metadata,andone ofthe frontiers in machine learning is the development ofschemes to allowlearning methods to take metadata into account in a useful way.You don’t have to look far for examples ofhow metadata might be applied.In Chapter 2 we divided attributes into nominal and numeric.But we also notedthat many ﬁner distinctions are possible.Ifan attribute is numeric an orderingis implied,but sometimes there is a zero point and sometimes not (for timeintervals there is,but for dates there is not).Even the ordering may be nonstandard:angular degrees have an ordering different from that ofintegersbecause 360°is the same as 0°and 180°is the same as -180°or indeed 900°.Discretization schemes assume ordinary linear ordering,as do learning schemesthat accommodate numeric attributes,but it would be a routine matter toextend them to circular orderings.Categorical data may also be ordered.Imagine how much more difﬁcult our lives would be ifthere were no conven-tional ordering for letters ofthe alphabet.(Looking up a listing in the HongKong telephone directory presents an interesting and nontrivial problem!) Andthe rhythms ofeveryday life are reﬂected in circular orderings:days ofthe week,months ofthe year.To further complicate matters there are many other kindsofordering,such as partial orderings on subsets:subset A may include subsetB,subset B may include subset A,or neither may include the other.Extendingordinary learning schemes to take account ofthis kind ofinformation in a satisfactory and general way is an open research problem.Metadata often involves relations among attributes.Three kinds ofrelationscan be distinguished:semantic,causal,and functional.A semanticrelationbetween two attributes indicates that ifthe ﬁrst is included in a rule,the secondshould be,too.In this case,it is known a priori that the attributes only makesense together.For example,in agricultural data that we have analyzed,anattribute called milk productionmeasures how much milk an individual cowproduces,and the purpose ofour investigation meant that this attribute had asemantic relationship with three other attributes,cow-identiﬁer,herd-identiﬁer,and farmer-identiﬁer.In other words,a milk production value can only beunderstood in the context ofthe cow that produced the milk,and the cow isfurther linked to a speciﬁc herd owned by a given farmer.Semantic relations8.2INCORPORATING DOMAIN KNOWLEDGE349P088407-Ch008.qxd  4/30/05  11:14 AM  Page 349are,ofcourse,problem dependent:they depend not just on the dataset but alsoon what you are trying to do with it.Causalrelations occur when one attribute causes another.In a system that istrying to predict an attribute caused by another,we know that the other attrib-ute must be included to make the prediction meaningful.For example,in theagricultural data mentioned previously there is a chain from the farmer,herd,and cow identiﬁers,through measured attributes such as milk production,downto the attribute that records whether a particular cow was retained or sold bythe farmer.Learned rules should recognize this chain ofdependence.Functionaldependencies occur in many databases,and the people who createdatabases strive to identify them for the purpose ofnormalizing the relations inthe database.When learning from the data,the signiﬁcance ofa functionaldependency ofone attribute on another is that ifthe latter is used in a rule thereis no need to consider the former.Learning schemes often rediscover functionaldependencies that are already known.Not only does this generate meaningless,or more accurately tautological,rules,but also other,more interesting patternsmay be obscured by the functional relationships.However,there has been muchwork in automatic database design on the problem ofinferring functionaldependencies from example queries,and the methods developed should proveuseful in weeding out tautological rules generated by learning schemes.Taking these kinds ofmetadata,or prior domain knowledge,into accountwhen doing induction using any ofthe algorithms we have met does not seemto present any deep or difﬁcult technical challenges.The only real problem—and it is a big one—is how to express the metadata in a general and easily under-standable way so that it can be generated by a person and used by the algorithm.It seems attractive to couch the metadata knowledge in just the same repre-sentation as the machine learning scheme generates.We focus on rules,whichare the norm for much ofthis work.The rules that specify metadata correspondto prior knowledge ofthe domain.Given training examples,additional rules canbe derived by one ofthe rule induction schemes we have already met.In thisway,the system might be able to combine “experience”(from examples) with“theory”(from domain knowledge).It would be capable ofconﬁrming andmodifying its programmed-in knowledge based on empirical evidence.Looselyput,the user tells the system what he or she knows,gives it some examples,andit ﬁgures the rest out for itself!To make use ofprior knowledge expressed as rules in a sufﬁciently ﬂexibleway,it is necessary for the system to be able to perform logical deduction.Otherwise,the knowledge has to be expressed in precisely the right form for thelearning algorithm to take advantage ofit,which is likely to be too demandingfor practical use.Consider causal metadata:ifA causes B and B causes C,thenwe would like the system to deduce that A causes C rather than having to statethat fact explicitly.Although in this simple example explicitly stating the new350CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 350fact presents little problem,in practice,with extensive metadata,it will be unrealistic to expect the system’s users to express all logical consequences oftheir prior knowledge.A combination ofdeduction from prespeciﬁed domain knowledge andinduction from training examples seems like a ﬂexible way ofaccommodatingmetadata.At one extreme,when examples are scarce (or nonexistent),deduc-tion is the prime (or only) means ofgenerating new rules.At the other,whenexamples are abundant but metadata is scarce (or nonexistent),the standardmachine learning techniques described in this book sufﬁce.Practical situationsspan the territory between.This is a compelling vision,and methods ofinductive logic programming,mentioned in Section 3.6,offer a general way ofspecifying domain knowledgeexplicitly through statements in a formal logic language.However,current logicprogramming solutions suffer serious shortcomings in real-world environ-ments.They tend to be brittle and to lack robustness,and they may be so com-putation intensive as to be completely infeasible on datasets ofany practical size.Perhaps this stems from the fact that they use ﬁrst-order logic,that is,they allowvariables to be introduced into the rules.The machine learning schemes we have seen,whose input and output are represented in terms ofattributes andconstant values,perform their machinations in propositional logic without variables—greatly reducing the search space and avoiding all sorts ofdifﬁcultproblems ofcircularity and termination.Some aspire to realize the visionwithout the accompanying brittleness and computational infeasibility offulllogic programming solutions by adopting simpliﬁed reasoning systems.Othersplace their faith in the general mechanism ofBayesian networks,introduced inSection 6.7,in which causal constraints can be expressed in the initial networkstructure and hidden variables can be postulated and evaluated automatically.It will be interesting to see whether systems that allow ﬂexible speciﬁcation ofdifferent types ofdomain knowledge will become widely deployed.8.3Text and Web miningData mining is about looking for patterns in data.Likewise,text mining is aboutlooking for patterns in text:it is the process ofanalyzing text to extract infor-mation that is useful for particular purposes.Compared with the kind ofdatawe have been talking about in this book,text is unstructured,amorphous,anddifﬁcult to deal with.Nevertheless,in modern Western culture,text is the mostcommon vehicle for the formal exchange ofinformation.The motivation fortrying to extract information from it is compelling—even ifsuccess is only partial.The superﬁcial similarity between text and data mining conceals real differ-ences.In Chapter 1 we characterized data mining as the extraction ofimplicit,8.3TEXT AND WEB MINING351P088407-Ch008.qxd  4/30/05  11:14 AM  Page 351previously unknown,and potentially useful information from data.With textmining,however,the information to be extracted is clearly and explicitly statedin the text.It is not hidden at all—most authors go to great pains to make surethat they express themselves clearly and unambiguously.From a human pointofview,the only sense in which it is “previously unknown”is that time restric-tions make it infeasible for people to read the text themselves.The problem,ofcourse,is that the information is not couched in a manner that is amenable toautomatic processing.Text mining strives to bring it out in a form suitable forconsumption by computers or by people who do not have time to read the full text.A requirement common to both data and text mining is that the informa-tion extracted should be potentially useful.In one sense,this means actionable—capable ofproviding a basis for actions to be taken automatically.In the case ofdata mining,this notion can be expressed in a relatively domain-independentway:actionable patterns are ones that allow nontrivial predictions to be madeon new data from the same source.Performance can be measured by countingsuccesses and failures,statistical techniques can be applied to compare differentdata mining methods on the same problem,and so on.However,in many textmining situations it is hard to characterize what “actionable”means in a waythat is independent ofthe particular domain at hand.This makes it difﬁcult toﬁnd fair and objective measures ofsuccess.As we have emphasized throughout this book,“potentially useful”is oftengiven another interpretation in practical data mining:the key for success is thatthe information extracted must be comprehensiblein that it helps to explain thedata.This is necessary whenever the result is intended for human consumptionrather than (or as well as) for automatic action.This criterion is less applicableto text mining because,unlike data mining,the input itselfis comprehensible.Text mining with comprehensible output is tantamount to summarizing salientfeatures from a large body oftext,which is a subﬁeld in its own right:text summarization.We have already encountered one important text mining problem:documentclassiﬁcation,in which each instance represents a document and the instance’sclass is the document’s topic.Documents are characterized by the words thatappear in them.The presence or absence ofeach word can be treated as aBoolean attribute,or documents can be treated as bags ofwords,rather thansets,by taking word frequencies into account.We encountered this distinctionin Section 4.2,where we learned how to extend Naïve Bayes to the bag-of-wordsrepresentation,yielding the multinomial version ofthe algorithm.There is,ofcourse,an immense number ofdifferent words,and most ofthemare not very useful for document classiﬁcation.This presents a classic featureselection problem.Some words—for example,function words,often called stopwords—can usually be eliminated a priori,but although these occur very352CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 352frequently there are not all that many ofthem.Other words occur so rarely thatthey are unlikely to be useful for classiﬁcation.Paradoxically,infrequent wordsare common—nearly halfthe words in a typical document or corpus ofdocu-ments occur just once.Nevertheless,such an overwhelming number ofwordsremains after these word classes are removed that further feature selection may benecessary using the methods described in Section 7.1.Another issue is that thebag- (or set-) of-words model neglects word order and contextual effects.Thereis a strong case for detecting common phrases and treating them as single units.Document classiﬁcation is supervised learning:the categories are knownbeforehand and given in advance for each training document.The unsupervisedversion ofthe problem is called document clustering.Here there is no predeﬁnedclass,but groups ofcognate documents are sought.Document clustering canassist information retrieval by creating links between similar documents,whichin turn allows related documents to be retrieved once one ofthe documents hasbeen deemed relevant to a query.There are many applications ofdocument classiﬁcation.A relatively easy categorization task,language identiﬁcation,provides an important piece ofmetadata for documents in international collections.A simple representationthat works well for language identiﬁcation is to characterize each document by a proﬁle that consists ofthe n-grams,or sequences ofnconsecutive letters,that appear in it.The most frequent 300 or so n-grams are highly correlatedwith the language.A more challenging application is authorship ascriptioninwhich a document’s author is uncertain and must be guessed from the text.Here,the stopwords,not the content words,are the giveaway,because their dis-tribution is author dependent but topic independent.A third problem is theassignment ofkey phrasesto documents from a controlled vocabulary ofpossi-ble phrases,given a large number oftraining documents that are tagged fromthis vocabulary.Another general class oftext mining problems is metadata extraction.Meta-data was mentioned previously as data about data:in the realm oftext the termgenerally refers to salient features ofa work,such as its author,title,subject clas-siﬁcation,subject headings,and keywords.Metadata is a kind ofhighly struc-tured (and therefore actionable) document summary.The idea ofmetadata isoften expanded to encompass words or phrases that stand for objects or “enti-ties”in the world,leading to the notion ofentity extraction.Ordinary documentsare full ofsuch terms:phone numbers,fax numbers,street addresses,emailaddresses,email signatures,abstracts,tables ofcontents,lists ofreferences,tables,ﬁgures,captions,meeting announcements,Web addresses,and more.Inaddition,there are countless domain-speciﬁc entities,such as international standard book numbers (ISBNs),stock symbols,chemical structures,and mathematical equations.These terms act as single vocabulary items,and manydocument processing tasks can be signiﬁcantly improved ifthey are identiﬁed8.3TEXT AND WEB MINING353P088407-Ch008.qxd  4/30/05  11:14 AM  Page 353as such.They can aid searching,interlinking,and cross-referencing betweendocuments.How can textual entities be identiﬁed? Rote learning,that is,dictionarylookup,is one idea,particularly when coupled with existing resources—lists ofpersonal names and organizations,information about locations from gazetteers,or abbreviation and acronym dictionaries.Another is to use capitalization andpunctuation patterns for names and acronyms;titles (Ms.),sufﬁxes (Jr.),andbaronial preﬁxes (von);or unusual language statistics for foreign names.Regularexpressions sufﬁce for artiﬁcial constructs such as uniform resource locators(URLs);explicit grammars can be written to recognize dates and sums ofmoney.Even the simplest task opens up opportunities for learning to cope withthe huge variation that real-life documents present.As just one example,whatcould be simpler than looking up a name in a table? But the name ofthe Libyanleader Muammar Qaddaﬁis represented in 47 different ways on documents thathave been received by the Library ofCongress!Many short documents describe a particular kind ofobject or event,com-bining entities into a higher-level composite that represent the document’sentire content.The task ofidentifying the composite structure,which can oftenbe represented as a template with slots that are ﬁlled by individual pieces ofstructured information,is called information extraction.Once the entities havebeen found,the text is parsed to determine relationships among them.Typicalextraction problems require ﬁnding the predicate structure ofa small set ofpre-determined propositions.These are usually simple enough to be captured byshallow parsing techniques such as small ﬁnite-state grammars,althoughmatters may be complicated by ambiguous pronoun references and attachedprepositional phrases and other modiﬁers.Machine learning has been appliedto information extraction by seeking rules that extract ﬁllers for slots in the template.These rules may be couched in pattern-action form,the patternsexpressing constraints on the slot-ﬁller and words in its local context.These constraints may involve the words themselves,their part-of-speech tags,andtheir semantic classes.Taking information extraction a step further,the extracted information canbe used in a subsequent step to learn rules—not rules about how to extractinformation but rules that characterize the content ofthe text itself.These rulesmight predict the values for certain slot-ﬁllers from the rest ofthe text.In certaintightly constrained situations,such as Internet job postings for computing-related jobs,information extraction based on a few manually constructed train-ing examples can compete with an entire manually constructed database interms ofthe quality ofthe rules inferred.The World Wide Web is a massive repository oftext.Almost all ofit differsfrom ordinary “plain”text because it contains explicit structural markup.Some354CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 354markup is internal and indicates document structure or format;other markupis external and deﬁnes explicit hypertext links between documents.These infor-mation sources give additional leverage for mining Web documents.Web miningis like text mining but takes advantage ofthis extra information and oftenimproves results by capitalizing on the existence oftopic directories and otherinformation on the Web.Internet resources that contain relational data—telephone directories orproduct catalogs—use hypertext markup language (HTML) formatting com-mands to clearly present the information they contain to Web users.However,it is quite difﬁcult to extract data from such resources automatically.To do so,existing software systems use simple parsing modules called wrappersto analyzethe page structure and extract the requisite information.Ifwrappers are codedby hand,which they often are,this is a trivial kind oftext mining because itrelies on the pages having a ﬁxed,predetermined structure from which infor-mation can be extracted algorithmically.But pages rarely obey the rules.Theirstructures vary;Web sites evolve.Errors that are insigniﬁcant to human readersthrow automatic extraction procedures completely awry.When change occurs,adjusting a wrapper manually can be a nightmare that involves getting yourhead around the existing code and patching it up in a way that does not causebreakage elsewhere.Enter wrapper induction—learning wrappers automatically from examples.The input is a training set ofpages along with tuples representing the informa-tion derived from each page.The output is a set ofrules that extracts the tuplesby parsing the page.For example,it might look for certain HTML delimiters—paragraph boundaries (<p>),list entries (<li>),or boldface (<b>)—that the Webpage designer has used to set offkey items ofinformation,and learn thesequence in which entities are presented.This could be accomplished by iterat-ing over all choices ofdelimiters,stopping when a consistent wrapper is encoun-tered.Then recognition will depend only on a minimal set ofcues,providingsome defense against extraneous text and markers in the input.Alternatively,one might follow Epicurus’s advice at the end ofSection 5.9 and seek a robustwrapper that uses multiple cues to guard against accidental variation.The greatadvantage ofautomatic wrapper induction is that when errors are caused bystylistic variants it is simple to add these to the training data and reinduce a newwrapper that takes them into account.Wrapper induction reduces recognitionproblems when small changes occur and makes it far easier to produce new setsofextraction rules when structures change radically.A development called the semantic Webaims to enable people to publishinformation in a way that makes its structure and semantics explicit so that it can be repurposed instead ofmerely read.This would render wrapper induction superﬂuous.But ifand when the semantic Web is deployed,the8.3TEXT AND WEB MINING355P088407-Ch008.qxd  4/30/05  11:14 AM  Page 355requirement for manual markup—not to mention the huge volumes oflegacypages—will likely increase the demand for automatic induction ofinformation structure.Text mining,including Web mining,is a burgeoning technology that is still,because ofits newness and intrinsic difﬁculty,in a ﬂuid state—akin,perhaps,to the state ofmachine learning in the mid-1980s.There is no real consensusabout what it covers:broadly interpreted,all natural language processing comesunder the ambit oftext mining.It is usually difﬁcult to provide general andmeaningful evaluations because the mining task is highly sensitive to the par-ticular text under consideration.Automatic text mining techniques have a longway to go before they rival the ability ofpeople,even without any special domainknowledge,to glean information from large document collections.But they willgo a long way,because the demand is immense.8.4Adversarial situationsA prime application ofmachine learning is junk email ﬁltering.As we writethese words (in late 2004),the scourge ofunwanted email is a burning issue—maybe by the time you read them the beast will have been vanquished or at leasttamed.At ﬁrst blush junk email ﬁltering appears to present a standard problemofdocument classiﬁcation:divide documents into “ham”and “spam”on thebasis ofthe text they contain,guided by training data,ofwhich there are copiousamounts.But it is not a standard problem because it involves an adversarialaspect.The documents that are being classiﬁed are not chosen randomly froman unimaginably huge set ofall possible documents;they contain emails thatare carefully crafted to evade the ﬁltering process,designed speciﬁcally to beatthe system.Early spam ﬁlters simply discarded messages containing “spammy”wordsthat connote such things as sex,lucre,and quackery.Ofcourse,much legitimatecorrespondence concerns gender,money,and medicine:a balance must bestruck.So ﬁlter designers recruited Bayesian text classiﬁcation schemes thatlearned to strike an appropriate balance during the training process.Spammersquickly adjusted with techniques that concealed the spammy words by mis-spelling them;overwhelmed them with legitimate text,perhaps printed in white on a white background so that only the ﬁlter saw it;or simply put thespam text elsewhere,in an image or a URL that most email readers downloadautomatically.The problem is complicated by the fact that it is hard to compare spam detec-tion algorithms objectively;although training data abounds,privacy issues preclude publishing large public corpora ofrepresentative email.And there arestrong temporal effects.Spam changes character rapidly,invalidating sensitive356CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 356statistical tests such as cross-validation.Finally,the bad guys can also usemachine learning.For example,ifthey could get hold ofexamples ofwhat yourﬁlter blocks and what it lets through,they could use this as training data to learnhow to evade it.There are,unfortunately,many other examples ofadversarial learning situa-tions in our world today.Closely related to junk email is search engine spam:sites that attempt to deceive Internet search engines into placing them pro-minently in lists ofsearch results.Highly ranked pages yield direct ﬁnancial beneﬁts to their owners because they present opportunities for advertising,pro-viding strong motivation for proﬁt seekers.Then there are the computer viruswars,in which designers ofviruses and virus-protection software react to oneanother’s innovations.Here the motivation tends to be general disruption anddenial ofservice rather than monetary gain.Computer network security is a continually escalating battle.Protectorsharden networks,operating systems,and applications,and attackers ﬁnd vulnerabilities in all three areas.Intrusion detection systems sniffout unusualpatterns ofactivity that might be caused by a hacker’s reconnaissance activity.Attackers realize this and try to obfuscate their trails,perhaps by working indi-rectly or by spreading their activities over a long time—or,conversely,by strik-ing very quickly.Data mining is being applied to this problem in an attempt todiscover semantic connections among attacker traces in computer network datathat intrusion detection systems miss.This is a large-scale problem:audit logsused to monitor computer network security can amount to gigabytes a day evenin medium-sized organizations.Many automated threat detection systems are based on matching current datato known attack types.The U.S.Federal Aviation Administration developed theComputer Assisted Passenger Pre-Screening System(CAPPS),which screensairline passengers on the basis oftheir ﬂight records and ﬂags individuals foradditional checked baggage screening.Although the exact details are unpub-lished,CAPPS is,for example,thought to assign higher threat scores to cashpayments.However,this approach can only spot known or anticipated threats.Researchers are using unsupervised approaches such as anomaly and outlierdetection in an attempt to detect suspicious activity.As well as ﬂagging poten-tial threats,anomaly detection systems can be applied to the detection ofillegalactivities such as ﬁnancial fraud and money laundering.Data mining is being used today to sift through huge volumes ofdata in thename ofhomeland defense.Heterogeneous information such as ﬁnancial trans-actions,health-care records,and network trafﬁc is being mined to create pro-ﬁles,construct social network models,and detect terrorist communications.This activity raises serious privacy concerns and has resulted in the devel-opment ofprivacy-preserving data mining techniques.These algorithms try to discern patterns in the data without accessing the original data directly,8.4ADVERSARIAL SITUATIONS357P088407-Ch008.qxd  4/30/05  11:14 AM  Page 357typically by distorting it with random values.To preserve privacy,they mustguarantee that the mining process does not receive enough information toreconstruct the original data.This is easier said than done.On a lighter note,not all adversarial data mining is aimed at combating nefar-ious activity.Multiagent systems in complex,noisy real-time domains involveautonomous agents that must both collaborate in a team and compete againstantagonists.Ifyou are having trouble visualizing this,think soccer.Robo-socceris a rich and popular domain for exploring how machine learning can be appliedto such difﬁcult problems.Players must not only hone low-level skills but mustalso learn to work together and adapt to the behavior patterns ofdifferent opponents.Finally,machine learning has been used to solve a historical literary mysteryby unmasking a proliﬁc author who had attempted to conceal his identity.AsKoppel and Schler (2004) relate,Ben Ish Chai was the leading rabbinic scholarin Baghdad in the late nineteenth century.Among his vast literary legacy are two separate collections ofabout 500 Hebrew-Aramaic letters written inresponse to legal queries.He is known to have written one collection.Althoughhe claims to have found the other in an archive,historians suspect that he wroteit,too,but attempted to disguise his authorship by deliberately altering his style.The problem this case presents to machine learning is that there is no corpus ofwork to ascribe to the mystery author.There were a few known candidates,butthe letters could equally well have been written by anyone else.A new techniqueappropriately called unmaskingwas developed that creates a model to distin-guish the known author’s work A from the unknown author’s work X,itera-tively removes those features that are most useful for distinguishing the two,andexamines the speed with which cross-validation accuracy degrades as more fea-tures are removed.The hypothesis is that ifwork X is written by work A’s author,who is trying to conceal his identity,whatever differences there are betweenwork X and work A will be reﬂected in only a relatively small number offea-tures compared with the differences between work X and the works ofa differ-ent author,say the author ofwork B.In other words,when work X is comparedwith works A and B,the accuracy curve as features are removed will declinemuch faster for work A than it does for work B.Koppel and Schler concludedthat Ben Ish Chai did indeed write the mystery letters,and their technique is astriking example ofthe original and creative use ofmachine learning in anadversarial situation.8.5Ubiquitous data miningWe began this book by pointing out that we are overwhelmed with data.Nowhere does this affect the lives ofordinary people more than on the WorldWide Web.At present,the Web contains more than 5 billion documents,total-358CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 358ing perhaps 20 TB—and it continues to grow exponentially,doubling every 6 months or so.Most U.S.consumers use the Web.None ofthem can keep pace with the information explosion.Whereas data mining originated in thecorporate world because that’s where the databases are,text mining is movingmachine learning technology out ofthe companies and into the home.When-ever we are overwhelmed by data on the Web,text mining promises tools totame it.Applications are legion.Finding friends and contacting them,main-taining ﬁnancial portfolios,shopping for bargains in an electronic world,usingdata detectors ofany kind—all ofthese could be accomplished automaticallywithout explicit programming.Already text mining techniques are being usedto predict what link you’re going to click next,to organize documents for you,and to sort your mail.In a world where information is overwhelming,disor-ganized,and anarchic,text mining may be the solution we so desperately need.Many believe that the Web is but the harbinger ofan even greater paradigmshift:ubiquitous computing.Small portable devices are everywhere—mobilephones,personal digital assistants,personal stereo and video players,digitalcameras,mobile Web access.Already some devices integrate all these functions.They know our location in physical time and space,help us communicate insocial space,organize our personal planning space,recall our past,and envelopus in global information space.It is easy to ﬁnd dozens ofprocessors in amiddle-class home in the U.S.today.They do not communicate with oneanother or with the global information infrastructure—yet.But they will,andwhen they do the potential for data mining will soar.Take consumer music.Popular music leads the vanguard oftechnologicaladvance.Sony’s original Walkman paved the way to today’s ubiquitous portableelectronics.Apple’s iPod pioneered large-scale portable storage.Napster’snetwork technology spurred the development ofpeer-to-peer protocols.Recommender systems such as Fireﬂy brought computing to social networks.In the near future content-aware music services will migrate to portable devices.Applications for data mining in networked communities ofmusic service users will be legion:discovering musical trends,tracking preferences and tastes,and analyzing listening behaviors.Ubiquitous computing will weave digital space closely into real-world activ-ities.To many,extrapolating their own computer experiences ofextreme frus-tration,arcane technology,perceived personal inadequacy,and machine failure,this sounds like a nightmare.But proponents point out that it can’t be like that,because,ifit is,it won’t work.Today’s visionaries foresee a world of“calm”com-puting in which hidden machines silently conspire behind the scenes to makeour lives richer and easier.They’ll reach beyond the big problems ofcorporateﬁnance and school homework to the little annoyances such as where are the carkeys,can I get a parking place,and is that shirt I saw last week at Macy’s still onthe rack? Clocks will ﬁnd the correct time after a power failure,the microwave8.5UBIQUITOUS DATA MINING359P088407-Ch008.qxd  4/30/05  11:14 AM  Page 359will download new recipes from the Internet,and kid’s toys will refresh them-selves with new games and new vocabularies.Clothes labels will track washing,coffee cups will alert cleaning staffto mold,light switches will save energy ifnoone is in the room,and pencils will digitize everything we draw.Where will datamining be in this new world? Everywhere!It’s hard to point to examples ofa future that does not yet exist.But ad-vances in user interface technology are suggestive.Many repetitive tasks indirect-manipulation computer interfaces cannot be automated with standardapplication tools,forcing computer users to perform the same interface actions repeatedly.This typiﬁes the frustrations alluded to previously:who’s incharge—me or it? Experienced programmers might write a script to carry outsuch tasks on their behalf,but as operating systems accrue layer upon layer ofcomplexity the power ofprogrammers to command the machine is eroded andvanishes altogether when complex functionality is embedded in appliancesrather than in general-purpose computers.Research in programming by demonstrationenables ordinary computer usersto automate predictable tasks without requiring any programming knowledgeat all.The user need only know how to perform the task in the usual way to beable to communicate it to the computer.One system,called Familiar,helps usersautomate iterative tasks involving existing applications on Macintosh comput-ers.It works across applications and can work with completely new ones neverbefore encountered.It does this by using Apple’s scripting language to gleaninformation from each application and exploiting that information to makepredictions.The agent tolerates noise.It generates explanations to inform thecomputer user about its predictions,and incorporates feedback.It’s adaptive:itlearns specialized tasks for individual users.Furthermore,it is sensitive to eachuser’s style.Iftwo people were teaching a task and happened to give identicaldemonstrations,Familiar would not necessarily infer identical programs—it’stuned to their habits because it learns from their interaction history.Familiar employs standard machine learning techniques to infer the user’sintent.Rules are used to evaluate predictions so that the best one can be pre-sented to the user at each point.These rules are conditional so that users canteach classiﬁcation tasks such as sorting ﬁles based on their type and assigninglabels based on their size.They are learned incrementally:the agent adapts toindividual users by recording their interaction history.Many difﬁculties arise.One is scarcity ofdata.Users are loathe to demon-strate several iterations ofa task—they think the agent should immediatelycatch on to what they are doing.Whereas a data miner would consider a 100-instance dataset miniscule,users bridle at the prospect ofdemonstrating a taskeven halfa dozen times.A second difﬁculty is the plethora ofattributes.Thecomputer desktop environment has hundreds offeatures that any given actionmight depend upon.This means that small datasets are overwhelmingly likely360CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 360to contain attributes that are apparently highly predictive but nevertheless irrelevant,and specialized statistical tests are needed to compare alternativehypotheses.A third is that the iterative,improvement-driven development stylethat characterizes data mining applications fails.It is impossible in principletocreate a ﬁxed training-and-testing corpus for an interactive problem such asprogramming by demonstration because each improvement in the agent altersthe test data by affecting how users react to it.A fourth is that existing applica-tion programs provide limited access to application and user data:often the rawmaterial on which successful operation depends is inaccessible,buried deepwithin the application program.Data mining is already widely used at work.Text mining is starting to bringthe techniques in this book into our own lives,as we read our email and surfthe Web.As for the future,it will be stranger than we can imagine.The spread-ing computing infrastructure will offer untold opportunities for learning.Datamining will be there,behind the scenes,playing a role that will turn out to befoundational.8.6Further readingThere is a substantial volume ofliterature that treats the topic ofmassivedatasets,and we can only point to a few references here.Fayyad and Smith(1995) describe the application ofdata mining to voluminous data from scien-tiﬁc experiments.Shafer et al.(1996) describe a parallel version ofa top-downdecision tree inducer.A sequential decision tree algorithm for massive disk-resident datasets has been developed by Mehta et al.(1996).The technique ofapplying any algorithm to a large dataset by splitting it into smaller chunks andbagging or boosting the result is described by Breiman (1999);Frank et al.(2002) explain the related pruning and selection scheme.Despite its importance,little seems to have been written about the generalproblem ofincorporating metadata into practical data mining.A scheme forencoding domain knowledge into propositional rules and its use for bothdeduction and induction has been investigated by Giraud-Carrier (1996).Therelated area ofinductive logic programming,which deals with knowledge rep-resented by ﬁrst-order logic rules,is covered by Bergadano and Gunetti (1996).Text mining is an emerging area,and there are few comprehensive surveys ofthe area as a whole:Witten (2004) provides one.A large number offeature selec-tion and machine learning techniques have been applied to text categorization(Sebastiani 2002).Martin (1995) describes applications ofdocument clusteringto information retrieval.Cavnar and Trenkle (1994) show how to use n-gramproﬁles to ascertain with high accuracy the language in which a document iswritten.The use ofsupport vector machines for authorship ascription is8.6FURTHER READING361P088407-Ch008.qxd  4/30/05  11:14 AM  Page 361described by Diederich et al.(2003);the same technology was used by Dumaiset al.(1998) to assign key phrases from a controlled vocabulary to documentson the basis ofa large number oftraining documents.The use ofmachine learn-ing to extract key phrases from the document text has been investigated byTurney (1999) and Frank et al.(1999).Appelt (1999) describes many problems ofinformation extraction.Manyauthors have applied machine learning to seek rules that extract slot-ﬁllers fortemplates,for example,Soderland et al.(1995),Huffman (1996),and Freitag(2002).Califfand Mooney (1999) and Nahm and Mooney (2000) investigatedthe problem ofextracting information from job ads posted on Internet newsgroups.An approach to ﬁnding information in running text based on compression techniques has been reported by Witten et al.(1999).Mann (1993)notes the plethora ofvariations ofMuammar Qaddaﬁon documents receivedby the Library ofCongress.Chakrabarti (2003) has written an excellent and comprehensive book ontechniques ofWeb mining.Kushmerick et al.(1997) developed techniques ofwrapper induction.The semantic Web was introduced by Tim Berners-Lee(Berners-Lee et al.2001),who 10 years earlier developed the technology behindthe World Wide Web.The ﬁrst paper on junk email ﬁltering was written by Sahami et al.(1998).Our material on computer network security is culled from work by Yurcik et al.(2003).The information on the CAPPS system comes from the U.S.House ofRepresentatives Subcommittee on Aviation (2002),and the use ofunsupervisedlearning for threat detection is described by Bay and Schwabacher (2003).Prob-lems with current privacy-preserving data mining techniques have been identi-ﬁed by Datta et al.(2003).Stone and Veloso (2000) surveyed multiagent systemsofthe kind that are used for playing robo-soccer from a machine learning perspective.The fascinating story ofBen Ish Chai and the technique used tounmask him is from Koppel and Schler (2004).The vision ofcalm computing,as well as the examples we have mentioned,is from Weiser (1996) and Weiser and Brown (1997).More information on dif-ferent methods ofprogramming by demonstration can be found in compendiaby Cypher (1993) and Lieberman (2001).Mitchell et al.(1994) report someexperience with learning apprentices.Familiar is described by Paynter (2000).Permutation tests (Good 1994) are statistical tests that are suitable for smallsample problems:Frank (2000) describes their application in machine learning.362CHAPTER 8|MOVING ON:EXTENSIONS AND APPLICATIONSP088407-Ch008.qxd  4/30/05  11:14 AM  Page 362partIIThe Weka Machine LearningWorkbenchP088407-Ch009.qxd  4/30/05  11:14 AM  Page 363P088407-Ch009.qxd  4/30/05  11:14 AM  Page 364Experience shows that no single machine learning scheme is appropriate to alldata mining problems.The universal learner is an idealistic fantasy.As we haveemphasized throughout this book,real datasets vary,and to obtain accuratemodels the bias ofthe learning algorithm must match the structure ofthedomain.Data mining is an experimental science.The Weka workbench is a collection ofstate-of-the-art machine learningalgorithms and data preprocessing tools.It includes virtually all the algorithmsdescribed in this book.It is designed so that you can quickly try out existingmethods on new datasets in ﬂexible ways.It provides extensive support for thewhole process ofexperimental data mining,including preparing the input data,evaluating learning schemes statistically,and visualizing the input data and the result oflearning.As well as a wide variety oflearning algorithms,itincludes a wide range ofpreprocessing tools.This diverse and comprehensivetoolkit is accessed through a common interface so that its users can comparedifferent methods and identify those that are most appropriate for the problemat hand.chapter9Introduction to Weka365P088407-Ch009.qxd  4/30/05  11:14 AM  Page 365Weka was developed at the University ofWaikato in New Zealand,and thename stands for Waikato Environment for Knowledge Analysis.Outside the university the weka,pronounced to rhyme with Mecca,is a ﬂightless bird withan inquisitive nature found only on the islands ofNew Zealand.The system iswritten in Java and distributed under the terms ofthe GNU General PublicLicense.It runs on almost any platform and has been tested under Linux,Windows,and Macintosh operating systems—and even on a personal digitalassistant.It provides a uniform interface to many different learning algorithms,along with methods for pre- and postprocessing and for evaluating the result oflearning schemes on any given dataset.9.1What’s in Weka?Weka provides implementations oflearning algorithms that you can easily applyto your dataset.It also includes a variety oftools for transforming datasets,suchas the algorithms for discretization described in Chapter 7.You can preprocessa dataset,feed it into a learning scheme,and analyze the resulting classiﬁer andits performance—all without writing any program code at all.The workbench includes methods for all the standard data mining problems:regression,classiﬁcation,clustering,association rule mining,and attribute selec-tion.Getting to know the data is an integral part ofthe work,and many datavisualization facilities and data preprocessing tools are provided.All algorithmstake their input in the form ofa single relational table in the ARFF formatdescribed in Section 2.4,which can be read from a ﬁle or generated by a data-base query.One way ofusing Weka is to apply a learning method to a dataset and analyzeits output to learn more about the data.Another is to use learned models togenerate predictions on new instances.A third is to apply several different learn-ers and compare their performance in order to choose one for prediction.Thelearning methods are called classiﬁers,and in the interactive Weka interface youselect the one you want from a menu.Many classiﬁers have tunable parameters,which you access through a property sheet or object editor.A common evalua-tion module is used to measure the performance ofall classiﬁers.Implementations ofactual learning schemes are the most valuable resourcethat Weka provides.But tools for preprocessing the data,called ﬁlters,come aclose second.Like classiﬁers,you select ﬁlters from a menu and tailor them to your requirements.We will show how different ﬁlters can be used,list the ﬁltering algorithms,and describe their parameters.Weka also includes imple-mentations ofalgorithms for learning association rules,clustering data forwhich no class value is speciﬁed,and selecting relevant attributes in the data,which we describe brieﬂy.366CHAPTER 9|INTRODUCTION TO WEKAP088407-Ch009.qxd  4/30/05  11:14 AM  Page 3669.2How do you use it?The easiest way to use Weka is through a graphical user interface called theExplorer.This gives access to all ofits facilities using menu selection and formﬁlling.For example,you can quickly read in a dataset from an ARFF ﬁle (orspreadsheet) and build a decision tree from it.But learning decision trees is justthe beginning:there are many other algorithms to explore.The Explorer inter-face helps you do just that.It guides you by presenting choices as menus,byforcing you to work in an appropriate order by graying out options until theyare applicable,and by presenting options as forms to be ﬁlled out.Helpful tooltipspop up as the mouse passes over items on the screen to explain what theydo.Sensible default values ensure that you can obtain results with a minimumofeffort—but you will have to think about what you are doing to understandwhat the results mean.There are two other graphical user interfaces to Weka.The Knowledge Flowinterface allows you to design conﬁgurations for streamed data processing.Afundamental disadvantage ofthe Explorer is that it holds everything in mainmemory—when you open a dataset,it immediately loads it all in.This meansthat it can only be applied to small to medium-sized problems.However,Wekacontains some incremental algorithms that can be used to process very largedatasets.The Knowledge Flow interface lets you drag boxes representing learn-ing algorithms and data sources around the screen and join them together into the conﬁguration you want.It enables you to specify a data stream by con-necting components representing data sources,preprocessing tools,learningalgorithms,evaluation methods,and visualization modules.Ifthe ﬁlters andlearning algorithms are capable ofincremental learning,data will be loaded andprocessed incrementally.Weka’s third interface,the Experimenter,is designed to help you answer abasic practical question when applying classiﬁcation and regression techniques:which methods and parameter values work best for the given problem? Thereis usually no way to answer this question a priori,and one reason we developedthe workbench was to provide an environment that enables Weka users tocompare a variety oflearning techniques.This can be done interactively usingthe Explorer.However,the Experimenter allows you to automate the process bymaking it easy to run classiﬁers and ﬁlters with different parameter settings ona corpus ofdatasets,collect performance statistics,and perform signiﬁcancetests.Advanced users can employ the Experimenter to distribute the comput-ing load across multiple machines using Java remote method invocation (RMI).In this way you can set up large-scale statistical experiments and leave them torun.Behind these interactive interfaces lies the basic functionality ofWeka.Thiscan be accessed in raw form by entering textual commands,which gives access9.2HOW DO YOU USE IT?367P088407-Ch009.qxd  4/30/05  11:14 AM  Page 367to all features ofthe system.When you ﬁre up Weka you have to choose amongfour different user interfaces:the Explorer,the Knowledge Flow,the Experi-menter,and the command-line interface.We describe them in turn in the nextchapters.Most people choose the Explorer,at least initially.9.3What else can you do?An important resource when working with Weka is the online documentation,which has been automatically generated from the source code and conciselyreﬂects its structure.We will explain how to use this documentation and howto identify Weka’s major building blocks,highlighting which parts containsupervised learning methods,which contain tools for data preprocessing,andwhich contain methods for other learning schemes.It gives the only completelist ofavailable algorithms because Weka is continually growing and—beinggenerated automatically from the source code—the online documentation isalways up to date.Moreover,it becomes essential ifyou want to proceed to thenext level and access the library from your own Java programs or write and testlearning schemes ofyour own.In most data mining applications,the machine learning component is just asmall part ofa far larger software system.Ifyou intend to write a data miningapplication,you will want to access the programs in Weka from inside your owncode.By doing so,you can solve the machine learning subproblem ofyour application with a minimum ofadditional programming.We show you how todo that by presenting an example ofa simple data mining application in Java.This will enable you to become familiar with the basic data structures in Weka,representing instances,classiﬁers,and ﬁlters.Ifyou intend to become an expert in machine learning algorithms (or,indeed,ifyou already are one),you’ll probably want to implement your ownalgorithms without having to address such mundane details as reading the datafrom a ﬁle,implementing ﬁltering algorithms,or providing code to evaluate theresults.Ifso,we have good news for you:Weka already includes all this.To makefull use ofit,you must become acquainted with the basic data structures.Tohelp you reach this point,we will describe these structures in more detail andexplain an illustrative implementation ofa classiﬁer.9.4How do you get it?Weka is available from http://www.cs.waikato.ac.nz/ml/weka.You can downloadeither a platform-speciﬁc installer or an executable Java jar ﬁle that you run inthe usual way ifJava is installed.We recommend that you download and installit now,and follow through the examples in the upcoming sections.368CHAPTER 9|INTRODUCTION TO WEKAP088407-Ch009.qxd  4/30/05  11:14 AM  Page 368Weka’s main graphical user interface,the Explorer,gives access to all its facili-ties using menu selection and form ﬁlling.It is illustrated in Figure 10.1.Thereare six different panels,selected by the tabs at the top,corresponding to thevarious data mining tasks that Weka supports.10.1Getting startedSuppose you have some data and you want to build a decision tree from it.First,you need to prepare the data then ﬁre up the Explorer and load in the data.Nextyou select a decision tree construction method,build a tree,and interpret theoutput.It’s easy to do it again with a different tree construction algorithm or a different evaluation method.In the Explorer you can ﬂip back and forthbetween the results you have obtained,evaluate the models that have been builton different datasets,and visualize graphically both the models and the datasetsthemselves—including any classiﬁcation errors the models make.chapter10The Explorer369P088407-Ch010.qxd  4/30/05  10:59 AM  Page 369Preparing the dataThe data is often presented in a spreadsheet or database.However,Weka’s nativedata storage method is ARFF format (Section 2.4).You can easily convert froma spreadsheet to ARFF.The bulk ofan ARFF ﬁle consists ofa list ofthe instances,and the attribute values for each instance are separated by commas (Figure 2.2).Most spreadsheet and database programs allow you to export data into a ﬁle incomma-separated value (CSV) format as a list ofrecords with commas betweenitems.Having done this,you need only load the ﬁle into a text editor or wordprocessor;add the dataset’s name using the @relationtag,the attribute infor-mation using @attribute,and a @dataline;and save the ﬁle as raw text.Forexample,Figure 10.2 shows an Excel spreadsheet containing the weather datafrom Section 1.2,the data in CSV form loaded into Microsoft Word,and theresult ofconverting it manually into an ARFF ﬁle.However,you don’t actuallyhave to go through these steps to create the ARFF ﬁle yourself,because theExplorer can read CSV spreadsheet ﬁles directly,as described later.Loading the data into the ExplorerLet’s load this data into the Explorer and start analyzing it.Fire up Weka to getthe panel shown in Figure 10.3(a).Select Explorerfrom the four graphical user370CHAPTER 10|THE EXPLORERFigure 10.1The Explorer interface.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 370interface choices at the bottom.(The others were mentioned earlier:Simple CLIis the old-fashioned command-line interface.)What you see next is the main Explorer screen,shown in Figure 10.3(b).Actu-ally,the ﬁgure shows what it will look like afteryou have loaded in the weatherdata.The six tabs along the top are the basic operations that the Explorer supports:right now we are on Preprocess.Click the Open ﬁlebutton to 10.1GETTING STARTED371(a)(b)(c)Figure 10.2Weather data:(a) spreadsheet,(b) CSV format,and (c) ARFF.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 371bring up a standard dialog through which you can select a ﬁle.Choose theweather.arffﬁle.Ifyou have it in CSV format,change from ARFF data ﬁlestoCSV data ﬁles.When you specify a .csvﬁle it is automatically converted intoARFF format.Having loaded the ﬁle,the screen will be as shown in Figure 10.3(b).Thistells you about the dataset:it has 14 instances and ﬁve attributes (center left);the attributes are called outlook,temperature,humidity,windy,and play(lowerleft).The ﬁrst attribute,outlook,is selected by default (you can choose othersby clicking them) and has no missing values,three distinct values,and no uniquevalues;the actual values are sunny,overcast,and rainy,and they occur ﬁve,four,and ﬁve times,respectively (center right).A histogram at the lower right showshow often each ofthe two values ofthe class,play,occurs for each value oftheoutlookattribute.The attribute outlookis used because it appears in the boxabove the histogram,but you can draw a histogram ofany other attributeinstead.Here playis selected as the class attribute;it is used to color the his-togram,and any ﬁlters that require a class value use it too.The outlookattribute in Figure 10.3(b) is nominal.Ifyou select a numericattribute,you see its minimum and maximum values,mean,and standard 372CHAPTER 10|THE EXPLORER(a)(b)Figure 10.3The Weka Explorer:(a) choosing the Explorer interface and (b) reading inthe weather data.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 372deviation.In this case the histogram will show the distribution ofthe class as a function ofthis attribute (an example appears in Figure 10.9 on page 384).You can delete an attribute by clicking its checkbox and using the Removebutton.Allselects all the attributes,Noneselects none,and Invertinverts thecurrent selection.You can undo a change by clicking the Undobutton.The Editbutton brings up an editor that allows you to inspect the data,search for par-ticular values and edit them,and delete instances and attributes.Right-clickingon values and column headers brings up corresponding context menus.Building a decision treeTo see what the C4.5 decision tree learner described in Section 6.1 does withthis dataset,use the J4.8 algorithm,which is Weka’s implementation ofthis deci-sion tree learner.(J4.8 actually implements a later and slightly improved versioncalled C4.5 revision 8,which was the last public version ofthis family ofalgo-rithms before the commercial implementation C5.0 was released.) Click theClassifytab to get a screen that looks like Figure 10.4(b).Actually,the ﬁgureshows what it will look like afteryou have analyzed the weather data.First select the classiﬁer by clicking the Choosebutton at the top left,openingup the treessection ofthe hierarchical menu in Figure 10.4(a),and ﬁnding J48.The menu structure represents the organization ofthe Weka code into modules,which will be described in Chapter 13.For now,just open up the hierarchy asnecessary—the items you need to select are always at the lowest level.Onceselected,J48appears in the line beside the Choosebutton as shown in Figure10.4(b),along with its default parameter values.Ifyou click that line,the J4.8classiﬁer’s object editor opens up and you can see what the parameters meanand alter their values ifyou wish.The Explorer generally chooses sensibledefaults.Having chosen the classiﬁer,invoke it by clicking the Startbutton.Wekaworks for a briefperiod—when it is working,the little bird at the lower rightofFigure 10.4(b) jumps up and dances—and then produces the output shownin the main panel ofFigure 10.4(b).Examining the outputFigure 10.5 shows the full output (Figure 10.4(b) only gives the lower half).At the beginning is a summary ofthe dataset,and the fact that tenfold cross-validation was used to evaluate it.That is the default,and ifyou look closely atFigure 10.4(b) you will see that the Cross-validationbox at the left is checked.Then comes a pruned decision tree in textual form.The ﬁrst split is on theoutlookattribute,and then,at the second level,the splits are on humidityandwindy,respectively.In the tree structure,a colon introduces the class label that10.1GETTING STARTED373P088407-Ch010.qxd  4/30/05  10:59 AM  Page 373374CHAPTER 10|THE EXPLORER(a)(b)Figure 10.4Using J4.8:(a) ﬁnding it in the classiﬁers list and (b) the Classifytab.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 37410.1GETTING STARTED375Relative absolute error                 60      % Root relative squared error             97.6586 % Total Number of Instances               14     Correctly Classified Instances           9               64.2857 % Incorrectly Classified Instances         5               35.7143 % Kappa statistic                          0.186 Mean absolute error                      0.2857Root mean squared error                  0.4818Time taken to build model: 0.27 seconds=== Stratified cross-validation ====== Summary ====== Run information ===Scheme:       weka.classifiers.trees.J48 -C 0.25 -M 2 Relation:     weatherInstances:    14Attributes:   5               outlook              temperature              humidity              windy              playTest mode:    10-fold cross-validation=== Classifier model (full training set) ===J48 pruned tree------------------outlook = sunny|   humidity <= 75: yes (2.0)|   humidity > 75: no (3.0)outlook = overcast: yes (4.0)outlook = rainy|   windy = TRUE: no (2.0)|   windy = FALSE: yes (3.0)Number of Leaves  :  5 Size of the tree :   8Figure 10.5Output from the J4.8 decision tree learner.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 375has been assigned to a particular leaf,followed by the number ofinstances thatreach that leaf,expressed as a decimal number because ofthe way the algorithmuses fractional instances to handle missing values.Ifthere were incorrectly clas-siﬁed instances (there aren’t in this example) their number would appear,too:thus 2.0/1.0means that two instances reached that leaf,ofwhich one is classi-ﬁed incorrectly.Beneath the tree structure the number ofleaves is printed;thenthe total number ofnodes (Size ofthe tree).There is a way to view decision treesmore graphically:see pages 378–379 later in this chapter.The next part ofthe output gives estimates ofthe tree’s predictive perform-ance.In this case they are obtained using stratiﬁed cross-validation with 10folds,the default in Figure 10.4(b).As you can see,more than 30% oftheinstances (5 out of14) have been misclassiﬁed in the cross-validation.This indi-cates that the results obtained from the training data are optimistic comparedwith what might be obtained from an independent test set from the same source.From the confusion matrix at the end (described in Section 5.7) observe that 2instances ofclass yeshave been assigned to class noand 3 ofclass noare assignedto class yes.As well as the classiﬁcation error,the evaluation module also outputs theKappa statistic (Section 5.7),the mean absolute error,and the root mean-squarederror ofthe class probability estimates assigned by the tree.The root mean-squared error is the square root ofthe average quadratic loss (Section 5.6).Themean absolute error is calculated in a similar way using the absolute instead ofthe squared difference.It also outputs relative errors,which are based on the priorprobabilities (i.e.,those obtained by the ZeroRlearning scheme described later).Finally,for each class it also outputs some statistics from page 172.376CHAPTER 10|THE EXPLORER=== Detailed Accuracy By Class ===TP Rate   FP Rate   Precision   Recall  F-Measure   Class  0.778     0.6        0.7       0.778     0.737    yes  0.4       0.222      0.5       0.4       0.444    no=== Confusion Matrix === a b   <-- classified as 7 2 | a = yes 3 2 | b = noFigure 10.5(continued)P088407-Ch010.qxd  4/30/05  10:59 AM  Page 376Doing it againYou can easily run J4.8 again with a different evaluation method.Select Usetraining set(near the top left in Figure 10.4(b)) and click Startagain.The clas-siﬁer output is quickly replaced to show how well the derived model performson the training set,instead ofshowing the cross-validation results.This evalu-ation is highly optimistic (Section 5.1).It may still be useful,because it gener-ally represents an upper bound to the model’s performance on fresh data.Inthis case,all 14 training instances are classiﬁed correctly.In some cases a classi-ﬁer may decide to leave some instances unclassiﬁed,in which case these will belisted as Unclassiﬁed Instances.This does not happen for most learning schemesin Weka.The panel in Figure 10.4(b) has further test options:Supplied test set,in whichyou specify a separate ﬁle containing the test set,and Percentage split,with whichyou can hold out a certain percentage ofthe data for testing.You can outputthe predictions for each instance by clicking the More optionsbutton and check-ing the appropriate entry.There are other useful options,such as suppressingsome output and including other statistics such as entropy evaluation measuresand cost-sensitive evaluation.For the latter you must enter a cost matrix:typethe number ofclasses into the Classesbox (and terminate it with the EnterorReturnkey) to get a default cost matrix (Section 5.7),then edit the values asrequired.The small pane at the lower left ofFigure 10.4(b),which contains one high-lighted line,is a history list ofthe results.The Explorer adds a new line when-ever you run a classiﬁer.Because you have now run the classiﬁer twice,the listwill contain two items.To return to a previous result set,click the correspon-ding line and the output for that run will appear in the classiﬁer output pane.This makes it easy to explore different classiﬁers or evaluation schemes andrevisit the results to compare them.Working with modelsThe result history list is the entry point to some powerful features oftheExplorer.When you right-click an entry a menu appears that allows you to viewthe results in a separate window,or save the result buffer.More importantly,youcan save the model that Weka has generated in the form ofa Java object ﬁle.You can reload a model that was saved previously,which generates a new entryin the result list.Ifyou now supply a test set,you can reevaluate the old modelon that new set.Several items in the right-click menu allow you to visualize the results invarious ways.At the top ofthe Explorer interface is a separate Visualizetab,butthat is different:it shows the dataset,not the results for a particular model.By10.1GETTING STARTED377P088407-Ch010.qxd  4/30/05  10:59 AM  Page 377right-clicking an entry in the history list you can see the classiﬁer errors.Ifthe model is a tree or a Bayesian network you can see its structure.You can also view the margin curve (page 324) and various cost and threshold curves(Section 5.7).For cost and threshold curves you must choose a class value froma submenu.The Visualize threshold curvemenu item allows you to see the effectofvarying the probability threshold above which an instance is assigned to thatclass.You can select from a wide variety ofcurves that include the ROC andrecall–precision curves (Table 5.7).To see these,choose the X- and Y-axes appro-priately from the menus given.For example,set X to False positive rateand Y toTrue positive ratefor an ROC curve or X to Recall and Y to Precisionfor arecall–precision curve.Figure 10.6 shows two ways oflooking at the result ofusing J4.8 to classifythe Iris dataset (Section 1.2)—we use this rather than the weather data becauseit produces more interesting pictures.Figure 10.6(a) shows the tree.Right-clicka blank space in this window to bring up a menu enabling you to automaticallyscale the view or force the tree into the window.Drag the mouse to pan aroundthe space.It’s also possible to visualize the instance data at any node,ifit hasbeen saved by the learning algorithm.Figure 10.6(b) shows the classiﬁer errors on a two-dimensional plot.You canchoose which attributes to use for X and Y using the selection boxes at the top.Alternatively,click one ofthe speckled horizontal strips to the right ofthe plot:left-click for X and right-click for Y.Each strip shows the spread ofinstancesalong that attribute.X and Y appear beside the ones you have chosen for the axes.The data points are colored according to their class:blue,red,and green forIris setosa,Iris versicolor,and Iris virginica,respectively (there is a key at thebottom ofthe screen).Correctly classiﬁed instances are shown as crosses;incor-rectly classiﬁed ones appear as boxes (ofwhich there are three in Figure 10.6(b)).You can click on an instance to bring up relevant details:its instance number,the values ofthe attributes,its class,and the predicted class.When things go wrongBeneath the result history list,at the bottom ofFigure 10.4(b),is a status linethat says,simply,OK.Occasionally,this changes to See error log,an indicationthat something has gone wrong.For example,there may be constraints amongthe various different selections you can make in a panel.Most ofthe time theinterface grays out inappropriate selections and refuses to let you choose them.But occasionally the interactions are more complex,and you can end up select-ing an incompatible set ofoptions.In this case,the status line changes whenWeka discovers the incompatibility—typically when you press Start.To see theerror,click the Logbutton to the left ofthe weka in the lower right-hand cornerofthe interface.378CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 37810.1GETTING STARTED379(a)(b)Figure 10.6Visualizing the result ofJ4.8 on the iris dataset:(a) the tree and (b) the clas-siﬁer errors.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 37910.2Exploring the ExplorerWe have brieﬂy investigated two ofthe six tabs at the top ofthe Explorer windowin Figure 10.3(b) and Figure 10.4(b).In summary,here’s what all ofthe tabs do:1.Preprocess:Choose the dataset and modify it in various ways.2.Classify:Train learning schemes that perform classiﬁcation or regressionand evaluate them.3.Cluster:Learn clusters for the dataset.4.Associate:Learn association rules for the data and evaluate them.5.Select attributes:Select the most relevant aspects in the dataset.6.Visualize:View different two-dimensional plots ofthe data and interactwith them.Each tab gives access to a whole range offacilities.In our tour so far,we havebarely scratched the surface ofthe Preprocessand Classifypanels.At the bottom ofevery panel is a Statusbox and a Logbutton.The status box displays messages that keep you informed about what’s going on.Forexample,ifthe Explorer is busy loading a ﬁle,the status box will say so.Right-clicking anywhere inside this box brings up a little menu with two options:display the amount ofmemory available to Weka,and run the Java garbage col-lector.Note that the garbage collector runs constantly as a background taskanyway.Clicking the Logbutton opens a textual log ofthe actions that Weka has per-formed in this session,with timestamps.As noted earlier,the little bird at the lower right ofthe window jumps up anddances when Weka is active.The number beside the ¥shows how many con-current processes that are running.Ifthe bird is standing but stops moving,it’ssick! Something has gone wrong,and you should restart the Explorer.Loading and ﬁltering ﬁlesAlong the top ofthe Preprocesspanel in Figure 10.3(b) are buttons for openingﬁles,URLs,and databases.Initially,only ﬁles whose names end in .arffappearin the ﬁle browser;to see others,change the Formatitem in the ﬁle selectionbox.Converting ﬁles to ARFFWeka has three ﬁle format converters:for spreadsheet ﬁles with the extension.csv,for C4.5’s native ﬁle format with the extensions .namesand .data,and forserialized instances with the extension .bsi.The appropriate converter is usedbased on the extension.IfWeka cannot load the data,it tries to interpret it asARFF.Ifthat fails,it pops up the box shown in Figure 10.7(a).380CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 38010.2EXPLORING THE EXPLORER381This is a generic object editor,used throughout Weka for selecting and conﬁguring objects.For example,when you set parameters for a classiﬁer,youuse the same kind ofbox.The CSVLoaderfor .csvﬁles is selected by default,andthe Morebutton gives you more information about it,shown in Figure 10.7(b).It is always worth looking at the documentation! In this case,it explains thatthe spreadsheet’s ﬁrst row determines the attribute names.Click OKto use thisconverter.For a different one,click Chooseto select from the list in Figure10.7(c).The ArffLoaderis the ﬁrst option,and we reached this point only because itfailed.The CSVLoaderis the default,and we clicked Choosebecause we want adifferent one.The third option is for the C4.5 format,in which there are twoﬁles for a dataset,one giving ﬁeld names and the other giving the actual data.The fourth,for serialized instances,is for reloading a dataset that has been savedas a Java serialized object.Any Java object can be saved in this form and reloaded.As a native Java format,it is quicker to load than an ARFF ﬁle,which must beparsed and checked.When repeatedly reloading a large dataset it may be worthsaving it in this form.Further features ofthe generic object editor in Figure 10.7(a) are Save,whichsaves a conﬁgured object,and Open,which opens a previously saved one.Theseare not useful for this particular kind ofobject.But other generic object editorpanels have many editable properties,and having gone to some trouble to setthem up you may want to save the conﬁgured object to reuse later.(a)(b)(c)Figure 10.7Generic object editor:(a) the editor,(b) more information (click More),and(c) choosing a converter (click Choose).P088407-Ch010.qxd  4/30/05  10:59 AM  Page 381Files on your computer are not the only source ofdatasets for Weka.You canopen a URL,and Weka will use the hypertext transfer protocol (HTTP) todownload an ARFF ﬁle from the Web.Or you can open a database (Open DB)—any database that has a Java database connectivity (JDBC) driver—and retrieveinstances using the SQL Selectstatement.This returns a relation that Weka readsin as an ARFF ﬁle.To make this work with your database,you may need tomodify the ﬁle weka/experiment/DatabaseUtils.propsin the Weka distributionby adding your database driver to it.(To access this ﬁle,expand the weka.jarﬁlein the Weka distribution.)Data can be saved in all these formats using the Savebutton in Figure 10.3(b).Apart from loading and saving datasets,the Preprocesspanel also allows you toﬁlter them.Filters are an important component ofWeka.Using ﬁltersClicking Choose(near the top left) in Figure 10.3(b) gives a list ofﬁlters like thatin Figure 10.8(a).Actually,you get a collapsed version:click on an arrow to openup its contents.We will describe how to use a simple ﬁlter to delete speciﬁedattributes from a dataset,in other words,to perform manual attribute selection.The same effect can be achieved more easily by selecting the relevant attributesusing the tick boxes and pressing the Removebutton.Nevertheless,we describethe equivalent ﬁltering operation explicitly,as an example.Removeis an unsupervised attribute ﬁlter,and to see it you must scroll furtherdown the list.When selected,it appears in the line beside the Choosebutton,along with its parameter values—in this case the line reads simply “Remove.”Click that line to bring up a generic object editor with which you can examineand alter the ﬁlter’s properties.(You did the same thing earlier by clicking theJ48line in Figure 10.4(b) to open the J4.8 classiﬁer’s object editor.) The objecteditor for the Removeﬁlter is shown in Figure 10.8(b).To learn about it,clickMoreto show the information in Figure 10.8(c).This explains that the ﬁlterremoves a range ofattributes from the dataset.It has an option,attributeIndices,that speciﬁes the range to act on and another called invertSelectionthat deter-mines whether the ﬁlter selects attributes or deletes them.There are boxes forboth ofthese in the object editor shown in Figure 10.8(b),and in fact we havealready set them to 1,2(to affect attributes 1 and 2,namely,outlookand tem-perature) and False(to remove rather than retain them).Click OKto set theseproperties and close the box.Notice that the line beside the Choosebutton nowreads Remove -R 1,2.In the command-line version ofthe Removeﬁlter,theoption -Ris used to specify which attributes to remove.After conﬁguring anobject it’s often worth glancing at the resulting command-line formulation thatthe Explorer sets up.Apply the ﬁlter by clicking Apply(at the right-hand side ofFigure 10.3(b)).Immediately the screen in Figure 10.9 appears—just like the one in Figure382CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 38210.2EXPLORING THE EXPLORER38310.3(b) but with only three attributes,humidity,windy,and play.At this pointthe fourth button in the row near the top becomes active.Undoreverses the ﬁl-tering operation and restores the original dataset,which is useful when youexperiment with different ﬁlters.The ﬁrst attribute,humidity,is selected and a summary ofits values appearson the right.As a numeric attribute,the minimum and maximum values,mean,and standard deviation are shown.Below is a histogram that shows the distri-(a)(c)(b)Figure 10.8Choosing a ﬁlter:(a) the ﬁltersmenu,(b) an object editor,and (c) moreinformation (click More).P088407-Ch010.qxd  4/30/05  10:59 AM  Page 383bution ofthe playattribute.Unfortunately,this display is impoverished becausethe attribute has so few different values that they fall into two equal-sized bins.More realistic datasets yield more informative histograms.Training and testing learning schemesThe Classifypanel lets you train and test learning schemes that perform classi-ﬁcation or regression.Section 10.1 explained how to interpret the output ofa decision tree learner and showed the performance ﬁgures that are auto-matically generated by the evaluation module.The interpretation ofthese is thesame for all models that predict a categorical class.However,when evaluatingmodels for numeric prediction,Weka produces a different set ofperformancemeasures.As an example,in Figure 10.10(a) the CPU performance dataset from Table1.5 (page 16) has been loaded into Weka.You can see the histogram ofvaluesofthe ﬁrst attribute,vendor,at the lower right.In Figure 10.10(b) the modeltree inducer M5¢has been chosen as the classiﬁer by going to the Classifypanel,clicking the Choosebutton at the top left,opening up the treessection ofthehierarchical menu shown in Figure 10.4(a),ﬁnding M5P,and clicking Start.Thehierarchy helps to locate particular classiﬁers by grouping items with commonfunctionality.384CHAPTER 10|THE EXPLORERFigure 10.9The weather data with two attributes removed.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 38410.2EXPLORING THE EXPLORER385Figure 10.11 shows the output.The pruned model tree is simply a decisionstump with a split on the MMAXattribute and two linear models,one for eachleaf.Both models involve a nominal attribute,vendor,as well as some numericones.The expressionvendor=adviser,sperry,amdahlis interpreted as follows:ifvendoris either adviser,sperry,or amdahl,then substitute 1;otherwise,substi-tute 0.The description ofthe model tree is followed by several ﬁgures thatmeasure its performance.These are derived from the test option chosen inFigure 10.10(b),10-fold cross-validation (not stratiﬁed,because that doesn’t(a)(b)Figure 10.10Processing the CPU performance data with M5¢.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 385+ 0.0016 * MMIN + 0.0034 * MMAX === Run information ===Scheme:       weka.classifiers.trees.M5P -M 4.0Relation:     cpuInstances:    209Attributes:   8               vendor              MYCT              MMIN               MMAX              CACH              CHMIN              CHMAX              classTest mode:    10-fold cross-validation=== Classifier model (full training set) ===M5 pruned model tree:(using smoothed linear models)MMAX <= 14000 : LM1 (141/4.178%)MMAX >  14000 : LM2 (68/50.073%)LM num: 1 class = -2.0542 * vendor=honeywell,ipl,ibm,cdc,ncr,basf,gould,siemens,nas,adviser,sperry,amdahl + 5.4303 * vendor=adviser,sperry,amdahl - 5.7791 * vendor=amdahl + 0.0064 * MYCT + 0.5524 * CACH + 1.1411 * CHMIN + 0.0945 * CHMAX + 4.1463vendor=honeywell,ipl,ibm,cdc,ncr,basf,gould,siemens,nas,adviser,sperry,amdahl+ 46.1469 * vendor=adviser,sperry,amdahl - 58.0762 * vendor=amdahl LM num: 2 class = -57.3649 * Figure 10.11Output from the M5¢program for numeric prediction.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 38610.2EXPLORING THE EXPLORER387make sense for numeric prediction).Section 5.8 (Table 5.8) explains themeaning ofthe various measures.Ordinary linear regression (Section 4.6),another scheme for numeric pre-diction,is found under LinearRegressionin the functionssection ofthe menu inFigure 10.4(a).It builds a single linear regression model rather than the two inFigure 10.11;not surprisingly,its performance is slightly worse.To get a feel for their relative performance,let’s visualize the errors theseschemes make,as we did for the Iris dataset in Figure 10.6(b).Right-click theentry in the history list and select Visualize classiﬁer errorsto bring up the two-dimensional plot ofthe data in Figure 10.12.The points are color coded byclass—but in this case the color varies continuously because the class is numeric.In Figure 10.12 the Vendorattribute has been selected for the X-axis and theinstance number has been chosen for the Y-axis because this gives a good spreadofpoints.Each data point is marked by a cross whose size indicates the absolutevalue ofthe error for that instance.The smaller crosses in Figure 10.12(a) (forM5¢),when compared with those in Figure 10.12(b) (for linear regression),show that M5¢is superior.+ 0.0162 * MMIN + 0.0086 * MMAX + 0.8332 * CACH - 1.2665 * CHMIN + 1.2741 * CHMAX - 107.243Number of Rules : 2 Time taken to build model: 1.37 seconds=== Cross-validation ====== Summary ===Correlation coefficient                  0.9766Mean absolute error                     13.6917Root mean squared error                 35.3003Relative absolute error                 15.6194 % Root relative squared error             22.8092 % Total Number of Instances              209     + 0.012 * MYCT Figure 10.11(continued)P088407-Ch010.qxd  4/30/05  10:59 AM  Page 387Do it yourself: The User ClassiﬁerThe User Classiﬁer (mentioned at the end ofSection 3.2) allows Weka users tobuild their own classiﬁers interactively.It resides in the treessection ofthe hier-archical menu in Figure 10.4(a) under UserClassiﬁer.We illustrate its operationon a new problem,segmenting visual image data into classes such as grass,sky,foliage,brick,and cementbased on attributes giving average intensity,hue,size,388CHAPTER 10|THE EXPLORER(a)(b)Figure 10.12Visualizing the errors:(a) from M5¢and (b) from linear regression.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 38810.2EXPLORING THE EXPLORER389position,and various simple textural features.The training data ﬁle is suppliedwith the Weka distribution and called segment-challenge.arff.Having loaded itin,select the User Classiﬁer.For evaluation use the special test set called segment-test.arffas the Supplied test seton the Classifypanel.Evaluation by cross-validation is impossible when you have to construct a classiﬁer manually foreach fold.Following Start,a new window appears and Weka waits for you to build theclassiﬁer.The Tree Visualizerand Data Visualizertabs switch between differentviews.The former shows the current state ofthe classiﬁcation tree,and each nodegives the number ofinstances ofeach class at that node.The aim is to come upwith a tree in which the leafnodes are as pure as possible.Initially there is onlyone node,the root,which contains all the data.Switch to the Data Visualizertocreate a split.This shows the same two-dimensional plot that we saw in Figure10.6(b) for the Iris dataset and Figure 10.12 for the CPU performance data.Theattributes to use for X and Y are selected as before,and the goal here is to ﬁnd acombination that separates the classes as cleanly as possible.Figure 10.13(a)shows a good choice:region–centroid–rowfor X and intensity–meanfor Y.Having found a good separation,you must specify a region in the graph.Fourtools for this appear in the pull-down menu below the Y-axis selector.SelectInstanceidentiﬁes a particular instance.Rectangle(shown in Figure 10.13(a))allows you to drag out a rectangle on the graph.With Polygonand Polylineyoubuild a free-form polygon or draw a free-form polyline (left-click to add a vertexand right-click to complete the operation).Once an area has been selected,itturns gray.In Figure 10.13(a) the user has deﬁned a rectangle.The Submitbutton creates two new nodes in the tree,one holding the selected instances andthe other with all the rest.Clearclears the selection;Savesaves the instances inthe current tree node as an ARFF ﬁle.At this point,the Tree Visualizershows the tree in Figure 10.13(b).There isa pure node for the skyclass,but the other node is mixed and should be splitfurther.Clicking on different nodes determines which subset ofdata is shownby the Data Visualizer.Continue adding nodes until you are satisﬁed with theresult—that is,until the leafnodes are mostly pure.Then right-click on anyblank space in the Tree Visualizerand choose Accept the Tree.Weka evaluatesyour tree on the test set and outputs performance statistics (80% is a good scoreon this problem).Building trees manually is very tedious.But Weka can complete the task foryou by building a subtree under any node:just right-click the node.Using a metalearnerMetalearners (Section 7.5) take simple classiﬁers and turn them into more pow-erful learners.For example,to boost decision stumps in the Explorer,go to theP088407-Ch010.qxd  4/30/05  10:59 AM  Page 389390CHAPTER 10|THE EXPLORER(a)(b)Figure 10.13Working on the segmentation data with the User Classiﬁer:(a) the datavisualizer and (b) the tree visualizer.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 39010.2EXPLORING THE EXPLORER391Classifypanel and choose the classiﬁer AdaboostM1from the metasection ofthehierarchical menu.When you conﬁgure this classiﬁer by clicking it,the objecteditor shown in Figure 10.14 appears.This has its own classiﬁer ﬁeld,which weset to DecisionStump(as shown).This method could itselfbe conﬁgured byclicking (except that DecisionStumphappens to have no editable properties).Click OKto return to the main Classifypanel and Startto try out boosting deci-sion stumps up to 10 times.It turns out that this mislabels only 7 ofthe 150instances in the Iris data—good performance considering the rudimentarynature ofdecision stumps and the rather small number ofboosting iterations.Clustering and association rulesUse the Clusterand Associatepanels to invoke clustering algorithms (Section6.6) and methods for ﬁnding association rules (Section 4.5).When clustering,Weka shows the number ofclusters and how many instances each cluster con-tains.For some algorithms the number ofclusters can be speciﬁed by setting aparameter in the object editor.For probabilistic clustering methods,Weka mea-sures the log-likelihood ofthe clusters on the training data:the larger this quan-tity,the better the model ﬁts the data.Increasing the number ofclustersnormally increases the likelihood,but may overﬁt.The controls on the Clusterpanel are similar to those for Classify.You canspecify some ofthe same evaluation methods—use training set,supplied testset,and percentage split (the last two are used with the log-likelihood).A furtherFigure 10.14Conﬁguring a metalearner for boosting decision stumps.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 391method,classes to clusters evaluation,compares how well the chosen clustersmatch a preassigned class in the data.You select an attribute (which must benominal) that represents the “true”class.Having clustered the data,Wekadetermines the majority class in each cluster and prints a confusion matrixshowing how many errors there would be ifthe clusters were used instead ofthe true class.Ifyour dataset has a class attribute,you can ignore it during clus-tering by selecting it from a pull-down list ofattributes,and see how well theclusters correspond to actual class values.Finally,you can choose whether ornot to store the clusters for visualization.The only reason not to do so is to con-serve space.As with classiﬁers,you visualize the results by right-clicking on theresult list,which allows you to view two-dimensional scatter plots like the onein Figure 10.6(b).Ifyou have chosen classes to clusters evaluation,the classassignment errors are shown.For the Cobwebclustering scheme,you can alsovisualize the tree.The Associatepanel is simpler than Classifyor Cluster.Weka contains onlythree algorithms for determining association rules and no methods for evalu-ating such rules.Figure 10.15 shows the output from the Apriori program forassociation rules (described in Section 4.5) on the nominal version oftheweather data.Despite the simplicity ofthe data,several rules are found.Thenumber before the arrow is the mumber ofinstances for which the antecedentis true;that after the arrow is the number ofinstances in which the consequentis true also;and the conﬁdence (in parentheses) is the ratio between the two.Ten rules are found by default:you can ask for more by using the object editorto change numRules.Attribute selectionThe Select attributespanel gives access to several methods for attribute selection.As explained in Section 7.1,this involves an attribute evaluator and a search392CHAPTER 10|THE EXPLORER1. outlook=overcast 4 ==> play=yes 4    conf:(1)2. temperature=cool 4 ==> humidity=normal 4    conf:(1)3. humidity=normal windy=FALSE 4 ==> play=yes 4    conf:(1)4. outlook=sunny play=no 3 ==> humidity=high 3    conf:(1)5. outlook=sunny humidity=high 3 ==> play=no 3    conf:(1)6. outlook=rainy play=yes 3 ==> windy=FALSE 3    conf:(1)7. outlook=rainy windy=FALSE 3 ==> play=yes 3    conf:(1)8. temperature=cool play=yes 3 ==> humidity=normal 3    conf:(1)9. outlook=sunny temperature=hot 2 ==> humidity=high 2    conf:(1)10. temperature=hot play=no 2 ==> outlook=sunny 2    conf:(1)Figure 10.15Output from the Apriori program for association rules.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 39210.3FILTERING ALGORITHMS393method.Both are chosen in the usual way and conﬁgured with the object editor.You must also decide which attribute to use as the class.Attribute selection canbe performed using the full training set or using cross-validation.In the lattercase it is done separately for each fold,and the output shows how many times—that is,in how many ofthe folds—each attribute was selected.The results arestored in the history list.When you right-click an entry here you can visualizethe dataset in terms ofthe selected attributes (choose Visualize reduced data).VisualizationThe Visualizepanel helps you visualize a dataset—not the result ofa classiﬁ-cation or clustering model,but the dataset itself.It displays a matrix oftwo-dimensional scatter plots ofevery pair ofattributes.Figure 10.16(a) shows theiris dataset.You can select an attribute—normally the class—for coloring thedata points using the controls at the bottom.Ifit is nominal,the coloring is dis-crete;ifit is numeric,the color spectrum ranges continuously from blue (lowvalues) to orange (high values).Data points with no class value are shown inblack.You can change the size ofeach plot,the size ofthe points,and the amountofjitter,which is a random displacement applied to X and Y values to separatepoints that lie on top ofone another.Without jitter,1000 instances at the samedata point would look just the same as 1 instance.You can reduce the size ofthe matrix ofplots by selecting certain attributes,and you can subsample thedata for efﬁciency.Changes in the controls do not take effect until the Updatebutton is clicked.Click one ofthe plots in the matrix to enlarge it.For example,clicking onthe top left plot brings up the panel in Figure 10.16(b).You can zoom in on anyarea ofthis panel by choosing Rectanglefrom the menu near the top right anddragging out a rectangle on the viewing area like that shown.The Submitbuttonnear the top left rescales the rectangle into the viewing area.10.3Filtering algorithmsNow we take a detailed look at the ﬁltering algorithms implemented withinWeka.These are accessible from the Explorer,and also from the Knowledge Flowand Experimenter interfaces described in Chapters 11 and 12.All ﬁlters trans-form the input dataset in some way.When a ﬁlter is selected using the Choosebutton,its name appears in the line beside that button.Click that line to get ageneric object editor to specify its properties.What appears in the line is thecommand-line version ofthe ﬁlter,and the parameters are speciﬁed with minussigns.This is a good way oflearning how to use the Weka commands directly.There are two kinds ofﬁlter:unsupervised and supervised (Section 7.2).Thisseemingly innocuous distinction masks a rather fundamental issue.Filters areP088407-Ch010.qxd  4/30/05  10:59 AM  Page 393often applied to a training dataset and then also applied to the test ﬁle.Iftheﬁlter is supervised—for example,ifit uses class values to derive good intervalsfor discretization—applying it to the test data will bias the results.It is the dis-cretization intervals derived from the trainingdata that must be applied to thetest data.When using supervised ﬁlters you must be careful to ensure that the results are evaluated fairly,an issue that does not arise with unsupervisedﬁlters.We treat Weka’s unsupervised and supervised ﬁltering methods separately.Within each type there is a further distinction between attribute ﬁlters,whichwork on the attributes in the datasets,and instance ﬁlters,which work on theinstances.To learn more about a particular ﬁlter,select it in the Weka Explorer394CHAPTER 10|THE EXPLORER(a)Figure 10.16Visualizing the Iris dataset.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 39410.3FILTERING ALGORITHMS395and look at its associated object editor,which deﬁnes what the ﬁlter does andthe parameters it takes.Unsupervised attribute ﬁltersTable 10.1 lists Weka’s unsupervised attribute ﬁlters.Many ofthe operationswere introduced in Section 7.3.Adding and removing attributesAddinserts an attribute at a given position,whose value is declared to be missingfor all instances.Use the generic object editor to specify the attribute’s name,where it will appear in the list ofattributes,and its possible values (for nominalattributes).Copycopies existing attributes so that you can preserve them whenexperimenting with ﬁlters that overwrite attribute values.Several attributes canbe copied together using an expression such as 1–3for the ﬁrst three attributes,(b)Figure 10.16(continued)P088407-Ch010.qxd  4/30/05  10:59 AM  Page 395396CHAPTER 10|THE EXPLORERTable 10.1Unsupervised attribute ﬁlters.NameFunctionAddAdd a new attribute, whose values are all marked as missing.AddClusterAdd a new nominal attribute representing the cluster assigned to each instance by a given clustering algorithm.AddExpressionCreate a new attribute by applying a speciﬁed mathematical function to existing attributes.AddNoiseChange a percentage of a given nominal attribute’s values.ClusterMembershipUse a clusterer to generate cluster membership values, which then form the new attributes.CopyCopy a range of attributes in the dataset.DiscretizeConvert numeric attributes to nominal: Specify which attributes, number of bins, whether to optimize the number of bins, and output binary attributes. Use equal-width (default) or equal-frequency binning.FirstOrderApply a ﬁrst-order differencing operator to a range of numeric attributes.MakeIndicatorReplace a nominal attribute with a Boolean attribute. Assign value 1 to instances with a particular range of attribute values; otherwise, assign 0. By default, the Boolean attribute is coded as numeric.MergeTwoValuesMerge two values of a given attribute: Specify the index of the two values to be merged.NominalToBinaryChange a nominal attribute to several binary ones, one for each value.NormalizeScale all numeric values in the dataset to lie within the interval [0,1].NumericToBinaryConvert all numeric attributes into binary ones: Nonzero values become 1.NumericTransformTransform a numeric attribute using any Java function.ObfuscateObfuscate the dataset by renaming the relation, all attribute names, and nominal and string attribute values.PKIDiscretizeDiscretize numeric attributes using equal-frequency binning, where the number of bins is equal to the square root of the number of values (excluding missing values).RandomProjectionProject the data onto a lower-dimensional subspace using a random matrix.RemoveRemove attributes.RemoveTypeRemove attributes of a given type (nominal, numeric, string, or date).RemoveUselessRemove constant attributes, along with nominal attributes that vary too much.ReplaceMissingValuesReplace all missing values for nominal and numeric attributes with the modes and means of the training data.StandardizeStandardize all numeric attributes to have zero mean and unit variance.StringToNominalConvert a string attribute to nominal.StringToWordVectorConvert a string attribute to a vector that represents word occurrence frequencies; you can choose the delimiter(s)—and there are many more options.SwapValuesSwap two values of an attribute.TimeSeriesDeltaReplace attribute values in the current instance with the difference between the current value and the value in some previous (or future) instance.TimeSeriesTranslateReplace attribute values in the current instance with the equivalent value in some previous (or future) instance.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 39610.3FILTERING ALGORITHMS397or ﬁrst-3,5,9-lastfor attributes 1,2,3,5,9,10,11,12,....The selection can beinverted,affecting all attributes exceptthose speciﬁed.These features are sharedby many ﬁlters.Removehas already been described.Similar ﬁlters are RemoveType,whichdeletes all attributes ofa given type (nominal,numeric,string,or date),andRemoveUseless,which deletes constant attributes and nominal attributes whosevalues are different for almost all instances.You can decide how much variationis tolerated before an attribute is deleted by specifying the number ofdistinctvalues as a percentage ofthe total number ofvalues.Some unsupervised attrib-ute ﬁlters behave differently ifthe menu in the Preprocesspanel has been usedto set a class attribute.For example,RemoveTypeand RemoveUselessboth skipthe class attribute.AddClusterapplies a clustering algorithm to the data before ﬁltering it.Youuse the object editor to choose the clustering algorithm.Clusterers are conﬁg-ured just as ﬁlters are (Section 10.6).The AddClusterobject editor contains itsown Choosebutton for the clusterer,and you conﬁgure the clusterer by clickingits line and getting anotherobject editor panel,which must be ﬁlled in beforereturning to the AddClusterobject editor.This is probably easier to understandwhen you do it in practice than when you read about it in a book! At any rate,once you have chosen a clusterer,AddClusteruses it to assign a cluster numberto each instance,as a new attribute.The object editor also allows you to ignorecertain attributes when clustering,speciﬁed as described previously for Copy.ClusterMembershipuses a clusterer,again speciﬁed in the ﬁlter’s object editor,to generate membership values.A new version ofeach instance is created whoseattributes are these values.The class attribute,ifset,is left unaltered.AddExpressioncreates a new attribute by applying a mathematical functionto numeric attributes.The expression can contain attribute references and con-stants;the arithmetic operators +,-,*,/,and Ÿ;the functions logand exp,absand sqrt,ﬂoor,ceiland rint,5and sin,cos,and tan;and parentheses.Attributesare speciﬁed by the preﬁx a,for example,a7 is the seventh attribute.An exampleexpression isThere is a debug option that replaces the new attribute’s value with a postﬁxparse ofthe supplied expression.Whereas AddExpressionapplies mathematical functions,NumericTransformperforms an arbitrary transformation by applying a given Java function toselected numeric attributes.The function can be anything that takes a doubleasits argument and returns another double,for example,sqrt()in java.lang.Math.aaa125740Ÿ**()log.5The rintfunction rounds to the closest integer.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 397One parameter is the name ofthe Java class that implements the function(which must be a fully qualiﬁed name);another is the name ofthe transfor-mation method itself.Normalizescales all numeric values in the dataset to lie between 0 and 1.Stan-dardizetransforms them to have zero mean and unit variance.Both skip theclass attribute,ifset.Changing valuesSwapValuesswaps the positions oftwo values ofa nominal attribute.The orderofvalues is entirely cosmetic—it does not affect learning at all—but ifthe classis selected,changing the order affects the layout ofthe confusion matrix.MergeTwoValuesmerges values ofa nominal attribute into a single category.Thenew value’s name is a concatenation ofthe two original ones,and every occur-rence ofeither ofthe original values is replaced by the new one.The index ofthe new value is the smaller ofthe original indices.For example,ifyou mergethe ﬁrst two values ofthe outlookattribute in the weather data—in which thereare ﬁve sunny,four overcast,and ﬁve rainyinstances—the new outlookattributewill have values sunny_overcastand rainy;there will be nine sunny_overcastinstances and the original ﬁve rainyones.One way ofdealing with missing values is to replace them globally beforeapplying a learning scheme.ReplaceMissingValuesreplaces each missing valuewith the mean for numeric attributes and the mode for nominal ones.Ifa classis set,missing values ofthat attribute are not replaced.ConversionsMany ﬁlters convert attributes from one form to another.Discretizeuses equal-width or equal-frequency binning (Section 7.2) to discretize a range ofnumericattributes,speciﬁed in the usual way.For the former method the number ofbinscan be speciﬁed or chosen automatically by maximizing the likelihood usingleave-one-out cross-validation.PKIDiscretizediscretizes numeric attributesusing equal-frequency binning in which the number ofbins is the square rootofthe number ofvalues (excluding missing values).Both these ﬁlters skip theclass attribute.MakeIndicatorconverts a nominal attribute into a binary indicator attributeand can be used to transform a multiclass dataset into several two-class ones.Itsubstitutes a binary attribute for the chosen nominal one,whose value for eachinstance is 1 ifa particular original value was present and 0 otherwise.The newattribute is declared to be numeric by default,but it can be made nominal ifdesired.Some learning schemes,such as support vector machines,only handle binaryattributes.The NominalToBinaryﬁlter transforms all multivalued nominal398CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 39810.3FILTERING ALGORITHMS399attributes in a dataset into binary ones,replacing each attribute with kvaluesby kbinary attributes using a simple one-per-value encoding.Attributes that arealready binary are left untouched.NumericToBinaryconverts all numeric attrib-utes into nominal binary ones (except the class,ifset).Ifthe value ofthenumeric attribute is exactly 0,the new attribute will be 0,and ifit is missing,the new attribute will be missing;otherwise,the value ofthe new attribute willbe 1.These ﬁlters also skip the class attribute.FirstOrdertakes a range ofNnumeric attributes and replaces them with N-1 numeric attributes whose values are the differences between consecutiveattribute values from the original instances.For example,ifthe original attrib-ute values were 3,2,and 1,the new ones will be -1 and -1.String conversionA string attribute has an unspeciﬁed number ofvalues.StringToNominalcon-verts it to nominal with a set number ofvalues.You should ensure that all stringvalues that will appear in potential test data are represented in the dataset.StringToWordVectorproduces attributes that represent the frequency ofeachword in the string.The set ofwords—that is,the new attribute set—is deter-mined from the dataset.By default each word becomes an attribute whose valueis 1 or 0,reﬂecting that word’s presence in the string.The new attributes can benamed with a user-determined preﬁx to keep attributes derived from differentstring attributes distinct.There are many options that affect tokenization.Words can be formed fromcontiguous alphabetic sequences or separated by a given set ofdelimiter char-acters.They can be converted to lowercase before being added to the diction-ary,or all words on a predetermined list ofEnglish stopwords can be ignored.Words that are not among the top kwords ranked by frequency can be discarded(slightly more than kwords will be retained ifthere are ties at the kth position).Ifa class attribute has been assigned,the top kwords for each class will be kept.The value ofeach word attribute reﬂects its presence or absence in the string,but this can be changed.A count ofthe number oftimes the word appears inthe string can be used instead.Word frequencies can be normalized to give eachdocument’s attribute vector the same Euclidean length—this length is notchosen to be 1,to avoid the very small numbers that would entail,but to be theaverage length ofall documents that appear as values ofthe original stringattribute.Alternatively,the frequencies fijfor word iin document jcan be trans-formed using log (1 +fij) or the TF ¥IDF measure (Section 7.3).Time seriesTwo ﬁlters work with time series data.TimeSeriesTranslatereplaces the valuesofan attribute (or attributes) in the current instance with the equivalent valueP088407-Ch010.qxd  4/30/05  10:59 AM  Page 399in some other (previous or future) instance.TimeSeriesDeltareplaces attributevalues in the current instance with the difference between the current value andthe value in some other instance.In both cases instances in which the time-shifted value is unknown may be removed,or missing values may be used.RandomizingOther attribute ﬁlters degrade the data.AddNoisetakes a nominal attribute andchanges a given percentage ofits values.Missing values can be retained orchanged along with the rest.Obfuscateanonymizes data by renaming the rela-tion,attribute names,and nominal and string attribute values.RandomProjec-tionprojects the dataset on to a lower-dimensional subspace using a randommatrix with columns ofunit length (Section 7.3).The class attribute is notincluded in the projection.Unsupervised instance ﬁltersWeka’s instance ﬁlters,listed in Table 10.2,affect all instances in a dataset ratherthan all values ofa particular attribute or attributes.Randomizing and subsamplingYou can Randomizethe order ofinstances in the dataset.Normalizetreats allnumeric attributes (excluding the class) as a vector and normalizes it to a givenlength.You can specify the vector length and the norm to be used.There are various ways ofgenerating subsets ofthe data.Use Resampletoproduce a random sample by sampling with replacement or RemoveFoldsto split400CHAPTER 10|THE EXPLORERTable 10.2Unsupervised instance ﬁlters.NameFunctionNonSparseToSparseConvert all incoming instances to sparse format (Section 2.4)NormalizeTreat numeric attributes as a vector and normalize it to a given lengthRandomizeRandomize the order of instances in a datasetRemoveFoldsOutput a speciﬁed cross-validation fold for the datasetRemoveMisclassiﬁedRemove instances incorrectly classiﬁed according to a speciﬁed classiﬁer—useful for removing outliersRemovePercentageRemove a given percentage of a datasetRemoveRangeRemove a given range of instances from a datasetRemoveWithValuesFilter out instances with certain attribute valuesResampleProduce a random subsample of a dataset, sampling with replacementSparseToNonSparseConvert all incoming sparse instances into nonsparse formatP088407-Ch010.qxd  4/30/05  10:59 AM  Page 40010.3FILTERING ALGORITHMS401it into a given number ofcross-validation folds and reduce it to just one ofthem.Ifa random number seed is provided,the dataset will be shufﬂed before thesubset is extracted.RemovePercentageremoves a given percentage ofinstances,and RemoveRangeremoves a certain range ofinstance numbers.To remove allinstances that have certain values for nominal attributes,or numeric valuesabove or below a certain threshold,use RemoveWithValues.By default allinstances are deleted that exhibit one ofa given set ofnominal attribute values(ifthe speciﬁed attribute is nominal) or a numeric value below a given thresh-old (ifit is numeric).However,the matching criterion can be inverted.You can remove outliers by applying a classiﬁcation method to the dataset(specifying it just as the clustering method was speciﬁed previously for AddCluster) and use RemoveMisclassiﬁedto delete the instances that it misclassiﬁes.Sparse instancesThe NonSparseToSparseand SparseToNonSparseﬁlters convert between theregular representation ofa dataset and its sparse representation (see Section2.4).Supervised ﬁltersSupervised ﬁlters are available from the Explorer’s Preprocesspanel,just as unsu-pervised ones are.You need to be careful with them because,despite appear-ances,they are not really preprocessing operations.We noted this previouslywith regard to discretization—the test data splits must not use the test data’sclass values because these are supposed to be unknown—and it is true for super-vised ﬁlters in general.Because ofpopular demand,Weka allows you to invoke supervised ﬁlters asa preprocessing operation,just like unsupervised ﬁlters.However,ifyou intendto use them for classiﬁcation you should adopt a different methodology.A meta-learner is provided that invokes a ﬁlter in a way that wraps the learning algo-rithm into the ﬁltering mechanism.This ﬁlters the test data using the ﬁlter thathas been created by the training data.It is also useful for some unsupervisedﬁlters.For example,in StringToWordVectorthe dictionary will be created fromthe training data alone:words that are novel in the test data will be discarded.To use a supervised ﬁlter in this way,invoke the FilteredClassiﬁermetalearningscheme from in the metasection ofthe menu displayed by the Classifypanel’sChoosebutton.Figure 10.17(a) shows the object editor for this metalearningscheme.With it you choose a classiﬁer and a ﬁlter.Figure 10.17(b) shows themenu ofﬁlters.Supervised ﬁlters,like unsupervised ones,are divided into attribute andinstance ﬁlters,listed in Table 10.3 and Table 10.4.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 401Supervised attribute ﬁltersDiscretize,highlighted in Figure 10.17,uses the MDL method ofsupervised dis-cretization (Section 7.2).You can specify a range ofattributes or force the dis-cretized attribute to be binary.The class must be nominal.By default Fayyadand Irani’s (1993) criterion is used,but Kononenko’s method (1995) is anoption.402CHAPTER 10|THE EXPLORER(a)(b)Figure 10.17Using Weka’s metalearner for discretization:(a) conﬁguring FilteredClas-siﬁer,and (b) the menu ofﬁlters.Table 10.3Supervised attribute ﬁlters.NameFunctionAttributeSelectionProvides access to the same attribute selection methods as the Select attributespanelClassOrderRandomize, or otherwise alter, the ordering of class valuesDiscretizeConvert numeric attributes to nominalNominalToBinaryConvert nominal attributes to binary, using a supervised method if the class is numericTable 10.4Supervised instance ﬁlters.NameFunctionResampleProduce a random subsample of a dataset, sampling with replacementSpreadSubsampleProduce a random subsample with a given spread between class frequencies, sampling with replacementStratiﬁedRemoveFoldsOutput a speciﬁed stratiﬁed cross-validation fold for the datasetP088407-Ch010.qxd  4/30/05  10:59 AM  Page 40210.4LEARNING ALGORITHMS403There is a supervised version ofthe NominalToBinaryﬁlter that transformsall multivalued nominal attributes to binary ones.In this version,the transfor-mation depends on whether the class is nominal or numeric.Ifnominal,thesame method as before is used:an attribute with kvalues is transformed into kbinary attributes.Ifthe class is numeric,however,the method described inSection 6.5 (page 246) is applied.In either case the class itselfis not transformed.ClassOrderchanges the ordering ofthe class values.The user determineswhether the new ordering is random or in ascending or descending order ofclass frequency.This ﬁlter must not be used with the FilteredClassiﬁermeta-learning scheme! AttributeSelectioncan be used for automatic attribute selec-tion and provides the same functionality as the Explorer’s Select attributespanel(described later).Supervised instance ﬁltersThere are three supervised instance ﬁlters.Resampleis like the eponymous un-supervised instance ﬁlter except that it maintains the class distribution in thesubsample.Alternatively,it can be conﬁgured to bias the class distributiontowards a uniform one.SpreadSubsamplealso produces a random subsample,but the frequency difference between the rarest and the most common class canbe controlled—for example,you can specify at most a 2:1 difference in classfrequencies.Like the unsupervised instance ﬁlter RemoveFolds,Strati-ﬁedRemoveFoldsoutputs a speciﬁed cross-validation fold for the dataset,exceptthat this time the fold is stratiﬁed.10.4Learning algorithmsOn the Classifypanel,when you select a learning algorithm using the Choosebutton the command-line version ofthe classiﬁer appears in the line beside thebutton,including the parameters speciﬁed with minus signs.To change them,click that line to get an appropriate object editor.Table 10.5 lists Weka’s classi-ﬁers.They are divided into Bayesian classiﬁers,trees,rules,functions,lazy clas-siﬁers,and a ﬁnal miscellaneous category.We describe them brieﬂy here,alongwith their parameters.To learn more,choose one in the Weka Explorer inter-face and examine its object editor.A further kind ofclassiﬁer,the Metalearner,is described in the next section.Bayesian classiﬁersNaiveBayesimplements the probabilistic Naïve Bayes classiﬁer (Section 4.2).NaiveBayesSimpleuses the normal distribution to model numeric attrib-utes.NaiveBayescan use kernel density estimators,which improves perform-ance ifthe normality assumption is grossly incorrect;it can also handle numericP088407-Ch010.qxd  4/30/05  10:59 AM  Page 403404CHAPTER 10|THE EXPLORERTable 10.5Classiﬁer algorithms in Weka.NameFunctionBayesAODEAveraged, one-dependence estimatorsBayesNetLearn Bayesian netsComplementNaiveBayesBuild a Complement Naïve Bayes classiﬁerNaiveBayesStandard probabilistic Naïve Bayes classiﬁerNaiveBayesMultinomialMultinomial version of Naïve BayesNaiveBayesSimpleSimple implementation of Naïve BayesNaiveBayesUpdateableIncremental Naïve Bayes classiﬁer that learns one instanceat a timeTreesADTreeBuild alternating decision treesDecisionStumpBuild one-level decision treesId3Basic divide-and-conquer decision tree algorithmJ48C4.5 decision tree learner (implements C4.5 revision 8)LMTBuild logistic model treesM5PM5¢model tree learnerNBTreeBuild a decision tree with Naïve Bayes classiﬁers at theleavesRandomForestConstruct random forestsRandomTreeConstruct a tree that considers a given number of randomfeatures at each nodeREPTreeFast tree learner that uses reduced-error pruningUserClassiﬁerAllow users to build their own decision treeRulesConjunctiveRuleSimple conjunctive rule learnerDecisionTableBuild a simple decision table majority classiﬁerJRipRIPPER algorithm for fast, effective rule inductionM5RulesObtain rules from model trees built using M5¢NngeNearest-neighbor method of generating rules usingnonnested generalized exemplarsOneR1R classiﬁerPartObtain rules from partial decision trees built using J4.8PrismSimple covering algorithm for rulesRidorRipple-down rule learnerZeroRPredict the majority class (if nominal) or the average value(if numeric)FunctionsLeastMedSqRobust regression using the median rather than the meanLinearRegressionStandard linear regressionLogisticBuild linear logistic regression modelsMultilayerPerceptronBackpropagation neural networkPaceRegressionBuild linear regression models using Pace regressionRBFNetworkImplements a radial basis function networkSimpleLinearRegressionLearn a linear regression model based on a single attributeSimpleLogisticBuild linear logistic regression models with built-inattribute selectionSMOSequential minimal optimization algorithm for supportvector classiﬁcationP088407-Ch010.qxd  4/30/05  10:59 AM  Page 40410.4LEARNING ALGORITHMS405attributes using supervised discretization.NaiveBayesUpdateableis an incre-mental version that processes one instance at a time;it can use a kernel esti-mator but not discretization.NaiveBayesMultinomialimplements themultinomial Bayes classiﬁer (Section 4.2,page 95).ComplementNaiveBayesbuilds a Complement Naïve Bayes classiﬁer as described by Rennie et al.(2003)(the TF ¥IDF and length normalization transforms used in this paper can beperformed using the StringToWordVectorﬁlter).AODE (averaged,one-dependence estimators)is a Bayesian method that aver-ages over a space ofalternative Bayesian models that have weaker independenceassumptions than Naïve Bayes (Webb et al.,2005).The algorithm may yieldmore accurate classiﬁcation than Naïve Bayes on datasets with nonindependentattributes.BayesNetlearns Bayesian networks under the assumptions made in Section6.7:nominal attributes (numeric ones are prediscretized) and no missing values(any such values are replaced globally).There are two different algorithms forestimating the conditional probability tables ofthe network.Search is doneusing K2 or the TAN algorithm (Section 6.7) or more sophisticated methodsbased on hill-climbing,simulated annealing,tabu search,and genetic algo-rithms.Optionally,search speed can be improved using AD trees (Section 6.7).There is also an algorithm that uses conditional independence tests to learn thestructure ofthe network;alternatively,the network structure can be loaded froman XML (extensible markup language) ﬁle.More details on the implementationofBayesian networks in Weka can be found in Bouckaert (2004).You can observe the network structure by right-clicking the history item andselecting Visualize graph.Figure 10.18(a) shows the graph for the nominalversion ofthe weather data,which in fact corresponds to the Naïve Bayes resultTable 10.5(continued)NameFunctionSMOregSequential minimal optimization algorithm for supportvector regressionVotedPerceptronVoted perceptron algorithmWinnowMistake-driven perceptron with multiplicative updatesLazyIB1Basic nearest-neighbor instance-based learnerIBkk-nearest-neighbor classiﬁerKStarNearest neighbor with generalized distance functionLBRLazy Bayesian Rules classiﬁerLWLGeneral algorithm for locally weighted learningMisc.HyperpipesExtremely simple, fast learner based onhypervolumes in instance spaceVFIVoting feature intervals method, simple and fastP088407-Ch010.qxd  4/30/05  10:59 AM  Page 405with all probabilities conditioned on the class value.This is because the searchalgorithm defaults to K2 with the maximum number ofparents ofa node setto one.Reconﬁguring this to three by clicking on K2in the conﬁguration panelyields the more interesting network in Figure 10.18(b).Clicking on a nodeshows its probability distribution—Figure 10.18(c) is obtained by clicking onthe windynode in Figure 10.18(b).TreesOfthe tree classiﬁers in Table 10.5 we have already seen how to use J4.8,whichreimplements C4.5 (Section 6.1).To see the options,click the line beside theChoosebutton in Figure 10.4(b) to bring up the object editor in Figure 10.19.You can build a binary tree instead ofone with multiway branches.You can set406CHAPTER 10|THE EXPLORER(a)(c)(b)Figure 10.18Visualizing a Bayesian network for the weather data (nominal version):(a)default output,(b) a version with the maximum number ofparents set to 3in the searchalgorithm,and (c) probability distribution table for the windynode in (b).P088407-Ch010.qxd  4/30/05  10:59 AM  Page 40610.4LEARNING ALGORITHMS407the conﬁdence threshold for pruning (default 0.25),and the minimum numberofinstances permissible at a leaf(default 2).Instead ofstandard C4.5 pruningyou can choose reduced-error pruning (Section 6.2).The numFoldsparameter(default 3) determines the size ofthe pruning set:the data is divided equallyinto that number ofparts and the last one used for pruning.When visualizingthe tree (pages 377–378) it is nice to be able to consult the original data points,which you can do ifsaveInstanceDatahas been turned on (it is off,or False,bydefault to reduce memory requirements).You can suppress subtree raising,yielding a more efﬁcient algorithm;force the algorithm to use the unprunedtree instead ofthe pruned one;or use Laplace smoothing for predicted proba-bilities (Section 4.2).Table 10.5 shows many other decision tree methods.Id3is the basic algo-rithm explained in Chapter 4.DecisionStump,designed for use with the boost-ing methods described later,builds one-level binary decision trees for datasetswith a categorical or numeric class,dealing with missing values by treating themas a separate value and extending a third branch from the stump.Trees built byRandomTreechooses a test based on a given number ofrandom features at eachnode,performing no pruning.RandomForestconstructs random forests bybagging ensembles ofrandom trees (Section 7.5,pages 320–321).REPTreebuilds a decision or regression tree using information gain/variancereduction and prunes it using reduced-error pruning (Section 6.2,page 203).Optimized for speed,it only sorts values for numeric attributes onceFigure 10.19Changing the parameters for J4.8.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 407(Section 6.1,page 190).It deals with missing values by splitting instances intopieces,as C4.5 does.You can set the minimum number ofinstances per leaf,maximum tree depth (useful when boosting trees),minimum proportion oftraining set variance for a split (numeric classes only),and number offolds forpruning.NBTreeis a hybrid between decision trees and Naïve Bayes.It creates treeswhose leaves are Naïve Bayes classiﬁers for the instances that reach the leaf.Whenconstructing the tree,cross-validation is used to decide whether a node shouldbe split further or a Naïve Bayes model should be used instead (Kohavi 1996).M5Pis the model tree learner described in Section 6.5.LMTbuilds logisticmodel trees (Section 7.5,page 331).LMTcan deal with binary and multiclasstarget variables,numeric and nominal attributes,and missing values.Whenﬁtting the logistic regression functions at a node,it uses cross-validation todetermine how many iterations to run just once and employs the same numberthroughout the tree instead ofcross-validating at every node.This heuristic(which you can switch off) improves the run time considerably,with little effecton accuracy.Alternatively,you can set the number ofboosting iterations to beused throughout the tree.Normally,it is the misclassiﬁcation error that cross-validation minimizes,but the root mean-squared error ofthe probabilities canbe chosen instead.The splitting criterion can be based on C4.5’s informationgain (the default) or on the LogitBoost residuals,striving to improve the purityofthe residuals.ADTreebuilds an alternating decision tree using boosting (Section 7.5,pages329–331) and is optimized for two-class problems.The number ofboosting iterations is a parameter that can be tuned to suit the dataset and the desiredcomplexity–accuracy tradeoff.Each iteration adds three nodes to the tree (onesplit node and two prediction nodes) unless nodes can be merged.The defaultsearch method is exhaustive search (Expand all paths);the others are heuristics andare much faster.You can determine whether to save instance data for visualization.RulesTable 10.5 shows many methods for generating rules.DecisionTablebuilds adecision table majority classiﬁer (Section 7.1,page 295).It evaluates featuresubsets using best-ﬁrst search and can use cross-validation for evaluation(Kohavi 1995b).An option uses the nearest-neighbor method to determine theclass for each instance that is not covered by a decision table entry,instead ofthe table’s global majority,based on the same set offeatures.OneRis the 1Rclassiﬁer (Section 4.1) with one parameter:the minimum bucket size for dis-cretization.ConjunctiveRulelearns a single rule that predicts either a numericor a nominal class value.Uncovered test instances are assigned the default class408CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 40810.4LEARNING ALGORITHMS409value (or distribution) ofthe uncovered training instances.The information gain(nominal class) or variance reduction (numeric class) ofeach antecedent is com-puted,and rules are pruned using reduced-error pruning.ZeroRis even simpler:it predicts the test data’s majority class (ifnominal) or average value (ifnumeric).Prismimplements the elementary covering algorithm for rules (Section 4.4).Partobtains rules from partial decision trees (Section 6.2,pages 207–210).Itbuilds the tree using C4.5’s heuristics with the same user-deﬁned parameters asJ4.8.M5Rulesobtains regression rules from model trees built using M5¢(Section6.5,pages 250–251).Ridorlearns rules with exceptions (Section 6.2,pages210–213) by generating the default rule,using incremental reduced-errorpruning to ﬁnd exceptions with the smallest error rate,ﬁnding the best excep-tions for each exception,and iterating.JRipimplements RIPPER (Section 6.2,pages 205–207),including heuristicglobal optimization ofthe rule set (Cohen 1995).Nngeis a nearest-neighbormethod for generating rules using nonnested generalized exemplars (Section6.4,pages 238–239).FunctionsThe functions category ofTable 10.5 includes an assorted group ofclassiﬁersthat can be written down as mathematical equations in a reasonably naturalway.Other methods,such as decision trees and rules,cannot (there are excep-tions:Naïve Bayes has a simple mathematical formulation).Three ofthemimplement linear regression (Section 4.6).SimpleLinearRegressionlearns a linearregression model based on a single attribute—it chooses the one that yields the smallest squared error.Missing values and nonnumeric attributes are notallowed.LinearRegressionperforms standard least-squares linear regression andcan optionally perform attribute selection,either by greedily using backwardelimination (Section 7.1) or by building a full model from all attributes anddropping terms one by one in decreasing order oftheir standardized coefﬁcientsuntil a stopping criteria is reached (this method was described in a slightly dif-ferent context in Section 6.5 under Pruning the tree,page 245).Both methodsuse a version ofthe AIC termination criterion ofSection 6.7 (page 277).Theimplementation has two further reﬁnements:a mechanism for detectingcollinear attributes (which can be turned off) and a ridgeparameter that stabi-lizes degenerate cases and can reduce overﬁtting by penalizing large coefﬁcients.Technically,LinearRegressionimplements ridge regression,which is described instandard statistics texts.LeastMedSqis a robust linear regression method that minimizes the median(rather than the mean) ofthe squares ofdivergences from the regression line(Section 7.4) (Rousseeuw and Leroy 1987).It repeatedly applies standard linearP088407-Ch010.qxd  4/30/05  10:59 AM  Page 409regression to subsamples ofthe data and outputs the solution that has the small-est median-squared error.SMOimplements the sequential minimal optimization algorithm for train-ing a support vector classiﬁer (Section 6.3),using polynomial or Gaussiankernels (Platt 1998,Keerthi et al.2001).Missing values are replaced globally,nominal attributes are transformed into binary ones,and attributes are nor-malized by default—note that the coefﬁcients in the output are based on thenormalized data.Normalization can be turned off,or the input can be stan-dardized to zero mean and unit variance.Pairwise classiﬁcation is used for multiclass problems.Logistic regression models can be ﬁtted to the supportvector machine output to obtain probability estimates.In the multiclass case thepredicted probabilities will be coupled pairwise (Hastie and Tibshirani 1998).When working with sparse instances,turn normalization offfor faster opera-tion.SMOregimplements the sequential minimal optimization algorithm forregression problems (Smola and Schölkopf1998).VotedPerceptronis the voted perceptron algorithm (Section 6.3,pages222–223).Winnow(Section 4.6,pages 126–128) modiﬁes the basic perceptronto use multiplicative updates.The implementation allows for a second multi-plier,b—different from 1/a—to be used in place ofthe divisions in Figure 4.11,and also provides the balanced version ofthe algorithm.PaceRegressionbuilds linear regression models using the new technique ofPace regression (Wang and Witten 2002).When there are many attributes,Paceregression is particularly good at determining which ones to discard—indeed,under certain regularity conditions it is provably optimal as the number ofattributes tends to inﬁnity.SimpleLogisticbuilds logistic regression models (Section 4.6,pages 121–124),ﬁtting them using LogitBoost with simple regression functions as base learnersand determining how many iterations to perform using cross-validation—which supports automatic attribute selection (Landwehr et al.2003).Logisticisan alternative implementation for building and using a multinomial logisticregression model with a ridge estimator to gaurd against overﬁtting by penal-izing large coefﬁcients,based on work by le Cessie and van Houwelingen (1992).RBFNetworkimplements a Gaussian radial basis function network (Section6.3,page 234),deriving the centers and widths ofhidden units using k-meansand combining the outputs obtained from the hidden layer using logistic regres-sion ifthe class is nominal and linear regression ifit is numeric.The activationsofthe basis functions are normalized to sum to one before they are fed into thelinear models.You can specify k,the number ofclusters;the maximum numberoflogistic regression iterations for nominal-class problems;the minimum stan-dard deviation for the clusters;and the ridge value for regression.Ifthe class isnominal,k-means is applied separately to each class to derive kclusters for eachclass.410CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 41010.4LEARNING ALGORITHMS411Neural networksMultilayerPerceptronis a neural network that trains using backpropagation(Section 6.3,page 227).Although listed under functions in Table 10.5,it differsfrom the other schemes because it has its own user interface.Ifyou load up thenumeric version ofthe weather data,invoke MultilayerPerceptron,set GUItoTruein its object editor,and run the network by clicking Starton the Classifypanel,the diagram in Figure 10.20 appears in a separate window.This networkhas three layers:an input layer on the left with one rectangular box for each(a)(b)Figure 10.20Using Weka’s neural-network graphical user interface.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 411attribute (colored green);a hidden layer next to it (red) to which all the inputnodes are connected;and an output layer at the right (orange).The labels at thefar right show the classes that the output nodes represent.Output nodes fornumeric classes are automatically converted to unthresholded linear units.Before clicking Startto run the network,you can alter its structure by addingnodes and connections.Nodes can be selected or deselected.All six nodes in thehidden and output layers in Figure 10.20(a) are deselected,indicated by the graycolor oftheir center.To select a node,simply click on it.This changes the colorofits center from gray to bright yellow.To deselect a node,right-click in anempty space.To add a node,ensure that none is selected and left-click anywherein the panel;the new node will be selected automatically.In Figure 10.20(a),anew node has been added at the lower center.To connect two nodes,select thestart node and then click on the end one.Ifseveral start nodes are selected,theyare all connected to the end node.Ifyou click in empty space instead,a newnode is created as the end node.Notice that connections are directional(although the directions are not shown).The start nodes remain selected;thusyou can add an entire hidden layer with just a few clicks,as shown in Figure10.20(b).To remove a node,ensure that no nodes are selected and right-clickit;this also removes all connections to it.To remove a single connection,selectone node and right-click the node at the other end.As well as conﬁguring the structure ofthe network,you can control the learn-ing rate,its momentum (Section 6.3),and the number ofpasses it will takethrough the data,called epochs.The network begins to train when you clickStart,and a running indication ofthe epoch and the error for that epoch isshown at the lower left ofthe panel in Figure 10.20.Note that the error is basedon a network that changes as the value is computed.For numeric classes theerror value depends on whether the class is normalized.The network stops whenthe speciﬁed number ofepochs is reached,at which point you can accept theresult or increase the desired number ofepochs and press Startagain to con-tinue training.MultilayerPerceptronneed not be run through the graphical interface.Severalparameters can be set from the object editor to control its operation.Ifyou areusing the graphical interface they govern the initial network structure,whichyou can override interactively.With autoBuildset,hidden layers are added andconnected up.The default is to have the one hidden layer shown in Figure10.20(a),but without autoBuildthis would not appear and there would be noconnections.The hiddenLayersparameter deﬁnes the hidden layers present andhow many nodes each one contains.Figure 10.20(a) is generated by a value of4(one hidden layer with four nodes),and although Figure 10.20(b) was createdby adding nodes interactively,it could have been generated by setting hidden-Layersto 4,5(one hidden layer with four nodes and another with ﬁve).The valueis a comma-separated list ofintegers;0gives no hidden layers.Furthermore,412CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 41210.4LEARNING ALGORITHMS413there are predeﬁned values that can be used instead ofintegers:iis the numberofattributes,othe number ofclass values,athe average ofthe two,and ttheirsum.The default,a,was used to generate Figure 10.20(a).The parameters learningRateand Momentumset values for these variables,which can be overridden in the graphical interface.A decayparameter causesthe learning rate to decrease with time:it divides the starting value by the epochnumber to obtain the current rate.This sometimes improves performance andmay stop the network from diverging.The resetparameter automatically resetsthe network with a lower learning rate and begins training again ifit is diverg-ing from the answer (this option is only available ifthe graphical user interfaceis notused).The trainingTimeparameter sets the number oftraining epochs.Alterna-tively,a percentage ofthe data can be set aside for validation (using validation-SetSize):then training continues until performance on the validation set startsto deteriorate consistently—or until the speciﬁed number ofepochs is reached.Ifthe percentage is set to zero,no validation set is used.The validationThresh-oldparameter determines how many consecutive times the validation set errorcan deteriorate before training is stopped.The nominalToBinaryFilterﬁlter is speciﬁed by default in the MultilayerPer-ceptronobject editor;turning it offmay improve performance on data in whichthe nominal attributes are really ordinal.The attributes can be normalized (withnormalizeAttributes),and a numeric class can be normalized too (with normal-izeNumericClass):both may improve performance.Lazy classiﬁersLazy learners store the training instances and do no real work until classiﬁca-tion time.IB1is a basic instance-based learner (Section 4.7) which ﬁnds thetraining instance closest in Euclidean distance to the given test instance and pre-dicts the same class as this training instance.Ifseveral instances qualify as theclosest,the ﬁrst one found is used.IBkis a k-nearest-neighbor classiﬁer that usesthe same distance metric.The number ofnearest neighbors (defaultk=1) canbe speciﬁed explicitly in the object editor or determined automatically usingleave-one-out cross-validation,subject to an upper limit given by the speciﬁedvalue.Predictions from more than one neighbor can be weighted according totheir distance from the test instance,and two different formulas are imple-mented for converting the distance into a weight.The number oftraininginstances kept by the classiﬁer can be restricted by setting the window sizeoption.As new training instances are added,the oldest ones are removed tomaintain the number oftraining instances at this size.KStaris a nearest-neighbor method with a generalized distance function based on transforma-tions (Section 6.4,pages 241–242).P088407-Ch010.qxd  4/30/05  10:59 AM  Page 413LBR(for Lazy Bayesian Rules) is a Bayesian classiﬁer that defers all process-ing to classiﬁcation time.For each test instance it selects a set ofattributes forwhich the independence assumption should not be made;the others are treatedas independent ofeach other given the class and the selected set ofattributes.It works well for small test sets (Zheng and Webb 2000).LWLis a general algorithm for locally weighted learning.It assigns weightsusing an instance-based method and builds a classiﬁer from the weightedinstances.The classiﬁer is selected in LWL’s object editor:a good choice is NaïveBayes for classiﬁcation problems and linear regression for regression problems(Section 6.5,pages 251–253).You can set the number ofneighbors used,whichdetermines the kernel bandwidth,and the kernel shape to use for weighting—linear,inverse,or Gaussian.Attribute normalization is turned on by default.Miscellaneous classiﬁersThe misc.category includes two simple classiﬁers that were mentioned at theend ofSection 4.7 (page 136).Hyperpipes,for discrete classiﬁcation problems,records the range ofvalues observed in the training data for each attribute andcategory and works out which ranges contain the attribute values ofa test in-stance,choosing the category with the largest number ofcorrect ranges.VFI(voting feature intervals)constructs intervals around each class by discretizingnumeric attributes and using point intervals for nominal ones,records classcounts for each interval on each attribute,and classiﬁes test instances by voting(Demiroz and Guvenir 1997).A simple attribute weighting scheme assignshigher weight to more conﬁdent intervals,where conﬁdence is a function ofentropy.VFIis faster than Naïve Bayes but slower than hyperpipes.Neithermethod can handle missing values.10.5Metalearning algorithmsMetalearning algorithms,listed in Table 10.6,take classiﬁers and turn them intomore powerful learners.One parameter speciﬁes the base classiﬁer;othersspecify the number ofiterations for schemes such as bagging and boosting andan initial seed for the random number generator.We already met FilteredClas-siﬁerin Section 10.3:it runs a classiﬁer on data that has been passed through aﬁlter,which is a parameter.The ﬁlter’s own parameters are based exclusively onthe training data,which is the appropriate way to apply a supervised ﬁlter totest data.Bagging and randomizationBaggingbags a classiﬁer to reduce variance (Section 7.5,page 316).This imple-mentation works for both classiﬁcation and regression,depending on the base414CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 414learner.In the case ofclassiﬁcation,predictions are generated by averaging prob-ability estimates,not by voting.One parameter is the size ofthe bags as a per-centage ofthe training set.Another is whether to calculate the out-of-bag error,which gives the average error ofthe ensemble members (Breiman 2001).RandomCommitteeis even simpler:it builds an ensemble ofbase classiﬁersand averages their predictions.Each one is based on the same data but uses adifferent random number seed (Section 7.5,page 320).This only makes senseifthe base classiﬁer is randomized;otherwise,all classiﬁers would be the same.10.5METALEARNING ALGORITHMS415Table 10.6Metalearning algorithms in Weka.NameFunctionMetaAdaBoostM1Boost using the AdaBoostM1 methodAdditiveRegressionEnhance the performance of a regression method byiteratively ﬁtting the residualsAttributeSelectedClassiﬁerReduce dimensionality of data by attribute selectionBaggingBag a classiﬁer; works for regression tooClassiﬁcationViaRegressionPerform classiﬁcation using a regression methodCostSensitiveClassiﬁerMake its base classiﬁer cost sensitiveCVParameterSelectionPerform parameter selection by cross-validationDecorateBuild ensembles of classiﬁers by using speciallyconstructed artiﬁcial training examplesFilteredClassiﬁerRun a classiﬁer on ﬁltered dataGradingMetalearners whose inputs are base-level predictions thathave been marked as correct or incorrectLogitBoostPerform additive logistic regressionMetaCostMake a classiﬁer cost-sensitiveMultiBoostABCombine boosting and bagging using the MultiBoosting methodMultiClassClassiﬁerUse a two-class classiﬁer for multiclass datasetsMultiSchemeUse cross-validation to select a classiﬁer from severalcandidatesOrdinalClassClassiﬁerApply standard classiﬁcation algorithms to problems with an ordinal class valueRacedIncrementalLogitBoostBatch-based incremental learning by racing logit-boosted committeesRandomCommitteeBuild an ensemble of randomizable base classiﬁersRegressionByDiscretizationDiscretize the class attribute and employ a classiﬁerStackingCombine several classiﬁers using the stacking methodStackingCMore efﬁcient version of stackingThresholdSelectorOptimize the F-measure for a probabilistic classiﬁerVoteCombine classiﬁers using average of probability estimates or numeric predictionsP088407-Ch010.qxd  4/30/05  10:59 AM  Page 415BoostingAdaBoostM1implements the algorithm described in Section 7.5 (page 321;Figure 7.7).It can be accelerated by specifying a threshold for weight pruning.AdaBoostM1resamples ifthe base classiﬁer cannot handle weighted instances(you can also force resampling anyway).MultiBoostABcombines boosting witha variant ofbagging to prevent overﬁtting (Webb 2000).Whereas boosting only applies to nominal classes,AdditiveRegressionen-hances the performance ofa regression learner (Section 7.5,page 325).There aretwo parameters:shrinkage,which governs the learning rate,and the maximumnumber ofmodels to generate.Ifthe latter is inﬁnite,work continues until theerror stops decreasing.Decoratebuilds ensembles ofdiverse classiﬁers by using specially constructedartiﬁcial training examples.This technique is claimed to consistently improveon the base classiﬁer and on the bagging and random forest metalearners(Melville and Mooney,2005).6It outperforms boosting on small training setsand rivals it on larger ones.One parameter is the number ofartiﬁcial examplesto use as a proportion ofthe training data.Another is the desired number ofclassiﬁers in the ensemble,although execution may terminate prematurelybecause the number ofiterations can also be capped.Larger ensembles usuallyproduce more accurate models but have greater training time and model com-plexity.LogitBoostperforms additive logistic regression (Section 7.5,page 327).LikeAdaBoostM1,it can be accelerated by specifying a threshold for weight pruning.The appropriate number ofiterations can be determined using internal cross-validation;there is a shrinkage parameter that can be tuned to preventoverﬁtting;and you can choose resampling instead ofreweighting.RacedIncre-mentalLogitBoostlearns by racing LogitBoosted committees,and operates incre-mentally by processing the data in batches (pages 347–348),making it useful forlarge datasets (Frank et al.2002).Each committee member is learned from adifferent batch.The batch size starts at a given minimum and repeatedly doublesuntil it reaches a preset maximum.Resampling is used ifthe base classiﬁercannot handle weighted instances (you can also force resampling anyway).Log-likelihood pruning can be used within each committee:this discards new committee members ifthey decrease the log-likelihood based on the validationdata.You can determine how many instances to hold out for validation.The val-idation data is also used to determine which committee to retain when trainingterminates.416CHAPTER 10|THE EXPLORER6The random forest scheme was mentioned on page 407.It is really a metalearner,but Wekaincludes it among the decision tree methods because it is hardwired to a particular classi-ﬁer,RandomTree.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 416Combining classiﬁersVoteprovides a baseline method for combining classiﬁers by averaging theirprobability estimates (classiﬁcation) or numeric predictions (regression).MultiSchemeselects the best classiﬁer from a set ofcandidates using cross-validation ofpercentage accuracy (classiﬁcation) or mean-squared error(regression).The number offolds is a parameter.Performance on training datacan be used instead.Stackingcombines classiﬁers using stacking (Section 7.5,page 332) for bothclassiﬁcation and regression problems.You specify the base classiﬁers,the meta-learner,and the number ofcross-validation folds.StackingCimplements a moreefﬁcient variant for which the metalearner must be a numeric prediction scheme(Seewald 2002).In Grading,the inputs to the metalearner are base-level pre-dictions that have been marked (i.e.,“graded”) as correct or incorrect.For eachbase classiﬁer,a metalearner is learned that predicts when the base classiﬁer willerr.Just as stacking may be viewed as a generalization ofvoting,grading gener-alizes selection by cross-validation (Seewald and Fürnkranz 2001).Cost-sensitive learningThere are two metalearners for cost-sensitive learning (Section 5.7).The costmatrix can be supplied as a parameter or loaded from a ﬁle in the directory setby the onDemandDirectoryproperty,named by the relation name and with theextension cost.CostSensitiveClassiﬁereither reweights training instances accord-ing to the total cost assigned to each class (cost-sensitive learning,page 165) orpredicts the class with the least expected misclassiﬁcation cost rather than themost likely one (cost-sensitive classiﬁcation,page 164).MetaCostgenerates asingle cost-sensitive classiﬁer from the base learner (Section 7.5,pages 319–320).This implementation uses all bagging iterations when reclassifying training data(Domingos 1999 reports a marginal improvement when using only those iter-ations containing each training instance to reclassify it).You can specify eachbag’s size and the number ofbagging iterations.Optimizing performanceThree metalearners use the wrapper technique to optimize the base classiﬁer’sperformance.AttributeSelectedClassiﬁerselects attributes,reducing the data’sdimensionality before passing it to the classiﬁer (Section 7.1,page 290).You canchoose the attribute evaluator and search method using the Select attributespanel described in Section 10.2.CVParameterSelectionoptimizes performanceby using cross-validation to select parameters.For each parameter you give astring containing its lower and upper bounds and the desired number ofincre-ments.For example,to vary parameter -Pfrom 1 to 10 in increments of1,useP1 10 11.The number ofcross-validation folds can be speciﬁed.10.5METALEARNING ALGORITHMS417P088407-Ch010.qxd  4/30/05  10:59 AM  Page 417The third metalearner,ThresholdSelector,optimizes the F-measure (Section5.7) by selecting a probability threshold on the classiﬁer’s output.Performancecan be measured on the training data,on a holdout set,or by cross-validation.The probabilities returned by the base learner can be rescaled into the full range[0,1],which is useful ifthe scheme’s probabilities are restricted to a narrow sub-range.The metalearner can be applied to multiclass problems by specifying theclass value for which the optimization is performed as1.The ﬁrst class value2.The second class value3.Whichever value is least frequent4.Whichever value is most frequent5.The ﬁrst class named yes,pos(itive),or 1.Retargeting classiﬁers for different tasksFour metalearners adapt learners designed for one kind oftask to another.Clas-siﬁcationViaRegressionperforms classiﬁcation using a regression method bybinarizing the class and building a regression model for each value.Regression-ByDiscretizationis a regression scheme that discretizes the class attribute into aspeciﬁed number ofbins using equal-width discretization and then employs aclassiﬁer.The predictions are the weighted average ofthe mean class value foreach discretized interval,with weights based on the predicted probabilities forthe intervals.OrdinalClassClassiﬁerapplies standard classiﬁcation algorithms toordinal-class problems (Frank and Hall 2001).MultiClassClassiﬁerhandlesmulticlass problems with two-class classiﬁers using any ofthese methods:1.One versus all the rest2.Pairwise classiﬁcation using voting to predict3.Exhaustive error-correcting codes (Section 7.5,page 334)4.Randomly selected error-correcting codesRandom code vectors are known to have good error-correcting properties:aparameter speciﬁes the length ofthe code vector (in bits).10.6Clustering algorithmsTable 10.7 lists Weka’s clustering algorithms;the ﬁrst two and SimpleKMeansaredescribed in Section 6.6.For the EMimplementation you can specify how manyclusters to generate or the algorithm can decide using cross-validation—inwhich case the number offolds is ﬁxed at 10 (unless there are fewer than 10training instances).You can specify the maximum number ofiterations and setthe minimum allowable standard deviation for the normal density calculation.418CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 418SimpleKMeansclusters data using k-means;the number ofclusters is speciﬁedby a parameter.Cobwebimplements both the Cobweb algorithm for nominalattributes and the Classit algorithm for numeric attributes.The ordering andpriority ofthe merging and splitting operators differs between the originalCobweb and Classit papers (where it is somewhat ambiguous).This imple-mentation always compares four different ways oftreating a new instance andchooses the best:adding it to the best host,making it into a new leaf,mergingthe two best hosts and adding it to the merged node,and splitting the best hostand adding it to one ofthe splits.Acuityand cutoffare parameters.FarthestFirstimplements the farthest-ﬁrst traversal algorithm ofHochbaumand Shmoys (1985),cited by Sanjoy Dasgupta (2002);a fast,simple,approxi-mate clusterer modeled on k-means.MakeDensityBasedClustereris a meta-clusterer that wraps a clustering algorithm to make it return a probabilitydistribution and density.To each cluster it ﬁts a discrete distribution or a symmetric normal distribution (whose minimum standard deviation is aparameter).10.7Association-rule learnersWeka has three association-rule learners,listed in Table 10.8.Aprioriimple-ments the Apriori algorithm (Section 4.5).It starts with a minimum support of100% ofthe data items and decreases this in steps of5% until there are at least10 rules with the required minimum conﬁdence of0.9 or until the support has10.7ASSOCIATION-RULE LEARNERS419Table 10.7Clustering algorithms.NameFunctionEMCluster using expectation maximizationCobwebImplements the Cobweb and Classit clustering algorithmsFarthestFirstCluster using the farthest ﬁrst traversal algorithmMakeDensityBasedClustererWrap a clusterer to make it return distribution and densitySimpleKMeansCluster using the k-means methodTable 10.8Association-rule learners.NameFunctionAprioriFind association rules using the Apriori algorithmPredictiveAprioriFind association rules sorted by predictive accuracyTertiusConﬁrmation-guided discovery of association or classiﬁcation rulesP088407-Ch010.qxd  4/30/05  10:59 AM  Page 419reached a lower bound of10%,whichever occurs ﬁrst.(These default values canbe changed.) There are four alternative metrics for ranking rules:Conﬁdence,which is the proportion ofthe examples covered by the premise that are alsocovered by the consequent (called accuracyin Section 4.5);Lift,which is deter-mined by dividing the conﬁdence by the support (called coveragein Section 4.5);Leverage,which is the proportion ofadditional examples covered by both thepremise and the consequent beyond those expected ifthe premise and conse-quent were statistically independent;and Conviction,a measure deﬁned by Brinet al.(1997).You can also specify a signiﬁcance level,and rules will be tested forsigniﬁcance at this level.PredictiveAprioricombines conﬁdence and support into a single measure ofpredictive accuracy(Scheffer 2001) and ﬁnds the best nassociation rules in order.Internally,the algorithm successively increases the support threshold,becausethe value ofpredictive accuracy depends on it.Tertiusﬁnds rules according toa conﬁrmation measure (Flach and Lachiche 1999),seeking rules with multipleconditions in the consequent,like Apriori,but differing in that these conditionsare OR’d together,not ANDed.It can be set to ﬁnd rules that predict a singlecondition or a predetermined attribute (i.e.,classiﬁcation rules).One parame-ter determines whether negation is allowed in the antecedent,the consequent,or both;others give the number ofrules sought,minimum degree ofconﬁr-mation,minimum coverage,maximum proportion ofcounterinstances,andmaximum rule size.Missing values can match any value,never match,or be sig-niﬁcant and possibly appear in rules.10.8Attribute selectionFigure 10.21 shows that part ofWeka’s attribute selection panel where youspecify the attribute evaluator and search method;Table 10.9 and Table 10.10list the choices.Attribute selection is normally done by searching the space ofattribute subsets,evaluating each one (Section 7.1).This is achieved by com-bining one ofthe four attribute subset evaluators in Table 10.9 with one oftheseven search methods in Table 10.10.A potentially faster but less accurateapproach is to evaluate the attributes individually and sort them,discarding420CHAPTER 10|THE EXPLORERFigure 10.21Attribute selection:specifying an evaluator and a search method.P088407-Ch010.qxd  4/30/05  10:59 AM  Page 420attributes that fall below a chosen cutoffpoint.This is achieved by selecting oneofthe eight single-attribute evaluators in Table 10.9 and using the rankingmethod in Table 10.10.The Weka interface allows both possibilities by lettingthe user choose a selection method from Table 10.9 and a search method fromTable 10.10,producing an error message ifyou select an inappropriate combi-10.8ATTRIBUTE SELECTION421Table 10.9Attribute evaluation methods for attribute selection.NameFunctionAttributeCfsSubsetEvalConsider the predictive value of eachsubset evaluatorattribute individually, along with the degree of redundancy among themClassiﬁerSubsetEvalUse a classiﬁer to evaluate attribute setConsistencySubsetEvalProject training set onto attribute set andmeasure consistency in class valuesWrapperSubsetEvalUse a classiﬁer plus cross-validationSingle-ChiSquaredAttributeEvalCompute the chi-squared statistic of eachattribute evaluatorattribute with respect to the classGainRatioAttributeEvalEvaluate attribute based on gain ratioInfoGainAttributeEvalEvaluate attribute based on information gainOneRAttributeEvalUse OneR’s methodology to evaluate attributesPrincipalComponentsPerform principal components analysis andtransformationReliefFAttributeEvalInstance-based attribute evaluatorSVMAttributeEvalUse a linear support vector machine todetermine the value of attributesSymmetricalUncertAttributeEvalEvaluate attribute based on symmetricuncertaintyTable 10.10Search methods for attribute selection.NameFunctionSearchBestFirstGreedy hill-climbing with backtrackingmethodExhaustiveSearchSearch exhaustivelyGeneticSearchSearch using a simple genetic algorithmGreedyStepwiseGreedy hill-climbing without backtracking; optionallygenerate ranked list of attributesRaceSearchUse race search methodologyRandomSearchSearch randomlyRankSearchSort the attributes and rank promising subsets using anattribute subset evaluatorRanking methodRankerRank individual attributes (not subsets) according to their evaluationP088407-Ch010.qxd  4/30/05  10:59 AM  Page 421nation.The status line refers you to the error log for the message (see the endofSection 10.1).Attribute subset evaluatorsSubset evaluators take a subset ofattributes and return a numeric measure thatguides the search.They are conﬁgured like any other Weka object.CfsSubsetEvalassesses the predictive ability ofeach attribute individually and the degree ofredundancy among them,preferring sets ofattributes that are highly correlatedwith the class but have low intercorrelation (Section 7.1).An option iterativelyadds attributes that have the highest correlation with the class,provided thatthe set does not already contain an attribute whose correlation with the attrib-ute in question is even higher.Missingcan be treated as a separate value,or itscounts can be distributed among other values in proportion to their frequency.ConsistencySubsetEvalevaluates attribute sets by the degree ofconsistency inclass values when the training instances are projected onto the set.The consis-tency ofany subset ofattributes can never improve on that ofthe full set,sothis evaluator is usually used in conjunction with a random or exhaustive searchthat seeks the smallest subset whose consistency is the same as that ofthe fullattribute set.Whereas the previously mentioned subset evaluators are ﬁlter methods ofattribute selection (Section 7.1),the remainder are wrapper methods.Classi-ﬁerSubsetEvaluses a classiﬁer,speciﬁed in the object editor as a parameter,toevaluate sets ofattributes on the training data or on a separate holdout set.WrapperSubsetEvalalso uses a classiﬁer to evaluate attribute sets,but it employscross-validation to estimate the accuracy ofthe learning scheme for each set.Single-attribute evaluatorsSingle-attribute evaluators are used with the Rankersearch method to generatea ranked list from which Rankerdiscards a given number (explained in the nextsubsection).They can also be used in the RankSearchmethod.ReliefFAttribute-Evalis instance-based:it samples instances randomly and checks neighboringinstances ofthe same and different classes (Section 7.1).It operates on discreteand continuous class data.Parameters specify the number ofinstances tosample,the number ofneighbors to check,whether to weight neighbors by dis-tance,and an exponential function that governs how rapidly weights decay withdistance.InfoGainAttributeEvalevaluates attributes by measuring their informationgain with respect to the class.It discretizes numeric attributes ﬁrst using theMDL-based discretization method (it can be set to binarize them instead).Thismethod,along with the next three,can treat missingas a separate value or dis-422CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 422tribute the counts among other values in proportion to their frequency.ChiSquaredAttributeEvalevaluates attributes by computing the chi-squared sta-tistic with respect to the class.GainRatioAttributeEvalevaluates attributes bymeasuring their gain ratio with respect to the class.SymmetricalUncertAttribu-teEvalevaluates an attribute Aby measuring its symmetric uncertainty withrespect to the class C(Section 7.1,page 291).OneRAttributeEvaluses the simple accuracy measure adopted by the OneRclassiﬁer.It can use the training data for evaluation,as OneRdoes,or it can applyinternal cross-validation:the number offolds is a parameter.It adopts OneR’ssimple discretization method:the minimum bucket size is a parameter.SVMAttributeEvalevaluates attributes using recursive feature eliminationwith a linear support vector machine (Section 7.1,page 291).Attributes areselected one by one based on the size oftheir coefﬁcients,relearning after eachone.To speed things up a ﬁxed number (or proportion) ofattributes can beremoved at each stage.Indeed,a proportion can be used until a certain num-ber ofattributes remain,thereupon switching to the ﬁxed-number method—rapidly eliminating many attributes and then considering each one more intensively.Various parameters are passed to the support vector machine:com-plexity,epsilon,tolerance,and the ﬁltering method used.Unlike other single-attribute evaluators,PrincipalComponentstransforms the set ofattributes.The new attributes are ranked in order oftheir eigen-values (Section 7.3,page 306);optionally,a subset is selected by choosing suf-ﬁcient eigenvectors to account for a given proportion ofthe variance (95% bydefault).You can also use it to transform the reduced data back to the originalspace.Search methodsSearch methods traverse the attribute space to ﬁnd a good subset.Quality ismeasured by the chosen attribute subset evaluator.Each search method can be conﬁgured with Weka’s object editor.BestFirstperforms greedy hill climb-ing with backtracking;you can specify how many consecutive nonimprov-ing nodes must be encountered before the system backtracks.It can searchforward from the empty set ofattributes,backward from the full set,or start atan intermediate point (speciﬁed by a list ofattribute indices) and search in bothdirections by considering all possible single-attribute additions and deletions.Subsets that have been evaluated are cached for efﬁciency;the cache size is aparameter.GreedyStepwisesearches greedily through the space ofattribute subsets.LikeBestFirst,it may progress forward from the empty set or backward from the fullset.Unlike BestFirst,it does not backtrack but terminates as soon as adding or10.8ATTRIBUTE SELECTION423P088407-Ch010.qxd  4/30/05  10:59 AM  Page 423deleting the best remaining attribute decreases the evaluation metric.In analternative mode,it ranks attributes by traversing the space from empty to full(or vice versa) and recording the order in which attributes are selected.You canspecify the number ofattributes to retain or set a threshold below which attrib-utes are discarded.RaceSearch,used with ClassiﬁerSubsetEval,calculates the cross-validationerror ofcompeting attribute subsets using race search (Section 7.1).The fourdifferent searches described on page 295 are implemented:forward selection,backward elimination,schemata search,and rank racing.In the last case a sep-arate attribute evaluator (which can also be speciﬁed) is used to generate aninitial ranking.Using forward selection,it is also possible to generate a rankedlist ofattributes by continuing racing until all attributes have been selected:theranking is set to the order in which they are added.As with GreedyStepwise,youcan specify the number ofattributes to retain or set a threshold below whichattributes are discarded.GeneticSearchuses a simple genetic algorithm (Goldberg 1989).Parametersinclude population size,number ofgenerations,and probabilities ofcrossoverand mutation.You can specify a list ofattribute indices as the starting point,which becomes a member ofthe initial population.Progress reports can be gen-erated every so many generations.RandomSearchrandomly searches the spaceofattribute subsets.Ifan initial set is supplied,it searches for subsets thatimprove on (or equal) the starting point and have fewer (or the same numberof) attributes.Otherwise,it starts from a random point and reports the bestsubset found.Placing all attributes in the initial set yields Liu and Setiono’s(1996) probabilistic feature selection algorithm.You can determine the fractionofthe search space to explore.ExhaustiveSearchsearches through the space ofattribute subsets,starting from the empty set,and reports the best subset found.Ifan initial set is supplied,it searches backward from this starting point andreports the smallest subset with a better (or equal) evaluation.RankSearchsorts attributes using a single-attribute evaluator and then rankspromising subsets using an attribute subset evaluator.The latter is speciﬁed inthe top box ofFigure 10.21,as usual;the attribute evaluator is speciﬁed as aproperty in RankSearch’s object editor.It starts by sorting the attributes withthe single-attribute evaluator and then evaluates subsets ofincreasing size usingthe subset evaluator—the best attribute,the best attribute plus the next best one,and so on—reporting the best subset.This procedure has low computationalcomplexity:the number oftimes both evaluators are called is linear in thenumber ofattributes.Using a simple single-attribute evaluator (e.g.,GainRa-tioAttributeEval),the selection procedure is very fast.Finally we describe Ranker,which as noted earlier is not a search method for attribute subsets but a ranking scheme for individual attributes.It sortsattributes by their individual evaluations and must be used in conjunction 424CHAPTER 10|THE EXPLORERP088407-Ch010.qxd  4/30/05  10:59 AM  Page 424with one ofthe single-attribute evaluators in the lower part ofTable 10.9—notan attribute subset evaluator.Rankernot only ranks attributes but also performsattribute selection by removing the lower-ranking ones.You can set a cutoffthreshold below which attributes are discarded,or specify how many attributesto retain.You can specify certain attributes that must be retained regardless oftheir rank.10.8ATTRIBUTE SELECTION425P088407-Ch010.qxd  4/30/05  10:59 AM  Page 425P088407-Ch010.qxd  4/30/05  10:59 AM  Page 426With the Knowledge Flow interface,users select Weka components from a toolbar,place them on a layout canvas,and connect them into a directed graph thatprocesses and analyzes data.It provides an alternative to the Explorer for thosewho like thinking in terms ofhow data ﬂows through the system.It also allowsthe design and execution ofconﬁgurations for streamed data processing,whichthe Explorer cannot do.You invoke the Knowledge Flow interface by selectingKnowledgeFlowfrom the choices at the bottom ofthe panel shown in Figure10.3(a).11.1Getting startedHere is a step-by-step example that loads an ARFF ﬁle and performs a cross-validation using J4.8.We describe how to build up the ﬁnal conﬁguration shownin Figure 11.1.First create a source ofdata by clicking on the DataSourcestab(rightmost entry in the bar at the top) and selecting ARFFLoaderfrom thechapter11The Knowledge Flow Interface427P088407-Ch011.qxd  4/30/05  11:00 AM  Page 427toolbar.The mouse cursor changes to crosshairs to signal that you should nextplace the component.Do this by clicking anywhere on the canvas,whereupona copy ofthe ARFF loader icon appears there.To connect it to an ARFF ﬁle,right-click it to bring up the pop-up menu shown in Figure 11.2(a).Click Con-ﬁgureto get the ﬁle browser in Figure 11.2(b),from which you select the desiredARFF ﬁle.The File Formatpull-down menu allows you to choose a differenttype ofdata source—for example,spreadsheet ﬁles.Now we specify which attribute is the class using a ClassAssignerobject.Thisis on the Evaluationpanel,so click the Evaluationtab,select the ClassAssigner,and place it on the canvas.To connect the data source to the class assigner,right-click the data source icon and select datasetfrom the menu,as shown in Figure11.2(a).A rubber-band line appears.Move the mouse over the class assignercomponent and left-click.A red line labeled datasetappears,joining the twocomponents.Having connected the class assigner,choose the class by right-clicking it,selecting Conﬁgure,and entering the location ofthe class attribute.We will perform cross-validation on the J48classiﬁer.In the data ﬂow model,we ﬁrst connect the CrossValidationFoldMakerto create the folds on which theclassiﬁer will run,and then pass its output to an object representing J48.Cross-ValidationFoldMakeris on the Evaluationpanel.Select it,place it on the canvas,and connect it to the class assigner by right-clicking the latter and selecting428CHAPTER 11|THE KNOWLEDGE FLOW INTERFACEFigure 11.1The Knowledge Flow interface.P088407-Ch011.qxd  4/30/05  11:00 AM  Page 428datasetfrom the menu (which is similar to that in Figure 11.2(a)).Next selectJ48from the Classiﬁerspanel and place a J48component on the canvas.Thereare so many different classiﬁers that you have to scroll along the toolbar to ﬁndit.Connect J48to the cross-validation fold maker in the usual way,but makethe connection twiceby ﬁrst choosing trainingSetand then choosing testSetfromthe pop-up menu for the cross-validation fold maker.The next step is to selecta ClassiﬁerPerformanceEvaluatorfrom the Evaluationpanel and connect J48toit by selecting the batchClassiﬁerentry from the pop-up menu for J48.Finally,from the Visualizationtoolbar we place a TextViewercomponent on the canvas.Connect the classiﬁer performance evaluator to it by selecting the textentryfrom the pop-up menu for the performance evaluator.At this stage the conﬁguration is as shown in Figure 11.1 except that there isas yet no graph viewer.Start the ﬂow ofexecution by selecting Start loadingfromthe pop-up menu for the ARFF loader,shown in Figure 11.2(a).For a smalldataset things happen quickly,but ifthe input were large you would see thatsome ofthe icons are animated—for example,J48’s tree would appear to growand the performance evaluator’s checkmarks would blink.Progress informationappears in the status bar at the bottom ofthe interface.Choosing Show resultsfrom the text viewer’s pop-up menu brings the results ofcross-validation up ina separate window,in the same form as for the Explorer.To complete the example,add a GraphViewerand connect it to J48’s graphoutput to see a graphical representation ofthe trees produced for each fold ofthe cross-validation.Once you have redone the cross-validation with this extracomponent in place,selecting Show resultsfrom its pop-up menu produces a11.1GETTING STARTED429(b)(a)Figure 11.2Conﬁguring a data source:(a) the right-click menu and (b) the ﬁle browserobtained from the Conﬁguremenu item.P088407-Ch011.qxd  4/30/05  11:00 AM  Page 429list oftrees,one for each cross-validation fold.By creating cross-validation foldsand passing them to the classiﬁer,the Knowledge Flow model provides a way tohook into the results for each fold.The Explorer cannot do this:it treats cross-validation as an evaluation method that is applied to the output ofa classiﬁer.11.2The Knowledge Flow componentsMost ofthe Knowledge Flow components will be familiar from the Explorer.The Classiﬁerspanel contains all ofWeka’s classiﬁers,the Filterspanel containsthe ﬁlters,and the Clustererspanel holds the clusterers.Possible data sources areARFF ﬁles,CSV ﬁles exported from spreadsheets,the C4.5 ﬁle format,and aserialized instance loader for data ﬁles that have been saved as an instance ofaJava object.There are data sinks and sources for the ﬁle formats supported bythe Explorer.There is also a data sink and a data source that can connect to adatabase.The components for visualization and evaluation,listed in Table 11.1,havenot yet been encountered.Under Visualization,the DataVisualizerpops up apanel for visualizing data in a two-dimensional scatter plot as in Figure 10.6(b),in which you can select the attributes you would like to see.ScatterPlotMatrixpops up a matrix oftwo-dimensional scatter plots for every pair ofattributes,430CHAPTER 11|THE KNOWLEDGE FLOW INTERFACETable 11.1Visualization and evaluation components.NameFunctionVisualizationDataVisualizerVisualize data in a 2D scatter plotScatterPlotMatrixMatrix of scatter plotsAttributeSummarizerSet of histograms, one for each attributeModelPerformanceChartDraw ROC and other threshold curvesTextViewerVisualize data or models as textGraphViewerVisualize tree-based modelsStripChartDisplay a scrolling plot of dataEvaluationTrainingSetMakerMake a dataset into a training setTestSetMakerMake a dataset into a test setCrossValidationFoldMakerSplit a dataset into foldsTrainTestSplitMakerSplit a dataset into training and test setsClassAssignerAssign one of the attributes to be the classClassValuePickerChoose a value for the positiveclassClassiﬁerPerformanceEvaluatorCollect evaluation statistics for batch evaluationIncrementalClassiﬁerEvaluatorCollect evaluation statistics for incrementalevaluationClustererPerformanceEvaluatorCollect evaluation statistics for clusterersPredictionAppenderAppend a classiﬁer’s predictions to a datasetP088407-Ch011.qxd  4/30/05  11:00 AM  Page 430shown in Figure 10.16(a).AttributeSummarizergives a matrix ofhistograms,one for each attribute,like that in the lower right-hand corner ofFigure 10.3(b).ModelPerformanceChartdraws ROC curves and other threshold curves.GraphViewerpops up a panel for visualizing tree-based models,as in Figure10.6(a).As before,you can zoom,pan,and visualize the instance data at a node(ifit has been saved by the learning algorithm).StripChartis a new visualization component designed for use with in-cremental learning.In conjunction with the IncrementalClassiﬁerEvaluatordescribed in the next paragraph it displays a learning curve that plots accu-racy—both the percentage accuracy and the root mean-squared probabilityerror—against time.It shows a ﬁxed-size time window that scrolls horizontallyto reveal the latest results.The Evaluationpanel has the components listed in the lower part ofTable11.1.The TrainingSetMakerand TestSetMakermake a dataset into the corre-sponding kind ofset.The CrossValidationFoldMakerconstructs cross-validationfolds from a dataset;the TrainTestSplitMakersplits it into training and test setsby holding part ofthe data out for the test set.The ClassAssignerallows you todecide which attribute is the class.With ClassValuePickeryou choose a valuethat is treated as the positiveclass when generating ROC and other thresholdcurves.The ClassiﬁerPerformanceEvaluatorcollects evaluation statistics:it cansend the textual evaluation to a text viewer and the threshold curves to a per-formance chart.The IncrementalClassiﬁerEvaluatorperforms the same functionfor incremental classiﬁers:it computes running squared errors and so on.Thereis also a ClustererPerformanceEvaluator,which is similar to the ClassiﬁerPerfor-manceEvaluator.The PredictionAppendertakes a classiﬁer and a dataset andappends the classiﬁer’s predictions to the dataset.11.3Conﬁguring and connecting the componentsYou establish the knowledge ﬂow by conﬁguring the individual components andconnecting them up.Figure 11.3 shows typical operations that are available byright-clicking the various component types.These menus have up to three sec-tions:Edit,Connections,and Actions.The Editoperations delete componentsand open up their conﬁguration panel.Classiﬁers and ﬁlters are conﬁgured justas in the Explorer.Data sources are conﬁgured by opening a ﬁle (as we saw pre-viously),and evaluation components are conﬁgured by setting parameters suchas the number offolds for cross-validation.The Actionsoperations are speciﬁcto that type ofcomponent,such as starting to load data from a data source oropening a window to show the results ofvisualization.The Connectionsopera-tions are used to connect components together by selecting the type ofcon-nection from the source component and then clicking on the target object.Not11.3CONFIGURING AND CONNECTING THE COMPONENTS431P088407-Ch011.qxd  4/30/05  11:00 AM  Page 431all targets are suitable;applicable ones are highlighted.Items on the connectionsmenu are disabled (grayed out) until the component receives other connectionsthat render them applicable.There are two kinds ofconnection from data sources:datasetconnectionsand instanceconnections.The former are for batch operations such as classi-ﬁers like J48;the latter are for stream operations such as NaiveBayesUpdateable.A data source component cannot provide both types ofconnection:once oneis selected,the other is disabled.When a datasetconnection is made to a batchclassiﬁer,the classiﬁer needs to know whether it is intended to serve as a train-ing set or a test set.To do this,you ﬁrst make the data source into a test or train-ing set using the TestSetMakeror TrainingSetMakercomponents from theEvaluationpanel.On the other hand,an instanceconnection to an incrementalclassiﬁer is made directly:there is no distinction between training and testingbecause the instances that ﬂow update the classiﬁer incrementally.In this casea prediction is made for each incoming instance and incorporated into the testresults;then the classiﬁer is trained on that instance.Ifyou make an instanceconnection to a batch classiﬁer it will be used as a test instance because train-ing cannot possibly be incremental whereas testing always can be.Conversely,it is quite possible to test an incremental classiﬁer in batch mode using a datasetconnection.Connections from a ﬁlter component are enabled when it receives input froma data source,whereupon follow-on datasetor instanceconnections can bemade.Instanceconnections cannot be made to supervised ﬁlters or to unsu-432CHAPTER 11|THE KNOWLEDGE FLOW INTERFACEdata sourcedata sinkfilterclassifiervisualizationevaluationcrossValidationFoldMakerClassifierPerformance-EvaluatorFigure 11.3Operations on the Knowledge Flow components.P088407-Ch011.qxd  4/30/05  11:00 AM  Page 432pervised ﬁlters that cannot handle data incrementally (such as Discretize).Toget a test or training set out ofa ﬁlter,you need to put the appropriate kind in.The classiﬁer menu has two types ofconnection.The ﬁrst type,namely,graphand textconnections,provides graphical and textual representations ofthe clas-siﬁer’s learned state and is only activated when it receives a training set input.The other type,namely,batchClassiﬁerand incrementalClassiﬁerconnections,makes data available to a performance evaluator and is only activated when atest set input is present,too.Which one is activated depends on the type oftheclassiﬁer.Evaluation components are a mixed bag.TrainingSetMakerand TestSetMakerturn a dataset into a training or test set.CrossValidationFoldMakerturns adataset into botha training set and a test set.ClassiﬁerPerformanceEvaluator(used in the example ofSection 11.1) generates textual and graphical output forvisualization components.Other evaluation components operate like ﬁlters:they enable follow-on dataset,instance,training set,or test setconnectionsdepending on the input (e.g.,ClassAssignerassigns a class to a dataset).Visualization components do not have connections,although some haveactions such as Show resultsand Clear results.11.4Incremental learningIn most respects the Knowledge Flow interface is functionally similar to theExplorer:you can do similar things with both.It does provide some additionalﬂexibility—for example,you can see the tree that J48makes for each cross-validation fold.But its real strength is the potential for incremental operation.Weka has several classiﬁers that can handle data incrementally:AODE,aversion ofNaïve Bayes (NaiveBayesUpdateable),Winnow,and instance-basedlearners (IB1,IBk,KStar,LWL).The metalearner RacedIncrementalLogitBoostoperates incrementally (page 416).All ﬁlters that work instance by instance areincremental:Add,AddExpression,Copy,FirstOrder,MakeIndicator,Merge-TwoValues,NonSparseToSparse,NumericToBinary,NumericTransform,Obfuscate,Remove,RemoveType,RemoveWithValues,SparseToNonSparse,and SwapValues.Ifall components connected up in the Knowledge Flow interface operateincrementally,so does the resulting learning system.It does not read in thedataset before learning starts,as the Explorer does.Instead,the data source com-ponent reads the input instance by instance and passes it through the Knowl-edge Flow chain.Figure 11.4(a) shows a conﬁguration that works incrementally.An instanceconnection is made from the loader to the updatable Naïve Bayes classiﬁer.The classiﬁer’s text output is taken to a viewer that gives a textual description11.4INCREMENTAL LEARNING433P088407-Ch011.qxd  4/30/05  11:00 AM  Page 433434CHAPTER 11|THE KNOWLEDGE FLOW INTERFACE(a)(b)Figure 11.4A Knowledge Flow that operates incrementally:(a) the conﬁguration and (b) the stripchart output.ofthe model.Also,an incrementalClassiﬁerconnection is made to the corresponding performance evaluator.This produces an output oftype chart,which is piped to a strip chart visualization component to generate a scrollingdata plot.Figure 11.4(b) shows the strip chart output.It plots both the accuracy andthe root mean-squared probability error against time.As time passes,the wholeplot (including the axes) moves leftward to make room for new data at the right.P088407-Ch011.qxd  4/30/05  11:00 AM  Page 434When the vertical axis representing time 0 can move left no farther,it stops andthe time origin starts to increase from 0 to keep pace with the data coming inat the right.Thus when the chart is full it shows a window ofthe most recenttime units.The strip chart can be conﬁgured to alter the number ofinstancesshown on the x axis.This particular Knowledge Flow conﬁguration can process input ﬁles ofanysize,even ones that do not ﬁt into the computer’s main memory.However,it alldepends on how the classiﬁer operates internally.For example,although theyare incremental,many instance-based learners store the entire dataset internally.11.4INCREMENTAL LEARNING435P088407-Ch011.qxd  4/30/05  11:00 AM  Page 435P088407-Ch011.qxd  4/30/05  11:00 AM  Page 436The Explorer and Knowledge Flow environments help you determine how wellmachine learning schemes perform on given datasets.But serious investigativework involves substantial experiments—typically running several learningschemes on different datasets,often with various parameter settings—and theseinterfaces are not really suitable for this.The Experimenter enables you to setup large-scale experiments,start them running,leave them,and come backwhen they have ﬁnished and analyze the performance statistics that have beencollected.They automate the experimental process.The statistics can be storedin ARFF format,and can themselves be the subject offurther data mining.Youinvoke this interface by selecting Experimenterfrom the choices at the bottomofthe panel in Figure 10.3(a).Whereas the Knowledge Flow transcends limitations ofspace by allowingmachine learning runs that do not load in the whole dataset at once,the Exper-imenter transcends limitations oftime.It contains facilities for advanced Wekausers to distribute the computing load across multiple machines using Java RMI.You can set up big experiments and just leave them to run.chapter12The Experimenter437P088407-Ch012.qxd  4/30/05  11:02 AM  Page 43712.1Getting startedAs an example,we will compare the J4.8 decision tree method with the baselinemethods OneRand ZeroRon the Iris dataset.The Experimenter has three panels:Setup,Run,and Analyze.Figure 12.1(a) shows the ﬁrst:you select the othersfrom the tabs at the top.Here,the experiment has already been set up.To dothis,ﬁrst click New(toward the right at the top) to start a new experiment (theother two buttons in that row save an experiment and open a previously savedone).Then,on the line below,select the destination for the results—in this casethe ﬁle Experiment1—and choose CSV ﬁle.Underneath,select the datasets—wehave only one,the iris data.To the right ofthe datasets,select the algorithms tobe tested—we have three.Click Add newto get a standard Weka object editorfrom which you can choose and conﬁgure a classiﬁer.Repeat this operation toadd the three classiﬁers.Now the experiment is ready.The other settings shownin Figure 12.1(a) are all default values.Ifyou want to reconﬁgure a classiﬁer thatis already in the list,you can use the Edit selectedbutton.You can also save theoptions for a particular classiﬁer in XML format for later reuse.438CHAPTER 12|THE EXPERIMENTER(a)Figure 12.1An experiment:(a) setting it up,(b) the results ﬁle,and (c) a spreadsheetwith the results.P088407-Ch012.qxd  4/30/05  11:02 AM  Page 438Running an experimentTo run the experiment,click the Runtab,which brings up a panel that containsa Startbutton (and little else);click it.A briefreport is displayed when the oper-ation is ﬁnished.The ﬁle Experiment1.csvcontains the results.The ﬁrst two linesare shown in Figure 12.1(b):they are in CSV format and can be read directlyinto a spreadsheet,the ﬁrst part ofwhich appears in Figure 12.1(c).Each rowrepresents 1 fold ofa 10-fold cross-validation (see the Foldcolumn).The cross-validation is run 10 times (the Runcolumn) for each classiﬁer (the Schemecolumn).Thus the ﬁle contains 100 rows for each classiﬁer,which makes 300rows in all (plus the header row).Each row contains plenty ofinformation—46columns,in fact—including the options supplied to the machine learning12.1GETTING STARTED439Dataset,Run,Fold,Scheme,Scheme_options,Scheme_version_ID,Date_time,Number_of_training_instances,Number_of_testing_instances,Number_correct,Number_incorrect,Number_unclassified,Percent_correct,Percent_incorrect,Percent_unclassified,Kappa_statistic,Mean_absolute_error,Root_mean_squared_error,Relative_absolute_error,Root_relative_squared_error,SF_prior_entropy,SF_scheme_entropy,SF_entropy_gain,SF_mean_prior_entropy,SF_mean_scheme_entropy,SF_mean_entropy_gain,KB_information,KB_mean_information,KB_relative_information,True_positive_rate,Num_true_positives,False_positive_rate,Num_false_positives,True_negative_rate,Num_true_negatives,False_negative_rate,Num_false_negatives,IR_precision,IR_recall,F_measure,Time_training,Time_testing,Summary,measureTreeSize,measureNumLeaves,measureNumRulesiris,1,1,weka.classifiers.trees.J48,'-C 0.25 -M 2',-217733168393644444,2.00405230549E7,135.0,15.0,14.0,1.0,0.0,93.33333333333333,6.666666666666667,0.0,0.9,0.0450160137965016,0.1693176548766098,10.128603104212857,35.917698581356284,23.77443751081735,2.632715099281766,21.141722411535582,1.5849625007211567,0.17551433995211774,1.4094481607690388,21.615653599867994,1.4410435733245328,1363.79589990507,1.0,5.0,0.0,0.0,1.0,10.0,0.0,0.0,1.0,1.0,1.0,0.0070,0.0,'Number of leaves: 4\nSize of the tree: 7\n',7.0,4.0,4.0(b)(c)Figure 12.1(continued)P088407-Ch012.qxd  4/30/05  11:02 AM  Page 439scheme;the number oftraining and test instances;the number (and percent-age) ofcorrect,incorrect,and unclassiﬁed instances;the mean absolute error,root mean-squared error,and many more.There is a great deal ofinformation in the spreadsheet,but it is hard to digest.In particular,it is not easy to answer the question posed previously:how doesJ4.8 compare with the baseline methods OneRand ZeroRon this dataset? Forthat we need the Analyzepanel.Analyzing the resultsThe reason that we generated the output in CSV format was to show the spread-sheet in Figure 12.1(c).The Experimenter normally produces its output in ARFFformat.You can also leave the ﬁle name blank,in which case the Experimenterstores the results in a temporary ﬁle.The Analyzepanel is shown in Figure 12.2.To analyze the experiment that hasjust been performed,click the Experimentbutton at the right near the top;otherwise,supply a ﬁle that contains the results ofanother experiment.Thenclick Perform test(near the bottom on the left).The result ofa statistical signiﬁ-440CHAPTER 12|THE EXPERIMENTERFigure 12.2Statistical test results for the experiment in Figure 12.1.P088407-Ch012.qxd  4/30/05  11:02 AM  Page 440cance test ofthe performance ofthe ﬁrst learning scheme (J48)versus that oftheother two (OneRand ZeroR) will be displayed in the large panel on the right.We are comparing the percent correct statistic:this is selected by default asthe comparison ﬁeld shown toward the left ofFigure 12.2.The three methodsare displayed horizontally,numbered (1),(2),and (3),as the heading ofa littletable.The labels for the columns are repeated at the bottom—trees.J48,rules.OneR,and rules.ZeroR—in case there is insufﬁcient space for them in theheading.The inscrutable integers beside the scheme names identify whichversion ofthe scheme is being used.They are present by default to avoid con-fusion among results generated using different versions ofthe algorithms.Thevalue in parentheses at the beginning ofthe irisrow (100)is the number ofexperimental runs:10 times 10-fold cross-validation.The percentage correct for the three schemes is shown in Figure 12.2:94.73%for method 1,93.53% for method 2,and 33.33% for method 3.The symbolplaced beside a result indicates that it is statistically better (v)or worse (*)thanthe baseline scheme—in this case J4.8—at the speciﬁed signiﬁcance level (0.05,or 5%).The corrected resampled t-test from Section 5.5 (page 157) is used.Here,method 3 is signiﬁcantly worse than method 1,because its success rate is followedby an asterisk.At the bottom ofcolumns 2 and 3 are counts (x/y/z) ofthe numberoftimes the scheme was better than (x),the same as (y),or worse than (z) thebaseline scheme on the datasets used in the experiment.In this case there is onlyone dataset;method 2 was equivalent to method 1 (the baseline) once,andmethod 3 was worse than it once.(The annotation (v/ /*)is placed at the bottomofcolumn 1 to help you remember the meanings ofthe three counts x/y/z.)12.2Simple setupIn the Setuppanel shown in Figure 12.1(a) we left most options at their defaultvalues.The experiment is a 10-fold cross-validation repeated 10 times.You canalter the number offolds in the box at center left and the number ofrepetitionsin the box at center right.The experiment type is classiﬁcation;you can specifyregression instead.You can choose several datasets,in which case each algorithmis applied to each dataset,and change the order ofiteration using the Data setsﬁrstand Algorithm ﬁrstbuttons.The alternative to cross-validation is theholdout method.There are two variants,depending on whether the order ofthedataset is preserved or the data is randomized.You can specify the percentagesplit (the default is two-thirds training set and one-third test set).Experimental setups can be saved and reopened.You can make notes aboutthe setup by pressing the Notesbutton,which brings up an editor window.Serious Weka users soon ﬁnd the need to open up an experiment and rerun itwith some modiﬁcations—perhaps with a new dataset or a new learning 12.2SIMPLE SETUP441P088407-Ch012.qxd  4/30/05  11:02 AM  Page 441algorithm.It would be nice to avoid having to recalculate all the results that havealready been obtained! Ifthe results have been placed in a database rather thanan ARFF or CSV ﬁle,this is exactly what happens.You can choose JDBC data-basein the results destination selector and connect to any database that has aJDBC driver.You need to specify the database’s URL and enter a username andpassword.To make this work with your database you may need to modify theweka/experiment/DatabaseUtils.propsﬁle in the Weka distribution.Ifyou alteran experiment that uses a database,Weka will reuse previously computed resultswhenever they are available.This greatly simpliﬁes the kind ofiterative experi-mentation that typically characterizes data mining research.12.3Advanced setupThe Experimenter has an advanced mode.Click near the top ofthe panel shownin Figure 12.1(a) to obtain the more formidable version ofthe panel shown inFigure 12.3.This enlarges the options available for controlling the experiment—including,for example,the ability to generate learning curves.However,the442CHAPTER 12|THE EXPERIMENTERFigure 12.3Setting up an experiment in advanced mode.P088407-Ch012.qxd  4/30/05  11:02 AM  Page 442advanced mode is hard to use,and the simple version sufﬁces for most pur-poses.For example,in advanced mode you can set up an iteration to test an algorithm with a succession ofdifferent parameter values,but the same effect can be achieved in simple mode by putting the algorithm into the listseveral times with different parameter values.Something you may need theadvanced mode for is to set up distributed experiments,which we describe inSection 12.5.12.4The Analyze panelOur walkthrough used the Analyzepanel to perform a statistical signiﬁcance testofone learning scheme (J48)versus two others (OneRand ZeroR).The test wason the error rate—the Comparisonﬁeld in Figure 12.2.Other statistics can beselected from the drop-down menu instead:percentage incorrect,percentageunclassiﬁed,root mean-squared error,the remaining error measures from Table5.8 (page 178),and various entropy ﬁgures.Moreover,you can see the standarddeviation ofthe attribute being evaluated by ticking the Show std deviationscheckbox.Use the Select basemenu to change the baseline scheme from J4.8 to one ofthe other learning schemes.For example,selecting OneRcauses the others to becompared with this scheme.In fact,that would show that there is a statisticallysigniﬁcant difference between OneRand ZeroRbut not between OneRand J48.Apart from the learning schemes,there are two other choices in the Select basemenu:Summaryand Ranking.The former compares each learning scheme withevery other scheme and prints a matrix whose cells contain the number ofdatasets on which one is signiﬁcantly better than the other.The latter ranks theschemes according to the total number ofdatasets that represent wins (>) andlosses (<) and prints a league table.The ﬁrst column in the output gives the dif-ference between the number ofwins and the number oflosses.The Rowand Columnﬁelds determine the dimensions ofthe comparisonmatrix.Clicking Selectbrings up a list ofall the features that have been meas-ured in the experiment—in other words,the column labels ofthe spreadsheetin Figure 12.1(c).You can select which to use as the rows and columns ofthematrix.(The selection does not appear in the Selectbox because more than oneparameter can be chosen simultaneously.) Figure 12.4 shows which items areselected for the rows and columns ofFigure 12.2.The two lists show the exper-imental parameters (the columns ofthe spreadsheet).Datasetis selected for therows (and there is only one in this case,the Iris dataset),and Scheme,Schemeoptions,and Scheme_version_IDare selected for the column (the usual conven-tion ofshift-clicking selects multiple entries).All three can be seen in Figure12.2—in fact,they are more easily legible in the key at the bottom.12.4THE ANALYZE PANEL443P088407-Ch012.qxd  4/30/05  11:02 AM  Page 443444CHAPTER 12|THE EXPERIMENTER(a)(c)(b)(d)Figure 12.4Rows and columns ofFigure 12.2:(a) row ﬁeld,(b) column ﬁeld,(c) resultofswapping the row and column selections,and (d) substituting Runfor Datasetasrows.P088407-Ch012.qxd  4/30/05  11:02 AM  Page 444Ifthe row and column selections were swapped and the Perform testbuttonwere pressed again,the matrix would be transposed,giving the result in Figure12.4(c).There are now three rows,one for each algorithm,and one column,forthe single dataset.Ifinstead the row ofDatasetwere replaced by Runand thetest were performed again,the result would be as in Figure 12.4(d).Runrefersto the runs ofthe cross-validation,ofwhich there are 10,so there are now 10rows.The number in parentheses after each row label (100 in Figure 12.4(c) and10 in Figure 12.4(d)) is the number ofresults corresponding to that row—inother words,the number ofmeasurements that participate in the averages dis-played by the cells in that row.There is also a button that allows you to select asubset ofcolumns to display (the baseline column is always included),andanother that allows you to select the output format:plain text (default),outputfor the LaTeX typesetting system,and CSV format.12.5Distributing processing over several machinesA remarkable feature ofthe Experimenter is that it can split up an experimentand distribute it across several processors.This is for advanced Weka users andis only available from the advanced version ofthe Setuppanel.Some users avoidworking with this panel by setting the experiment up on the simple version andswitching to the advanced version to distribute it,because the experiment’sstructure is preserved when you switch.However,distributing an experiment isan advanced feature and is often difﬁcult.For example,ﬁle and directory per-missions can be tricky to set up.Distributing an experiment works best when the results are all sent to acentral database by selecting JDBC databaseas the results destination in thepanel shown in Figure 12.1(a).It uses the RMI facility,and works with any data-base that has a JDBC driver.It has been tested on several freely available data-bases.Alternatively,you could instruct each host to save its results to a differentARFF ﬁle and merge the ﬁles afterwards.To distribute an experiment,each host must (1) have Java installed,(2) haveaccess to whatever datasets you are using,and (3) be running the weka.experi-ment.RemoteEngineexperiment server.Ifresults are sent to a central database,the appropriate JDBC drivers must be installed on each host.Getting all thisright is the difﬁcult part ofrunning distributed experiments.To initiate a remote engine experiment server on a host machine,ﬁrst copyremoteExperimentServer.jarfrom the Weka distribution to a directory on thehost.Unpack it withjar xvf remoteExperimentServer.jar12.5DISTRIBUTING PROCESSING OVER SEVERAL MACHINES445P088407-Ch012.qxd  4/30/05  11:02 AM  Page 445It expands to two ﬁles:remoteEngine.jar,an executable jarﬁle that contains theexperiment server,and remote.policy.The remote.policyﬁle grants the remote engine permission to perform certainoperations,such as connecting to ports or accessing a directory.It needs to be edited to specify correct paths in some ofthe permissions;this is self-explanatory when you examine the ﬁle.By default,it speciﬁes that code can bedownloaded on HTTP port 80 from anywhere on the Web,but the remoteengines can also load code from a ﬁle URL instead.To arrange this,uncommentthe example and replace the pathname appropriately.The remote engines alsoneed to be able to access the datasets used in an experiment (see the ﬁrst entryin remote.policy).The paths to the datasets are speciﬁed in the Experimenter(i.e.,the client),and the same paths must be applicable in the context oftheremote engines.To facilitate this it may be necessary to specify relative path-names by selecting the Use relative pathstick box shown in the Setuppanel ofthe Experimenter.To start the remote engine server,typejava -classpath remoteEngine.jar:<path_to_any_jdbc_drivers>-Djava.security.policy=remote.policy weka.experiment.RemoteEnginefrom the directory containing remoteEngine.jar.Ifall goes well you will see thismessage (or something like it):Host name : ml.cs.waikato.ac.nzRemoteEngineexception:Connectionrefusedtohost:ml.cs.waikato.ac.nz; nested exception is:java.net.ConnectException: Connection refusedAttempting to start rmi registry...RemoteEngine bound in RMI registryDespite initial appearances,this is good news! The connection was refusedbecause no RMI registry was running on that server,and hence the remoteengine has started one.Repeat the process on all hosts.It does not make senseto run more than one remote engine on a machine.Start the Experimenter by typingjava -Djava.rmi.server.codebase=<URL_for_weka_code>weka.gui.experiment.ExperimenterThe URL speciﬁes where the remote engines can ﬁnd the code to be executed.Ifthe URL denotes a directory (i.e.,one that contains the Weka directory) ratherthan a jarﬁle,it must end with path separator (e.g.,/).The Experimenter’s advanced Setuppanel in Figure 12.3 contains a smallpane at center left that determines whether an experiment will be distributedor not.This is normally inactive.To distribute the experiment click the check-446CHAPTER 12|THE EXPERIMENTERP088407-Ch012.qxd  4/30/05  11:02 AM  Page 446box to activate the Hostsbutton;a window will pop up asking for the machinesover which to distribute the experiment.Host names should be fully qualiﬁed(e.g.,ml.cs.waikato.ac.nz).Having entered the hosts,conﬁgure the rest ofthe experiment in the usualway (better still,conﬁgure it before switching to the advanced setup mode).When the experiment is started using the Runpanel,the progress ofthe subex-periments on the various hosts is displayed,along with any error messages.Distributing an experiment involves splitting it into subexperiments thatRMI sends to the hosts for execution.By default,experiments are partitionedby dataset,in which case there can be no more hosts than there are datasets.Then each subexperiment is self-contained:it applies all schemes to a singledataset.An experiment with only a few datasets can be partitioned by runinstead.For example,a 10 times 10-fold cross-validation would be split into 10subexperiments,1 per run.12.5DISTRIBUTING PROCESSING OVER SEVERAL MACHINES447P088407-Ch012.qxd  4/30/05  11:02 AM  Page 447P088407-Ch012.qxd  4/30/05  11:02 AM  Page 448Lurking behind Weka’s interactive interfaces—the Explorer,the KnowledgeFlow,and the Experimenter—lies its basic functionality.This can be accessed inraw form through a command-line interface.Select Simple CLIfrom the inter-face choices at the bottom ofFigure 10.3(a) to bring up a plain textual panelwith a line at the bottom on which you enter commands.Alternatively,use theoperating system’s command-line interface to run the classes in weka.jardirectly,in which case you must ﬁrst set the CLASSPATHenvironment variableas explained in Weka’s READMEﬁle.13.1Getting startedAt the beginning ofSection 10.1 we used the Explorer to invoke the J4.8 learneron the weather data.To do the same thing in the command-line interface,typejava weka.classiﬁers.trees.J48 -t data/weather.arffchapter13The Command-line Interface449P088407-Ch013.qxd  4/30/05  11:01 AM  Page 449into the line at the bottom ofthe text panel.This incantation calls the Javavirtual machine (in the Simple CLI,Java is already loaded) and instructs it toexecute J4.8.Weka is organized in packagesthat correspond to a directory hier-archy.The program to be executed is called J48and resides in the treespackage,which is a subpackage ofclassiﬁers,which is part ofthe overall wekapackage.The next section gives more details ofthe package structure.The -toptionsignals that the next argument is the name ofthe training ﬁle:we are assumingthat the weather data resides in a datasubdirectory ofthe directory from which you ﬁred up Weka.The result resembles the text shown in Figure 10.5.In the Simple CLI it appears in the panel above the line where you typed thecommand.13.2The structure of WekaWe have explained how to invoke ﬁltering and learning schemes with theExplorer and connect them together with the Knowledge Flow interface.To gofurther,it is necessary to learn something about how Weka is put together.Detailed,up-to-date information can be found in the online documentationincluded in the distribution.This is more technical than the descriptions ofthelearning and ﬁltering schemes given by the Morebutton in the Explorer andKnowledge Flow’s object editors.It is generated directly from comments in thesource code using Sun’s Javadoc utility.To understand its structure,you need toknow how Java programs are organized.Classes, instances, and packagesEvery Java program is implemented as a class.In object-oriented programming,a classis a collection ofvariables along with some methodsthat operate on them.Together,they deﬁne the behavior ofan object belonging to the class.An objectis simply an instantiation ofthe class that has values assigned to all the class’svariables.In Java,an object is also called an instanceofthe class.Unfortunately,this conﬂicts with the terminology used in this book,where the terms classandinstanceappear in the quite different context ofmachine learning.From nowon,you will have to infer the intended meaning ofthese terms from theircontext.This is not difﬁcult—and sometimes we’ll use the word objectinsteadofJava’s instanceto make things clear.In Weka,the implementation ofa particular learning algorithm is encapsu-lated in a class.For example,the J48class described previously builds a C4.5decision tree.Each time the Java virtual machine executes J48,it creates aninstance ofthis class by allocating memory for building and storing a decisiontree classiﬁer.The algorithm,the classiﬁer it builds,and a procedure for out-putting the classiﬁer are all part ofthat instantiation ofthe J48class.450CHAPTER 13|THE COMMAND-LINE INTERFACEP088407-Ch013.qxd  4/30/05  11:01 AM  Page 450Larger programs are usually split into more than one class.The J48class,forexample,does not actually contain any code for building a decision tree.Itincludes references to instances ofother classes that do most ofthe work.Whenthere are a lot ofclasses—as in Weka—they become difﬁcult to comprehendand navigate.Java allows classes to be organized into packages.A packageis justa directory containing a collection ofrelated classes:for example,the treespackage mentioned previously contains the classes that implement decisiontrees.Packages are organized in a hierarchy that corresponds to the directoryhierarchy:treesis a subpackage ofthe classiﬁerspackage,which is itselfa sub-package ofthe overall wekapackage.When you consult the online documentation generated by Javadoc from yourWeb browser,the ﬁrst thing you see is an alphabetical list ofall the packages inWeka,as shown in Figure 13.1(a).Here we introduce a few ofthem in order ofimportance.The weka.core packageThe corepackage is central to the Weka system,and its classes are accessed from almost every other class.You can determine what they are by clicking on the weka.corehyperlink,which brings up the Web page shown in Figure13.1(b).The Web page in Figure 13.1(b) is divided into two parts:the interfacesummaryand the class summary.The latter is a list ofclasses contained withinthe package,and the former lists the interfaces it provides.An interface is similarto a class,the only difference being that it doesn’t actually do anything by itself—it is merely a list ofmethods without actual implementations.Other classes candeclare that they “implement”a particular interface and then provide code forits methods.For example,the OptionHandlerinterface deﬁnes those methodsthat are implemented by all classes that can process command-line options,including all classiﬁers.The key classes in the corepackage are Attribute,Instance,and Instances.Anobject ofclass Attributerepresents an attribute.It contains the attribute’s name,its type,and,in the case ofa nominal or string attribute,its possible values.Anobject ofclass Instancecontains the attribute values ofa particular instance;andan object ofclass Instancesholds an ordered set ofinstances,in other words,adataset.You can learn more about these classes by clicking their hyperlinks;wereturn to them in Chapter 14 when we show you how to invoke machine learn-ing schemes from other Java code.However,you can use Weka from thecommand line without knowing the details.Clicking the Overviewhyperlink in the upper left corner ofany documenta-tion page returns you to the listing ofall the packages in Weka that is shown inFigure 13.1(a).13.2THE STRUCTURE OF WEKA451P088407-Ch013.qxd  4/30/05  11:01 AM  Page 451452CHAPTER 13|THE COMMAND-LINE INTERFACE(a)(b)Figure 13.1Using Javadoc:(a) the front page and (b) the weka.corepackage.P088407-Ch013.qxd  4/30/05  11:01 AM  Page 452The weka.classiﬁers packageThe classiﬁerspackage contains implementations ofmost ofthe algorithms forclassiﬁcation and numeric prediction described in this book.(Numeric predic-tion is included in classiﬁers:it is interpreted as prediction ofa continuous class.)The most important class in this package is Classiﬁer,which deﬁnes the generalstructure ofany scheme for classiﬁcation or numeric prediction.Classiﬁercon-tains three methods,buildClassiﬁer(),classifyInstance(),and distributionForIn-stance().In the terminology ofobject-oriented programming,the learningalgorithms are represented by subclasses ofClassiﬁerand therefore automati-cally inherit these three methods.Every scheme redeﬁnes them according to howit builds a classiﬁer and how it classiﬁes instances.This gives a uniform inter-face for building and using classiﬁers from other Java code.Hence,for example,the same evaluation module can be used to evaluate the performance ofanyclassiﬁer in Weka.To see an example,click on weka.classiﬁers.treesand then on DecisionStump,which is a class for building a simple one-level binary decision tree (with anextra branch for missing values).Its documentation page,shown in Figure 13.2,shows the fully qualiﬁed name ofthis class,weka.classiﬁers.trees.DecisionStump,near the top.You have to use this rather lengthy name whenever you build adecision stump from the command line.The class name is sited in a small treestructure showing the relevant part ofthe class hierarchy.As you can see,Deci-sionStumpis a subclass ofweka.classiﬁers.Classiﬁer,which is itselfa subclass ofjava.lang.Object.The Objectclass is the most general one in Java:all classes areautomatically subclasses ofit.After some generic information about the class—briefdocumentation,itsversion,and its author—Figure 13.2 gives an index ofthe constructors andmethods ofthis class.A constructoris a special kind ofmethod that is calledwhenever an object ofthat class is created,usually initializing the variables thatcollectively deﬁne its state.The index ofmethods lists the name ofeach one,thetype ofparameters it takes,and a short description ofits functionality.Beneaththose indices,the Web page gives more details about the constructors andmethods.We will return to these details later.As you can see,DecisionStumpoverwrites the distributionForInstance()method from Classiﬁer:the default implementation ofclassifyInstance()in Clas-siﬁerthen uses this method to produce its classiﬁcations.In addition,it containsthe toString(),toSource(),and main()methods.The ﬁrst returns a textualdescription ofthe classiﬁer,used whenever it is printed on the screen.Thesecond is used to obtain a source code representation ofthe learned classiﬁer.The third is called when you ask for a decision stump from the command line,in other words,every time you enter a command beginning withjava weka.classiﬁers.trees.DecisionStump13.2THE STRUCTURE OF WEKA453P088407-Ch013.qxd  4/30/05  11:01 AM  Page 453454CHAPTER 13|THE COMMAND-LINE INTERFACEFigure 13.2DecisionStump:A class ofthe weka.classiﬁers.treespackage.P088407-Ch013.qxd  4/30/05  11:01 AM  Page 454The presence ofa main()method in a class indicates that it can be run from thecommand line and that all learning methods and ﬁlter algorithms implementit.Other packagesSeveral other packages listed in Figure 13.1(a) are worth mentioning:weka.asso-ciations,weka.clusterers,weka.estimators,weka.ﬁlters,and weka.attributeSelec-tion.The weka.associationspackage contains association rule learners.Thesehave been placed in a separate package because association rules are funda-mentally different from classiﬁers.The weka.clustererspackage containsmethods for unsupervised learning.The weka.estimatorspackage contains sub-classes ofa generic Estimatorclass,which computes different types ofproba-bility distribution.These subclasses are used by the Naïve Bayes algorithm(among others).In the weka.ﬁlterspackage,the Filterclass deﬁnes the general structure ofclasses containing ﬁlter algorithms,which are all implemented as subclasses ofFilter.Like classiﬁers,ﬁlters can be used from the command line:we will seehow shortly.The weka.attributeSelectionpackage contains several classes for attribute selection.These are used by the AttributeSelectionFilterinweka.ﬁlters.supervised.attribute,but can also be invoked separately.13.2THE STRUCTURE OF WEKA455Figure 13.2(continued)P088407-Ch013.qxd  4/30/05  11:01 AM  Page 455Javadoc indicesAs mentioned previously,all classes are automatically subclasses ofObject.Toexamine the tree that corresponds to Weka’s hierarchy ofclasses,select theOverviewlink from the top ofany page ofthe online documentation.Click Treeto display the overview as a tree that shows which classes are subclasses or super-classes ofa particular class—for example,which classes inherit from Classiﬁer.The online documentation contains an index ofall publicly accessible vari-ables (called ﬁelds) and methods in Weka—in other words,all ﬁelds andmethods that you can access from your own Java code.To view it,click Overviewand then Index.Suppose you want to check which Weka classiﬁers and ﬁlters are capable ofoperating incrementally.Searching for the word incrementalin the index wouldsoon lead you to the keyword UpdateableClassiﬁer.In fact,this is a Java inter-face;interfaces are listed after the classes in the overview tree.You are lookingfor all classes that implement this interface.Clicking any occurrence ofit in thedocumentation brings up a page that describes the interface and lists the clas-siﬁers that implement it.Finding the ﬁlters is a little trickier unless you knowthe keyword StreamableFilter,which is the name ofthe interface that streamsdata through a ﬁlter:again,its page lists the ﬁlters that implement it.You wouldstumble across that keyword ifyou knew any example ofa ﬁlter that couldoperate incrementally.13.3Command-line optionsIn the preceding example,the -toption was used in the command line to com-municate the name ofthe training ﬁle to the learning algorithm.There are manyother options that can be used with any learning scheme,and also scheme-speciﬁc ones that apply only to particular schemes.Ifyou invoke a schemewithout any command-line options at all,it displays the applicable options:ﬁrstthe general options,then the scheme-speciﬁc ones.In the command-line inter-face,type:java weka.classiﬁers.trees.J48You’ll see a list ofthe options common to all learning schemes,shown in Table13.1,followed by those that apply only to J48,shown in Table 13.2.We willexplain the generic options and then brieﬂy review the scheme-speciﬁc ones.Generic optionsThe options in Table 13.1 determine which data is used for training and testing,how the classiﬁer is evaluated,and what kind ofstatistics are displayed.Forexample,the -Toption is used to provide the name ofthe test ﬁle when evalu-456CHAPTER 13|THE COMMAND-LINE INTERFACEP088407-Ch013.qxd  4/30/05  11:01 AM  Page 456ating a learning scheme on an independent test set.By default the class is thelast attribute in an ARFF ﬁle,but you can declare another one to be the classusing -cfollowed by the position ofthe desired attribute,1 for the ﬁrst,2 forthe second,and so on.When cross-validation is performed (the default ifa testﬁle is not provided),the data is randomly shufﬂed ﬁrst.To repeat the cross-validation several times,each time reshufﬂing the data in a different way,set therandom number seed with -s(default value 1).With a large dataset you maywant to reduce the number offolds for the cross-validation from the defaultvalue of10 using -x.In the Explorer,cost-sensitive evaluation is invoked as described in Section10.1.To achieve the same effect from the command line,use the -moption toprovide the name ofa ﬁle containing the cost matrix.Here is a cost matrix forthe weather data:22% Number of rows and columns in the matrix010% If true class yes and prediction no, penalty is 1010% If true class no and prediction yes, penalty is 1The ﬁrst line gives the number ofrows and columns,that is,the number ofclassvalues.Then comes the matrix ofpenalties.Comments introduced by %can beappended to the end ofany line.It is also possible to save and load models.Ifyou provide the name ofanoutput ﬁle using -d,Weka saves the classiﬁer generated from the training data.13.3COMMAND-LINE OPTIONS457Table 13.1Generic options for learning schemes in Weka.OptionFunction-t <training ﬁle>Specify training ﬁle-T <test ﬁle>Specify test ﬁle; if none, a cross-validation is performed on the training data-c <class index>Specify index of class attribute-s <random number seed>Specify random number seed for cross-validation-x <number of folds>Specify number of folds for cross-validation-m <cost matrix ﬁle>Specify ﬁle containing cost matrix-d <output ﬁle>Specify output ﬁle for model-l <input ﬁle>Specify input ﬁle for model-oOutput statistics only, not the classiﬁer-iOutput information retrieval statistics for two-class problems-kOutput information-theoretical statistics-p <attribute range>Output predictions for test instances-vOutput no statistics for training data-rOutput cumulative margin distribution-z <class name>Output source representation of classiﬁer-gOutput graph representation of classiﬁerP088407-Ch013.qxd  4/30/05  11:01 AM  Page 457To evaluate the same classiﬁer on a new batch oftest data,you load it back using-linstead ofrebuilding it.Ifthe classiﬁer can be updated incrementally,youcan provide both a training ﬁle and an input ﬁle,and Weka will load the clas-siﬁer and update it with the given training instances.Ifyou wish only to assess the performance ofa learning scheme,use -otosuppress output ofthe model.Use -ito see the performance measures ofpre-cision,recall,and F-measure (Section 5.7).Use -kto compute information-theoretical measures from the probabilities derived by a learning scheme(Section 5.6).Weka users often want to know which class values the learning scheme actu-ally predicts for each test instance.The -poption prints each test instance’snumber,its class,the conﬁdence ofthe scheme’s prediction,and the predictedclass value.It also outputs attribute values for each instance and must be fol-lowed by a speciﬁcation ofthe range (e.g.,1–2)—use 0 ifyou don’t want anyattribute values.You can also output the cumulative margin distributionfor thetraining data,which shows how the distribution ofthe margin measure (Section7.5,page 324) changes with the number ofboosting iterations.Finally,you canoutput the classiﬁer’s source representation,and a graphical representation ifthe classiﬁer can produce one.Scheme-speciﬁc optionsTable 13.2 shows the options speciﬁc to J4.8.You can force the algorithm to usethe unpruned tree instead ofthe pruned one.You can suppress subtree raising,which increases efﬁciency.You can set the conﬁdence threshold for pruning andthe minimum number ofinstances permissible at any leaf—both parameterswere described in Section 6.1 (page 199).As well as C4.5’s standard pruning458CHAPTER 13|THE COMMAND-LINE INTERFACETable 13.2Scheme-speciﬁc options for the J4.8 decision tree learner.OptionFunction-UUse unpruned tree-C <pruning conﬁdence>Specify conﬁdence threshold for pruning-M <number of instances>Specify minimum number of instances in any leaf-RUse reduced-error pruning-N <number of folds>Specify number of folds for reduced-error pruning; use one fold as pruning set-BUse binary splits only-SDon’t perform subtree raising-LRetain instance information-ASmooth the probability estimates using Laplace smoothing-QSeed for shufﬂing dataP088407-Ch013.qxd  4/30/05  11:01 AM  Page 458procedure,reduced-error pruning (Section 6.2,pages 202–203) can be per-formed.The -Noption governs the size ofthe holdout set:the dataset is dividedequally into that number ofparts and the last is held out (default value 3).Youcan smooth the probability estimates using the Laplace technique,set therandom number seed for shufﬂing the data when selecting a pruning set,andstore the instance information for future visualization.Finally,to build a binarytree instead ofone with multiway branches for nominal attributes,use -B.13.3COMMAND-LINE OPTIONS459P088407-Ch013.qxd  4/30/05  11:01 AM  Page 459P088407-Ch013.qxd  4/30/05  11:01 AM  Page 460When invoking learning schemes from the graphical user interfaces or thecommand line,there is no need to know anything about programming in Java.In this section we show how to access these algorithms from your own code.Indoing so,the advantages ofusing an object-oriented programming language willbecome clear.From now on,we assume that you have at least some rudimen-tary knowledge ofJava.In most practical applications ofdata mining the learn-ing component is an integrated part ofa far larger software environment.Iftheenvironment is written in Java,you can use Weka to solve the learning problemwithout writing any machine learning code yourself.14.1A simple data mining applicationWe present a simple data mining application for learning a model that classi-ﬁes text ﬁles into two categories,hitand miss.The application works for arbi-trary documents:We refer to them as messages.The implementation uses thechapter14Embedded Machine Learning461P088407-Ch014.qxd  4/30/05  11:04 AM  Page 461StringToWordVectorﬁlter mentioned in Section 10.3 (page 399) to convert themessages into attribute vectors in the manner described in Section 7.3.Weassume that the program is called every time a new ﬁle is to be processed.IftheWeka user provides a class label for the ﬁle,the system uses it for training;ifnot,it classiﬁes it.The decision tree classiﬁer J48is used to do the work.14.2Going through the codeFigure 14.1 shows the source code for the application program,implemented ina class called MessageClassiﬁer.The command-line arguments that the main()method accepts are the name ofa text ﬁle (given by -m),the name ofa ﬁleholding an object ofclass MessageClassiﬁer (-t),and,optionally,the classiﬁca-tion ofthe message in the ﬁle (-c).Ifthe user provides a classiﬁcation,themessage will be converted into an example for training;ifnot,the Message-Classiﬁerobject will be used to classify it as hitor miss.main()The main()method reads the message into a Java StringBufferand checkswhether the user has provided a classiﬁcation for it.Then it reads a Message-Classiﬁerobject from the ﬁle given by -tand creates a new object ofclass MessageClassiﬁerifthis ﬁle does not exist.In either case the resulting object iscalled messageCl.After checking for illegal command-line options,the programcalls the method updateData()to update the training data stored in messageClifa classiﬁcation has been provided;otherwise,it calls classifyMessage()to clas-sify it.Finally,the messageClobject is saved back into the ﬁle,because it may have changed.In the following sections,we ﬁrst describe how a new MessageClassiﬁerobject is created by the constructor MessageClassiﬁer()andthen explain how the two methods updateData()and classifyMessage()work.MessageClassiﬁer()Each time a new MessageClassiﬁeris created,objects for holding the ﬁlter andclassiﬁer are generated automatically.The only nontrivial part ofthe process iscreating a dataset,which is done by the constructor MessageClassiﬁer().First thedataset’s name is stored as a string.Then an Attributeobject is created for eachattribute,one to hold the string corresponding to a text message and the otherfor its class.These objects are stored in a dynamic array oftype FastVector.(FastVectoris Weka’s own implementation ofthe standard Java Vectorclass andis used throughout Weka for historical reasons.)Attributes are created by invoking one ofthe constructors in the classAttribute.This class has a constructor that takes one parameter—the attribute’sname—and creates a numeric attribute.However,the constructor we use here462CHAPTER 14|EMBEDDED MACHINE LEARNINGP088407-Ch014.qxd  4/30/05  11:04 AM  Page 46214.2GOING THROUGH THE CODE463/** * Java program for classifying text messages into two classes. */import weka.core.Attribute;import weka.core.Instance;import weka.core.Instances;import weka.core.FastVector;import weka.core.Utils;import weka.classifiers.Classifier;import weka.classifiers.trees.J48;import weka.filters.Filter;import weka.filters.unsupervised.attribute.StringToWordVector;import java.io.*;public class MessageClassifier implements Serializable {   /* The training data gathered so far. */  private Instances m_Data = null;  /* The filter used to generate the word counts. */  private StringToWordVector m_Filter = new StringToWordVector();  /* The actual classifier. */  private Classifier m_Classifier = new J48();  /* Whether the model is up to date. */  private boolean m_UpToDate;  /**   * Constructs empty training dataset.   */  public MessageClassifier() throws Exception {     String nameOfDataset = "MessageClassificationProblem";    // Create vector of attributes.    FastVector attributes = new FastVector(2);    // Add attribute for holding messages.    attributes.addElement(new Attribute("Message", (FastVector)null));Figure 14.1Source code for the message classiﬁer.P088407-Ch014.qxd  4/30/05  11:04 AM  Page 463464CHAPTER 14|EMBEDDED MACHINE LEARNING    // Add class attribute.    FastVector classValues = new FastVector(2);    classValues.addElement("miss");    classValues.addElement("hit");    attributes.addElement(new Attribute("Class", classValues));    // Create dataset with initial capacity of 100, and set index of class.    m_Data = new Instances(nameOfDataset, attributes, 100);    m_Data.setClassIndex(m_Data.numAttributes() - 1);  }   /**   * Updates data using the given training message.   */  public void updateData(String message, String classValue) throws Exception {     // Make message into instance.    Instance instance = makeInstance(message, m_Data);    // Set class value for instance.    instance.setClassValue(classValue);    // Add instance to training data.    m_Data.add(instance);    m_UpToDate = false;  }   /**   * Classifies a given message.   */  public void classifyMessage(String message) throws Exception {     // Check whether classifier has been built.    if (m_Data.numInstances() == 0) {       throw new Exception("No classifier available.");    }     // Check whether classifier and filter are up to date.    if (!m_UpToDate) { Figure 14.1(continued)P088407-Ch014.qxd  4/30/05  11:04 AM  Page 46414.2GOING THROUGH THE CODE465      // Initialize filter and tell it about the input format.      m_Filter.setInputFormat(m_Data);      // Generate word counts from the training data.      Instances filteredData  = Filter.useFilter(m_Data, m_Filter);      // Rebuild classifier.      m_Classifier.buildClassifier(filteredData);      m_UpToDate = true;    }     // Make separate little test set so that message    // does not get added to string attribute in m_Data.    Instances testset = m_Data.stringFreeStructure();    // Make message into test instance.    Instance instance = makeInstance(message, testset);    // Filter instance.    m_Filter.input(instance);    Instance filteredInstance = m_Filter.output();    // Get index of predicted class value.    double predicted = m_Classifier.classifyInstance(filteredInstance);    // Output class value.    System.err.println("Message classified as : " +                       m_Data.classAttribute().value((int)predicted));  }   /**   * Method that converts a text message into an instance.   */  private Instance makeInstance(String text, Instances data) {     // Create instance of length two.    Instance instance = new Instance(2);    // Set value for message attribute    Attribute messageAtt = data.attribute("Message");    instance.setValue(messageAtt, messageAtt.addStringValue(text));Figure 14.1(continued)P088407-Ch014.qxd  4/30/05  11:04 AM  Page 465466CHAPTER 14|EMBEDDED MACHINE LEARNING    // Give instance access to attribute information from the dataset.    instance.setDataset(data);    return instance;  }   /**   * Main method.   */  public static void main(String[] options) {     try {       // Read message file into string.      String messageName = Utils.getOption('m', options);      if (messageName.length() == 0) {         throw new Exception("Must provide name of message file.");      }       FileReader m = new FileReader(messageName);      StringBuffer message = new StringBuffer(); int l;      while ((l = m.read()) != -1) {         message.append((char)l);      }       m.close();      // Check if class value is given.      String classValue = Utils.getOption('c', options);      // If model file exists, read it, otherwise create new one.      String modelName = Utils.getOption('o', options);      if (modelName.length() == 0) {         throw new Exception("Must provide name of model file.");      }       MessageClassifier messageCl;      try {         ObjectInputStream modelInObjectFile =           new ObjectInputStream(new FileInputStream(modelName));        messageCl = (MessageClassifier) modelInObjectFile.readObject();        modelInObjectFile.close();      } catch (FileNotFoundException e) {         messageCl = new MessageClassifier();Figure 14.1(continued)P088407-Ch014.qxd  4/30/05  11:04 AM  Page 46614.2GOING THROUGH THE CODE467      }       // Check if there are any options left      Utils.checkForRemainingOptions(options);      // Process message.      if (classValue.length() != 0) {         messageCl.updateData(message.toString(), classValue);      } else {         messageCl.classifyMessage(message.toString());      }       // Save message classifier object.      ObjectOutputStream modelOutObjectFile =         new ObjectOutputStream(new FileOutputStream(modelName));      modelOutObjectFile.writeObject(messageCl);      modelOutObjectFile.close();    } catch (Exception e) {       e.printStackTrace();    }   } } Figure 14.1(continued)takes two parameters:the attribute’s name and a reference to a FastVector.Ifthisreference is null,as in the ﬁrst application ofthis constructor in our program,Weka creates an attribute oftype string.Otherwise,a nominal attribute iscreated.In that case it is assumed that the FastVectorholds the attribute valuesas strings.This is how we create a class attribute with two values hitand miss:by passing the attribute’s name (class)and its values—stored in a FastVector—to Attribute().To create a dataset from this attribute information,MessageClassiﬁer()mustcreate an object ofthe class Instancesfrom the corepackage.The constructor ofInstancesused by MessageClassiﬁer()takes three arguments:the dataset’sname,a FastVectorcontaining the attributes,and an integer indicating thedataset’s initial capacity.We set the initial capacity to 100;it is expanded auto-matically ifmore instances are added.After constructing the dataset,Message-Classiﬁer()sets the index ofthe class attribute to be the index ofthe lastattribute.P088407-Ch014.qxd  4/30/05  11:04 AM  Page 467updateData()Now that you know how to create an empty dataset,consider how the Mes-sageClassiﬁerobject actually incorporates a new training message.The methodupdateData()does this job.It ﬁrst converts the given message into a traininginstance by calling makeInstance(),which begins by creating an object ofclassInstancethat corresponds to an instance with two attributes.The constructor ofthe Instanceobject sets all the instance’s values to be missingand its weight to1.The next step in makeInstance()is to set the value ofthe string attributeholding the text ofthe message.This is done by applying the setValue()methodofthe Instanceobject,providing it with the attribute whose value needs to bechanged,and a second parameter that corresponds to the new value’s index inthe deﬁnition ofthe string attribute.This index is returned by the addString-Value()method,which adds the message text as a new value to the string attrib-ute and returns the position ofthis new value in the deﬁnition ofthe stringattribute.Internally,an Instancestores all attribute values as double-precision ﬂoating-point numbers regardless ofthe type ofthe corresponding attribute.In the caseofnominal and string attributes this is done by storing the index ofthe corre-sponding attribute value in the deﬁnition ofthe attribute.For example,the ﬁrstvalue ofa nominal attribute is represented by 0.0,the second by 1.0,and so on.The same method is used for string attributes:addStringValue()returns the indexcorresponding to the value that is added to the deﬁnition ofthe attribute.Once the value for the string attribute has been set,makeInstance()gives thenewly created instance access to the data’s attribute information by passing it areference to the dataset.In Weka,an Instanceobject does not store the type ofeach attribute explicitly;instead,it stores a reference to a dataset with the corresponding attribute information.Returning to updateData(),once the new instance has been returned frommakeInstance()its class value is set and it is added to the training data.We alsoinitialize m_UpToDate,a ﬂag indicating that the training data has changed andthe predictive model is hence not up to date.classifyMessage()Now let’s examine how MessageClassiﬁerprocesses a message whose class labelis unknown.The classifyMessage()method ﬁrst checks whether a classiﬁer hasbeen built by determining whether any training instances are available.It thenchecks whether the classiﬁer is up to date.Ifnot (because the training data haschanged) it must be rebuilt.However,before doing so the data must be con-verted into a format appropriate for learning using the StringToWordVectorﬁlter.First,we tell the ﬁlter the format ofthe input data by passing it a reference tothe input dataset using setInputFormat().Every time this method is called,the468CHAPTER 14|EMBEDDED MACHINE LEARNINGP088407-Ch014.qxd  4/30/05  11:04 AM  Page 468ﬁlter is initialized—that is,all its internal settings are reset.In the next step,thedata is transformed by useFilter().This generic method from the Filterclassapplies a ﬁlter to a dataset.In this case,because StringToWordVectorhas just beeninitialized,it computes a dictionary from the training dataset and then uses itto form word vectors.After returning from useFilter(),all the ﬁlter’s internal set-tings are ﬁxed until it is initialized by another call ofinputFormat().This makesit possible to ﬁlter a test instance without updating the ﬁlter’s internal settings(in this case,the dictionary).Once the data has been ﬁltered,the program rebuilds the classiﬁer—in ourcase a J4.8 decision tree—by passing the training data to its buildClassiﬁer()method and sets m_UpToDateto true.It is an important convention in Wekathat the buildClassiﬁer()method completely initializes the model’s internal set-tings before generating a new classiﬁer.Hence we do not need to construct anew J48object before we call buildClassiﬁer().Having ensured that the model stored in m_Classiﬁeris current,we proceedto classify the message.Before makeInstance()is called to create an Instanceobject from it,a new Instancesobject is created to hold the new instance andpassed as an argument to makeInstance().This is done so that makeInstance()does not add the text ofthe message to the deﬁnition ofthe string attribute inm_Data.Otherwise,the size ofthe m_Dataobject would grow every time a newmessage was classiﬁed,which is clearly not desirable—it should only grow whentraining instances are added.Hence a temporary Instancesobject is created anddiscarded once the instance has been processed.This object is obtained usingthe method stringFreeStructure(),which returns a copy ofm_Datawith anempty string attribute.Only then is makeInstance()called to create the newinstance.The test instance must also be processed by the StringToWordVectorﬁlterbefore being classiﬁed.This is easy:the input()method enters the instance intothe ﬁlter object,and the transformed instance is obtained by calling output().Then a prediction is produced by passing the instance to the classiﬁer’s classi-fyInstance()method.As you can see,the prediction is coded as a doublevalue.This allows Weka’s evaluation module to treat models for categorical andnumeric prediction similarly.In the case ofcategorical prediction,as in thisexample,the doublevariable holds the index ofthe predicted class value.Tooutput the string corresponding to this class value,the program calls the value()method ofthe dataset’s class attribute.There is at least one way in which our implementation could be improved.The classiﬁer and the StringToWordVectorﬁlter could be combined using the FilteredClassiﬁermetalearner described in Section 10.3 (page 401).This classi-ﬁer would then be able to deal with string attributes directly,without explicitlycalling the ﬁlter to transform the data.We didn’t do this because we wanted todemonstrate how ﬁlters can be used programmatically.14.2GOING THROUGH THE CODE469P088407-Ch014.qxd  4/30/05  11:04 AM  Page 469P088407-Ch014.qxd  4/30/05  11:04 AM  Page 470Suppose you need to implement a special-purpose learning algorithm that isnot included in Weka.Or suppose you are engaged in machine learning researchand want to investigate a new learning scheme.Or suppose you just want tolearn more about the inner workings ofan induction algorithm by actually pro-gramming it yourself.This section uses a simple example to show how to makefull use ofWeka’s class hierarchy when writing classiﬁers.Weka includes the elementary learning schemes listed in Table 15.1,mainlyfor educational purposes.None take any scheme-speciﬁc command-lineoptions.They are all useful for understanding the inner workings ofa classiﬁer.As an example,we describe the weka.classiﬁers.trees.Id3scheme,which imple-ments the ID3 decision tree learner from Section 4.3.15.1An example classiﬁerFigure 15.1 gives the source code ofweka.classiﬁers.trees.Id3,which,as you cansee from the code,extends the Classiﬁerclass.Every classiﬁer in Weka does so,whether it predicts a nominal class or a numeric one.chapter15Writing New Learning Schemes471P088407-Ch015.qxd  4/30/05  10:58 AM  Page 471The ﬁrst method in weka.classiﬁers.trees.Id3is globalInfo():we mention it here before moving on to the more interesting parts.It simply returns a string that is displayed in Weka’s graphical user interfaces when this scheme isselected.buildClassiﬁer()The buildClassiﬁer()method constructs a classiﬁer from a training dataset.Inthis case it ﬁrst checks the data for a nonnominal class,missing attribute value,or any attribute that is not nominal,because the ID3 algorithm cannot handlethese.It then makes a copy ofthe training set (to avoid changing the originaldata) and calls a method from weka.core.Instancesto delete all instances withmissing class values,because these instances are useless in the training process.Finally,it calls makeTree(),which actually builds the decision tree by recursivelygenerating all subtrees attached to the root node.makeTree()The ﬁrst step in makeTree()is to check whether the dataset is empty.Ifit is,aleafis created by setting m_Attributeto null.The class value m_ClassValueassigned to this leafis set to be missing,and the estimated probability for eachofthe dataset’s classes in m_Distributionis initialized to 0.Iftraining instancesare present,makeTree()ﬁnds the attribute that yields the greatest informationgain for them.It ﬁrst creates a Java enumerationofthe dataset’s attributes.Iftheindex ofthe class attribute is set—as it will be for this dataset—the class is auto-matically excluded from the enumeration.Inside the enumeration,each attribute’s information gain is computed bycomputeInfoGain()and stored in an array.We will return to this method later.The index()method from weka.core.Attributereturns the attribute’s index in thedataset,which is used to index the array.Once the enumeration is complete,theattribute with the greatest information gain is stored in the instance variablem_Attribute.The maxIndex()method from weka.core.Utilsreturns the index ofthe greatest value in an array ofintegers or doubles.(Ifthere is more than oneelement with the maximum value,the ﬁrst is returned.) The index ofthis attrib-472CHAPTER 15|WRITING NEW LEARNING SCHEMESTable 15.1Simple learning schemes in Weka.SchemeDescriptionBook sectionweka.classiﬁers.bayes.NaiveBayesSimpleProbabilistic learner4.2weka.classiﬁers.trees.Id3Decision tree learner4.3weka.classiﬁers.rules.PrismRule learner4.4weka.classiﬁers.lazy.IB1Instance-based learner4.7P088407-Ch015.qxd  4/30/05  10:58 AM  Page 47215.1AN EXAMPLE CLASSIFIER473package weka.classifiers.trees;import weka.classifiers.*;import weka.core.*;import java.io.*;import java.util.*;/** * Class implementing an Id3 decision tree classifier. */public class Id3 extends Classifier {   /** The node's successors. */   private Id3[] m_Successors;  /** Attribute used for splitting. */  private Attribute m_Attribute;  /** Class value if node is leaf. */  private double m_ClassValue;  /** Class distribution if node is leaf. */  private double[] m_Distribution;  /** Class attribute of dataset. */  private Attribute m_ClassAttribute;  /**   * Returns a string describing the classifier.   * @return a description suitable for the GUI.   */  public String globalInfo() {     return  "Class for constructing an unpruned decision tree based on the ID3 "       + "algorithm. Can only deal with nominal attributes. No missing values "       + "allowed. Empty leaves may result in unclassified instances. For more "       + "information see: \n\n"      + " R. Quinlan (1986). \"Induction of decision "       + "trees\". Machine Learning. Vol.1, No.1, pp. 81-106";  } Figure 15.1Source code for the ID3 decision tree learner.P088407-Ch015.qxd  4/30/05  10:58 AM  Page 473474CHAPTER 15|WRITING NEW LEARNING SCHEMES  /**   * Builds Id3 decision tree classifier.   *    * @param data the training data   * @exception Exception if classifier can't be built successfully   */  public void buildClassifier(Instances data) throws Exception {     if (!data.classAttribute().isNominal()) {       throw new UnsupportedClassTypeException("Id3: nominal class, please.");    }     Enumeration enumAtt = data.enumerateAttributes();    while (enumAtt.hasMoreElements()) {       if (!((Attribute) enumAtt.nextElement()).isNominal()) {         throw new UnsupportedAttributeTypeException("Id3: only nominal " +                                                     "attributes, please.");      }     }     Enumeration enum = data.enumerateInstances();    while (enum.hasMoreElements()) {       if (((Instance) enum.nextElement()).hasMissingValue()) {         throw new NoSupportForMissingValuesException("Id3: no missing values, "                                                      + "please.");      }     }     data = new Instances(data);    data.deleteWithMissingClass();     makeTree(data);  }   /**   * Method for building an Id3 tree.   *    * @param data the training data   * @exception Exception if decision tree can't be built successfully   */  private void makeTree(Instances data) throws Exception {     // Check if no instances have reached this node.    if (data.numInstances() == 0) {       m_Attribute = null;Figure 15.1(continued)P088407-Ch015.qxd  4/30/05  10:58 AM  Page 47415.1AN EXAMPLE CLASSIFIER475      m_ClassValue = Instance.missingValue();      m_Distribution = new double[data.numClasses()];      return;    }     // Compute attribute with maximum information gain.    double[] infoGains = new double[data.numAttributes()];    Enumeration attEnum = data.enumerateAttributes();    while (attEnum.hasMoreElements()) {       Attribute att = (Attribute) attEnum.nextElement();      infoGains[att.index()] = computeInfoGain(data, att);    }     m_Attribute = data.attribute(Utils.maxIndex(infoGains));    // Make leaf if information gain is zero.     // Otherwise create successors.    if (Utils.eq(infoGains[m_Attribute.index()], 0)) {       m_Attribute = null;      m_Distribution = new double[data.numClasses()];      Enumeration instEnum = data.enumerateInstances();      while (instEnum.hasMoreElements()) {         Instance inst = (Instance) instEnum.nextElement();        m_Distribution[(int) inst.classValue()]++;      }       Utils.normalize(m_Distribution);      m_ClassValue = Utils.maxIndex(m_Distribution);      m_ClassAttribute = data.classAttribute();    } else {       Instances[] splitData = splitData(data, m_Attribute);      m_Successors = new Id3[m_Attribute.numValues()];      for (int j = 0; j < m_Attribute.numValues(); j++) {         m_Successors[j] = new Id3();        m_Successors[j].makeTree(splitData[j]);      }     }   }   /**   * Classifies a given test instance using the decision tree.   *    * @param instance the instance to be classifiedFigure 15.1(continued)P088407-Ch015.qxd  4/30/05  10:58 AM  Page 475476CHAPTER 15|WRITING NEW LEARNING SCHEMES    if (instance.hasMissingValue()) {       throw new NoSupportForMissingValuesException("Id3: no missing values, "                                                    + "please.");    }     if (m_Attribute == null) {       return m_ClassValue;    } else {       return m_Successors[(int) instance.value(m_Attribute)].        classifyInstance(instance);    }   }   /**   * Computes class distribution for instance using decision tree.   *    * @param instance the instance for which distribution is to be computed   * @return the class distribution for the given instance   */  public double[] distributionForInstance(Instance instance)     throws NoSupportForMissingValuesException {     if (instance.hasMissingValue()) {       throw new NoSupportForMissingValuesException("Id3: no missing values, "                                                    + "please.");    }     if (m_Attribute == null) {       return m_Distribution;    } else {       return m_Successors[(int) instance.value(m_Attribute)].        distributionForInstance(instance);    }   }    * @return the classification   */  public double classifyInstance(Instance instance)     throws NoSupportForMissingValuesException {   /**   * Prints the decision tree using the private toString method from below.   * Figure 15.1(continued)P088407-Ch015.qxd  4/30/05  10:58 AM  Page 47615.1AN EXAMPLE CLASSIFIER477   * @return a textual description of the classifier   */  public String toString() {     if ((m_Distribution == null) && (m_Successors == null)) {       return "Id3: No model built yet.";    }     return "Id3\n\n" + toString(0);  }   /**   * Computes information gain for an attribute.   *    * @param data the data for which info gain is to be computed   * @param att the attribute   * @return the information gain for the given attribute and data   */  private double computeInfoGain(Instances data, Attribute att)     throws Exception {     double infoGain = computeEntropy(data);    Instances[] splitData = splitData(data, att);    for (int j = 0; j < att.numValues(); j++) {       if (splitData[j].numInstances() > 0) {         infoGain -= ((double) splitData[j].numInstances() /                      (double) data.numInstances()) *           computeEntropy(splitData[j]);      }     }     return infoGain;  }   /**   * Computes the entropy of a dataset.   *    * @param data the data for which entropy is to be computed   * @return the entropy of the data's class distribution   */  private double computeEntropy(Instances data) throws Exception {     double [] classCounts = new double[data.numClasses()];Figure 15.1(continued)P088407-Ch015.qxd  4/30/05  10:58 AM  Page 477478CHAPTER 15|WRITING NEW LEARNING SCHEMES    Enumeration instEnum = data.enumerateInstances();    while (instEnum.hasMoreElements()) {       Instance inst = (Instance) instEnum.nextElement();      classCounts[(int) inst.classValue()]++;    }     double entropy = 0;    for (int j = 0; j < data.numClasses(); j++) {       if (classCounts[j] > 0) {         entropy -= classCounts[j] * Utils.log2(classCounts[j]);      }     }     entropy /= (double) data.numInstances();    return entropy + Utils.log2(data.numInstances());  }   /**   * Splits a dataset according to the values of a nominal attribute.   *    * @param data the data which is to be split   * @param att the attribute to be used for splitting   * @return the sets of instances produced by the split   */  private Instances[] splitData(Instances data, Attribute att) {     Instances[] splitData = new Instances[att.numValues()];    for (int j = 0; j < att.numValues(); j++) {       splitData[j] = new Instances(data, data.numInstances());    }     Enumeration instEnum = data.enumerateInstances();    while (instEnum.hasMoreElements()) {       Instance inst = (Instance) instEnum.nextElement();      splitData[(int) inst.value(att)].add(inst);    }     for (int i = 0; i < splitData.length; i++) {       splitData[i].compactify();    }     return splitData;  }   /**   * Outputs a tree at a certain level. Figure 15.1(continued)P088407-Ch015.qxd  4/30/05  10:58 AM  Page 47815.1AN EXAMPLE CLASSIFIER479   *    * @param level the level at which the tree is to be printed   */  private String toString(int level) {     StringBuffer text = new StringBuffer();    if (m_Attribute == null) {       if (Instance.isMissingValue(m_ClassValue)) {         text.append(": null");      } else {         text.append(": " + m_ClassAttribute.value((int) m_ClassValue));      }     } else {       for (int j = 0; j < m_Attribute.numValues(); j++) {         text.append("\n");        for (int i = 0; i < level; i++) {           text.append("|  ");        }         text.append(m_Attribute.name() + " = " + m_Attribute.value(j));        text.append(m_Successors[j].toString(level + 1));      }     }     return text.toString();  }   /**   * Main method.   *    * @param args the options for the classifier   */  public static void main(String[] args) {     try {       System.out.println(Evaluation.evaluateModel(new Id3(), args));    } catch (Exception e) {       System.err.println(e.getMessage());    }   } } Figure 15.1(continued)P088407-Ch015.qxd  4/30/05  10:58 AM  Page 479ute is passed to the attribute()method from weka.core.Instances,which returnsthe corresponding attribute.You might wonder what happens to the array ﬁeld corresponding to the classattribute.We need not worry about this because Java automatically initializesall elements in an array ofnumbers to zero,and the information gain is alwaysgreater than or equal to zero.Ifthe maximum information gain is zero,make-Tree()creates a leaf.In that case m_Attributeis set to null,and makeTree()com-putes both the distribution ofclass probabilities and the class with greatestprobability.(The normalize()method from weka.core.Utilsnormalizes an arrayofdoubles to sum to one.)When it makes a leafwith a class value assigned to it,makeTree()stores theclass attribute in m_ClassAttribute.This is because the method that outputs thedecision tree needs to access this to print the class label.Ifan attribute with nonzero information gain is found,makeTree()splits thedataset according to the attribute’s values and recursively builds subtrees foreach ofthe new datasets.To make the split it calls the method splitData().Thiscreates as many empty datasets as there are attribute values,stores them in anarray (setting the initial capacity ofeach dataset to the number ofinstances inthe original dataset),and then iterates through all instances in the originaldataset,and allocates them to the new dataset that corresponds to the attribute’svalue.It then reduces memory requirements by compacting the Instancesobjects.Returning to makeTree(),the resulting array ofdatasets is used forbuilding subtrees.The method creates an array ofId3objects,one for eachattribute value,and calls makeTree()on each one by passing it the correspon-ding dataset.computeInfoGain()Returning to computeInfoGain(),the information gain associated with an attrib-ute and a dataset is calculated using a straightforward implementation oftheformula in Section 4.3 (page 102).First,the entropy ofthe dataset is computed.Then,splitData()is used to divide it into subsets,and computeEntropy()is calledon each one.Finally,the difference between the former entropy and theweighted sum ofthe latter ones—the information gain—is returned.Themethod computeEntropy()uses the log2()method from weka.core.Utilsto obtainthe logarithm (to base 2) ofa number.classifyInstance()Having seen how ID3 constructs a decision tree,we now examine how it usesthe tree structure to predict class values and probabilities.Every classiﬁer mustimplement the classifyInstance()method or the distributionForInstance()method (or both).The Classiﬁersuperclass contains default implementations480CHAPTER 15|WRITING NEW LEARNING SCHEMESP088407-Ch015.qxd  4/30/05  10:58 AM  Page 480for both methods.The default implementation ofclassifyInstance()calls distri-butionForInstance().Ifthe class is nominal,it predicts the class with maximumprobability,or a missing value ifall probabilities returned by distributionForIn-stance()are zero.Ifthe class is numeric,distributionForInstance()must return asingle-element array that holds the numeric prediction,and this is what classi-fyInstance()extracts and returns.Conversely,the default implementation ofdis-tributionForInstance()wraps the prediction obtained from classifyInstance()intoa single-element array.Ifthe class is nominal,distributionForInstance()assignsa probability ofone to the class predicted by classifyInstance()and a probabil-ity ofzero to the others.IfclassifyInstance()returns a missing value,then allprobabilities are set to zero.To give you a better feeling for just what thesemethods do,the weka.classiﬁers.trees.Id3class overrides them both.Let’s look ﬁrst at classifyInstance(),which predicts a class value for a giveninstance.As mentioned in the previous section,nominal class values,likenominal attribute values,are coded and stored in doublevariables,representingthe index ofthe value’s name in the attribute declaration.This is used in favorofa more elegant object-oriented approach to increase speed ofexecution.Inthe implementation ofID3,classifyInstance()ﬁrst checks whether there aremissing values in the instance to be classiﬁed;ifso,it throws an exception.Otherwise,it descends the tree recursively,guided by the instance’s attributevalues,until a leafis reached.Then it returns the class value m_ClassValuestoredat the leaf.Note that this might be a missing value,in which case the instanceis left unclassiﬁed.The method distributionForInstance()works in exactly thesame way,returning the probability distribution stored in m_Distribution.Most machine learning models,and in particular decision trees,serve as amore or less comprehensible explanation ofthe structure found in the data.Accordingly,each ofWeka’s classiﬁers,like many other Java objects,implementsa toString()method that produces a textual representation ofitselfin the formofa Stringvariable.ID3’s toString()method outputs a decision tree in roughlythe same format as J4.8 (Figure 10.5).It recursively prints the tree structure intoa Stringvariable by accessing the attribute information stored at the nodes.Toobtain each attribute’s name and values,it uses the name()and value()methodsfrom weka.core.Attribute.Empty leaves without a class value are indicated by thestring null.main()The only method in weka.classiﬁers.trees.Id3that hasn’t been described ismain(),which is called whenever the class is executed from the command line.As you can see,it’s simple:it basically just tells Weka’s Evaluationclass to eval-uate Id3with the given command-line options and prints the resulting string.The one-line expression that does this is enclosed in a try–catchstatement,15.1AN EXAMPLE CLASSIFIER481P088407-Ch015.qxd  4/30/05  10:58 AM  Page 481which catches the various exceptions that can be thrown by Weka’s routines orother Java methods.The evaluation()method in weka.classiﬁers.Evaluationinterprets the genericscheme-independent command-line options described in Section 13.3 and actsappropriately.For example,it takes the -toption,which gives the name ofthetraining ﬁle,and loads the corresponding dataset.Ifthere is no test ﬁle it per-forms a cross-validation by creating a classiﬁer object and repeatedly callingbuildClassiﬁer() andclassifyInstance() or distributionForInstance()on differentsubsets ofthe training data.Unless the user suppresses output ofthe model bysetting the corresponding command-line option,it also calls the toString()method to output the model built from the full training dataset.What happens ifthe scheme needs to interpret a speciﬁc option such as apruning parameter? This is accomplished using the OptionHandlerinterface inweka.core.A classiﬁer that implements this interface contains three methods,listOptions(),setOptions(),and getOptions(),which can be used to list all theclassiﬁer’s scheme-speciﬁc options,to set some ofthem,and to get the optionsthat are currently set.The evaluation()method in Evaluationautomatically callsthese methods ifthe classiﬁer implements the OptionHandlerinterface.Oncethe scheme-independent options have been processed,it calls setOptions()toprocess the remaining options before using buildClassiﬁer()to generate a newclassiﬁer.When it outputs the classiﬁer,it uses getOptions()to output a list ofthe options that are currently set.For a simple example ofhow to implementthese methods,look at the source code for weka.classiﬁers.rules.OneR.OptionHandlermakes it possible to set options from the command line.Toset them from within the graphical user interfaces,Weka uses the Java beansframework.All that is required are set...()and get...()methods for every param-eter used by the class.For example,the methods setPruningParameter()and get-PruningParameter()would be needed for a pruning parameter.There shouldalso be a pruningParameterTipText()method that returns a description oftheparameter for the graphical user interface.Again,see weka.classiﬁers.rules.OneRfor an example.Some classiﬁers can be incrementally updated as new training instancesarrive;they don’t have to process all the data in one batch.In Weka,incremen-tal classiﬁers implement the UpdateableClassiﬁerinterface in weka.classiﬁers.This interface declares only one method,namely,updateClassiﬁer(),which takesa single training instance as its argument.For an example ofhow to use thisinterface,look at the source code for weka.classiﬁers.lazy.IBk.Ifa classiﬁer is able to make use ofinstance weights,it should implement theWeightedInstancesHandler()interface from weka.core.Then other algorithms,such as those for boosting,can make use ofthis property.In weka.coreare many other useful interfaces for classiﬁers—for example,interfaces for classiﬁers that are randomizable,summarizable,drawable,and482CHAPTER 15|WRITING NEW LEARNING SCHEMESP088407-Ch015.qxd  4/30/05  10:58 AM  Page 482graphable.For more information on these and other interfaces,look at theJavadoc for the classes in weka.core.15.2Conventions for implementing classiﬁersThere are some conventions that you must obey when implementing classiﬁersin Weka.Ifyou do not,things will go awry.For example,Weka’s evaluationmodule might not compute the classiﬁer’s statistics properly when evaluatingit.The ﬁrst convention has already been mentioned:each time a classiﬁer’sbuildClassiﬁer()method is called,it must reset the model.The CheckClassiﬁerclass performs tests to ensure that this is the case.When buildClassiﬁer()is calledon a dataset,the same result must always be obtained,regardless ofhow oftenthe classiﬁer has previously been applied to the same or other datasets.However,buildClassiﬁer()must not reset instance variables that correspond to scheme-speciﬁc options,because these settings must persist through multiple calls ofbuildClassiﬁer().Also,calling buildClassiﬁer()must never change the input data.Two other conventions have also been mentioned.One is that when a classiﬁer can’t make a prediction,its classifyInstance()method must returnInstance.missingValue()and its distributionForInstance()method must returnprobabilities ofzero for all classes.The ID3 implementation in Figure 15.1 doesthis.Another convention is that with classiﬁers for numeric prediction,classi-fyInstance()returns the numeric value that the classiﬁer predicts.Some classi-ﬁers,however,are able to predict nominal classes and their class probabilities,as well as numeric class values—weka.classiﬁers.lazy.IBkis an example.Theseimplement the distributionForInstance()method,and ifthe class is numeric itreturns an array ofsize 1 whose only element contains the predicted numericvalue.Another convention—not absolutely essential but useful nonetheless—is thatevery classiﬁer implements a toString()method that outputs a textual descrip-tion ofitself.15.2CONVENTIONS FOR IMPLEMENTING CLASSIFIERS483P088407-Ch015.qxd  4/30/05  10:58 AM  Page 483P088407-Ch015.qxd  4/30/05  10:58 AM  Page 484ReferencesAdriaans,P.,and D.Zantige.1996.Data mining.Harlow,England:Addison-Wesley.Agrawal,R.,and R.Srikant.1994.Fast algorithms for mining association rules inlarge databases.In J.Bocca,M.Jarke,and C.Zaniolo,editors,Proceedings ofthe International Conference on Very Large Databases,Santiago,Chile.SanFrancisco:Morgan Kaufmann,pp.478–499.Agrawal,R.,T.Imielinski,and A.Swami.1993a.Database mining:A performanceperspective.IEEE Transactions on Knowledge and Data Engineering5(6):914–925.———.1993b.Mining association rules between sets ofitems in large databases.In P.Buneman and S.Jajodia,editors,Proceedings ofthe ACM SIGMOD Inter-national Conference on Management ofData,Washington,DC.New York:ACM,pp.207–216.Aha,D.1992.Tolerating noisy,irrelevant,and novel attributes in instance-based learning algorithms.International Journal ofMan-Machine Studies36(2):267–287.Almuallin,H.,and T.G.Dietterich.1991.Learning with many irrelevant features.In Proceedings ofthe Ninth National Conference on Artiﬁcial Intelligence,Anaheim,CA.Menlo Park,CA:AAAI Press,pp.547–552.———.1992.Efﬁcient algorithms for identifying relevant features.In Proceedingsofthe Ninth Canadian Conference on Artiﬁcial Intelligence,Vancouver,BC.SanFrancisco:Morgan Kaufmann,pp.38–45.Appelt,D.E.1999.Introduction to information extraction technology.Tutorial,IntJoint Confon Artiﬁcial Intelligence IJCAI’99.Morgan Kaufmann,San Mateo.Tutorial notes available at www.ai.sri.com/~appelt/ie-tutorial.485P088407-REF.qxd  4/30/05  11:24 AM  Page 485Asmis,E.1984.Epicurus’scientiﬁc method.Ithaca,NY:Cornell University Press.Atkeson,C.G.,S.A.Schaal,and A.W.Moore.1997.Locally weighted learning.AIReview11:11–71.Bay,S.D.1999.Nearest-neighbor classiﬁcation from multiple feature subsets.Intelligent Data Analysis3(3):191–209.Bay,S.D.,and M.Schwabacher.2003.Near linear time detection ofdistance-basedoutliers and applications to security.In Proceedings ofthe Workshop on DataMining for Counter Terrorism and Security,San Francisco.Society for Industrial and Applied Mathematics,Philadelphia,PA.Bayes,T.1763.An essay towards solving a problem in the doctrine ofchances.Philosophical Transactions ofthe Royal Society ofLondon53:370–418.Beck,J.R.,and E.K.Schultz.1986.The use ofROC curves in test performance eval-uation.Archives ofPathology and Laboratory Medicine110:13–20.Bergadano,F.,and D.Gunetti.1996.Inductive logic programming: From machinelearning to software engineering.Cambridge,MA:MIT Press.Berners-Lee,T.,J.Hendler,and O.Lassila.2001.The semantic web.Scientiﬁc American284(5):34–43.Berry,M.J.A.,and G.Linoff.1997.Data mining techniques for marketing,sales,andcustomer support.New York:John Wiley.Bigus,J.P.1996.Data mining with neural networks.New York:McGraw Hill.Bishop,C.M.1995.Neural networks for pattern recognition.New York:Oxford University Press.Blake,C.,E.Keogh,and C.J.Merz.1998.UCI Repository ofmachine learning databases[http://www.ics.uci.edu/~mlearn/MLRepository.html].Depart-ment ofInformation and Computer Science,University ofCalifornia,Irvine,CA.BLI (Bureau ofLabour Information).1988.Collective Bargaining Review(November).Ottawa,Ontario,Canada:Labour Canada,Bureau ofLabourInformation.Blum,A.,and T.Mitchell.1998.Combining labeled and unlabeled data with co-training.In Proceedings ofthe Eleventh Annual Conference on ComputationalLearning Theory,Madison,WI.San Francisco:Morgan Kaufmann,pp.92–100.Bouckaert,R.R.1995.Bayesian beliefnetworks: From construction to inference.PhDDissertation,Computer Science Department,University ofUtrecht,TheNetherlands.486REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 486Bouckaert,R.R.2004.Bayesian network classiﬁers in Weka.Working Paper 14/2004,Department ofComputer Science,University ofWaikato,New Zealand.Brachman,R.J.,and H.J.Levesque,editors.1985.Readings in knowledge represen-tation.San Francisco:Morgan Kaufmann.Brefeld,U.,and T.Scheffer.2004.Co-EM support vector learning.In R.Greiner and D.Schuurmans,editors,Proceedings ofthe Twenty-First International Conference on Machine Learning,Banff,Alberta,Canada.New York:ACM,pp.121–128.Breiman,L.1996a.Stacked regression.Machine Learning24(1):49–64.———.1996b.Bagging predictors.Machine Learning24(2):123–140.———.1999.Pasting small votes for classiﬁcation in large databases and online.Machine Learning36(1–2):85–103.———.2001.Random forests.Machine Learning45(1):5–32.Breiman,L.,J.H.Friedman,R.A.Olshen,and C.J.Stone.1984.Classiﬁcation andregression trees.Monterey,CA:Wadsworth.Brin,S.,R.Motwani,J.D.Ullman,and S.Tsur.1997.Dynamic itemset counting and implication rules for market basket data.ACM SIGMOD Record26(2):255–264.Brodley,C.E.,and M.A.Friedl.1996.Identifying and eliminating mislabeled train-ing instances.In Proceedings ofthe Thirteenth National Conference on Artiﬁcial Intelligence,Portland,OR.Menlo Park,CA:AAAI Press,pp.799–805.Brownstown,L.,R.Farrell,E.Kant,and N.Martin.1985.Programming expertsystems in OPS5.Reading,MA:Addison-Wesley.Buntine,W.1992.Learning classiﬁcation trees.Statistics and Computing2(2):63–73.Burges,C.J.C.1998.A tutorial on support vector machines for pattern recogni-tion.Data Mining and Knowledge Discovery2(2):121–167.Cabena,P.,P.Hadjinian,R.Stadler,J.Verhees,and A.Zanasi.1998.Discovering datamining: From concept to implementation.Upper Saddle River,NJ:PrenticeHall.Califf,M.E.,and R.J.Mooney.1999.Relational learning ofpattern-match rules forinformation extraction.In Proceedings ofthe Sixteenth National Conference onArtiﬁcial Intelligence,Orlando,FL.Menlo Park,AC:AAAI Press,pp.328–334.Cardie,C.1993.Using decision trees to improve case-based learning.In P.Utgoff,editor,Proceedings ofthe Tenth International Conference on Machine Learning,Amherst,MA.San Francisco:Morgan Kaufmann,pp.25–32.REFERENCES487P088407-REF.qxd  4/30/05  11:24 AM  Page 487Cavnar,W.B.,and J.M.Trenkle.1994.N-Gram-based text categorization.Proceed-ings ofthe Third Symposium on Document Analysis and Information Retrieval.Las Vegas,NV,UNLV Publications/Reprographics,pp.161–175.Cendrowska,J.1998.PRISM:An algorithm for inducing modular rules.Interna-tional Journal ofMan-Machine Studies27(4):349–370.Chakrabarti,S.2003.Mining the web: discovering knowledge from hypertext data.SanFrancisco,CA:Morgan Kaufmann.Cheeseman,P.,and J.Stutz.1995.Bayesian classiﬁcation (AutoClass):Theory andresults.In U.M.Fayyad,G.Piatetsky-Shapiro,P.Smyth,and R.Uthurusamy,editors,Advances in Knowledge Discovery and Data Mining.Menlo Park,CA:AAAI Press,pp.153–180.Chen,M.S.,J.Jan,and P.S.Yu.1996.Data mining:An overview from a databaseperspective.IEEE Transactions on Knowledge and Data Engineering8(6):866–883.Cherkauer,K.J.,and J.W.Shavlik.1996.Growing simpler decision trees to facili-tate knowledge discovery.In E.Simoudis,J.W.Han,and U.Fayyad,editors,Proceedings ofthe Second International Conference on Knowledge Discovery andData Mining,Portland,OR.Menlo Park,CA:AAAI Press,pp.315–318.Cleary,J.G.,and L.E.Trigg.1995.K*:An instance-based learner using an entropicdistance measure.In A.Prieditis and S.Russell,editors,Proceedings oftheTwelfth International Conference on Machine Learning,Tahoe City,CA.SanFrancisco:Morgan Kaufmann,pp.108–114.Cohen,J.1960.A coefﬁcient ofagreement for nominal scales.Educational and Psychological Measurement20:37–46.Cohen,W.W.1995.Fast effective rule induction.In A.Prieditis and S.Russell,editors,Proceedings ofthe Twelfth International Conference on Machine Learning,Tahoe City,CA.San Francisco:Morgan Kaufmann,pp.115–123.Cooper,G.F.,and E.Herskovits.1992.A Bayesian method for the induction ofprob-abilistic networks from data.Machine Learning9(4):309–347.Cortes,C.,and V.Vapnik.1995.Support vector networks.Machine Learning20(3):273–297.Cover,T.M.,and P.E.Hart.1967.Nearest-neighbor pattern classiﬁcation.IEEETransactions on Information TheoryIT-13:21–27.Cristianini,N.,and J.Shawe-Taylor.2000.An introduction to support vector machinesand other kernel-based learning methods.Cambridge,UK:Cambridge University Press.488REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 488Cypher,A.,editor.1993.Watch what I do: Programming by demonstration.Cambridge,MA:MIT Press.Dasgupta,S.2002.Performance guarantees for hierarchical clustering.In J.Kivinenand R.H.Sloan,editors,Proceedings ofthe Fifteenth Annual Conference onComputational Learning Theory,Sydney,Australia.Berlin:Springer-Verlag,pp.351–363.Datta,S.,H.Kargupta,and K.Sivakumar.2003.Homeland defense,privacy-sensi-tive data mining,and random value distortion.In Proceedings ofthe Workshopon Data Mining for Counter Terrorism and Security,San Francisco.Society forInternational and Applied Mathematics,Philadelphia,PA.Demiroz,G.,and A.Guvenir.1997.Classiﬁcation by voting feature intervals.In M.van Someren and G.Widmer,editors,Proceedings ofthe Ninth European Conference on Machine Learning,Prague,Czech Republic.Berlin:Springer-Verlag,pp.85–92.Devroye,L.,L.Györﬁ,and G.Lugosi.1996.A probabilistic theory ofpattern recog-nition.New York:Springer-Verlag.Dhar,V.,and R.Stein.1997.Seven methods for transforming corporate data into busi-ness intelligence.Upper Saddle River,NJ:Prentice Hall.Diederich,J.,J.Kindermann,E.Leopold,and G.Paass.2003.Authorship attribu-tion with support vector machines.Applied Intelligence19(1):109–123.Dietterich,T.G.2000.An experimental comparison ofthree methods for con-structing ensembles ofdecision trees:Bagging,boosting,and randomization.Machine Learning40(2):139–158.Dietterich,T.G.,and G.Bakiri.1995.Solving multiclass learning problems via error-correcting output codes.Journal Artiﬁcial Intelligence Research2:263–286.Domingos,P.1997.Knowledge acquisition from examples via multiple models.InD.H.Fisher Jr.,editor,Proceedings ofthe Fourteenth International Conferenceon Machine Learning,Nashville,TN.San Francisco:Morgan Kaufmann,pp.98–106.———.1999.MetaCost:A general method for making classiﬁers cost sensitive.InU.M.Fayyad,S.Chaudhuri,and D.Madigan,editors,Proceedings ofthe FifthInternational Conference on Knowledge Discovery and Data Mining,San Diego,CA.New York:ACM,pp.155–164.Dougherty,J.,R.Kohavi,and M.Sahami.1995.Supervised and unsupervised dis-cretization ofcontinuous features.In A.Prieditis and S.Russell,editors,Proceedings ofthe Twelfth International Conference on Machine Learning,TahoeCity,CA.San Francisco:Morgan Kaufmann,pp.194–202.REFERENCES489P088407-REF.qxd  4/30/05  11:24 AM  Page 489Drucker,H.1997.Improving regressors using boosting techniques.In D.H.Fisher,editor,Proceedings ofthe Fourteenth International Conference on MachineLearning,Nashville,TN.San Francisco:Morgan Kaufmann,pp.107–115.Drummond,C.,and R.C.Holte.2000.Explicitly representing expected cost:Analternative to ROC representation.In R.Ramakrishnan,S.Stolfo,R.Bayardo,and I.Parsa,editors,Proceedings ofthe Sixth International Conference onKnowledge Discovery and Data Mining,Boston,MA.New York:ACM,pp.198–207.Duda,R.O.,and P.E.Hart.1973.Pattern classiﬁcation and scene analysis.New York:John Wiley.Duda,R.O.,P.E.Hart,and D.G.Stork.2001.Pattern Classiﬁcation,second edition.New York:John Wiley.Dumais,S.T.,J.Platt,D.Heckerman,and M.Sahami.1998.Inductive learning algo-rithms and representations for text categorization.In Proceedings ofthe ACMSeventh International Conference on Information and Knowledge Management,Bethesda,MD.New York:ACM,pp.148–155.Efron,B.,and R.Tibshirani.1993.An introduction to the bootstrap.London:Chapman and Hall.Egan,J.P.1975.Signal detection theory and ROC analysis.Series in Cognition andPerception.New York:Academic Press.Fayyad,U.M.,and K.B.Irani.1993.Multi-interval discretization ofcontinuous-valued attributes for classiﬁcation learning.In Proceedings ofthe ThirteenthInternational Joint Conference on Artiﬁcial Intelligence,Chambery,France.SanFrancisco:Morgan Kaufmann,pp.1022–1027.Fayyad,U.M.,and P.Smyth.1995.From massive datasets to science catalogs:Applications and challenges.In Proceedings ofthe Workshop on MassiveDatasets.Washington,DC:NRC,Committee on Applied and Theoretical Statistics.Fayyad,U.M.,G.Piatetsky-Shapiro,P.Smyth,and R.Uthurusamy,editors.1996.Advances in knowledge discovery and data mining.Menlo Park,CA:AAAIPress/MIT Press.Fisher,D.1987.Knowledge acquisition via incremental conceptual clustering.Machine Learning2(2):139–172.Fisher,R.A.1936.The use ofmultiple measurements in taxonomic problems.Annual Eugenics7(part II):179–188.Reprinted in Contributions to Mathematical Statistics,1950.New York:John Wiley.490REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 490Fix,E.,and J.L.Hodges Jr.1951.Discriminatory analysis;nonparametric discrim-ination:Consistency properties.Technical Report 21-49-004(4),USAF SchoolofAviation Medicine,Randolph Field,Texas.Flach,P.A.,and N.Lachiche.1999.Conﬁrmation-guided discovery ofﬁrst-orderrules with Tertius.Machine Learning42:61–95.Fletcher,R.1987.Practical methods ofoptimization,second edition.New York:JohnWiley.Fradkin,D.,and D.Madigan.2003.Experiments with random projections formachine learning.In L.Getoor,T.E.Senator,P.Domingos,and C.Faloutsos,editors,Proceedings ofthe Ninth International Conference on Knowledge Discovery and Data Mining,Washington,DC.New York:ACM,pp.517–522.Frank E.2000.Pruning decision trees and lists.PhD Dissertation,Department ofComputer Science,University ofWaikato,New Zealand.Frank,E.,and M.Hall.2001.A simple approach to ordinal classiﬁcation.In L.deRaedt and P.A.Flach,editors,Proceedings ofthe Twelfth European Conferenceon Machine Learning,Freiburg,Germany.Berlin:Springer-Verlag,pp.145–156.Frank,E.,and I.H.Witten.1998.Generating accurate rule sets without global opti-mization.In J.Shavlik,editor,Proceedings ofthe Fifteenth International Conference on Machine Learning,Madison,WI.San Francisco:Morgan Kaufmann,pp.144–151.———.1999.Making better use ofglobal discretization.In I.Bratko and S.Dze-roski,editors,Proceedings ofthe Sixteenth International Conference on MachineLearning,Bled,Slovenia.San Francisco:Morgan Kaufmann,pp.115–123.Frank,E.,M.Hall,and B.Pfahringer.2003.Locally weighted Naïve Bayes.In U.Kjærulffand C.Meek,editors,Proceedings ofthe Nineteenth Conference onUncertainty in Artiﬁcial Intelligence,Acapulco,Mexico.San Francisco:MorganKaufmann,pp.249–256.Frank,E.,G.Holmes,R.Kirkby,and M.Hall.2002.Racing committees for largedatasets.In S.Lange and K.Satoh,and C.H.Smith,editors,Proceedings oftheFifth International Conference on Discovery Science,Lübeck,Germany.Berlin:Springer-Verlag,pp.153–164.Frank,E.,G.W.Paynter,I.H.Witten,C.Gutwin,and C.G.Nevill-Manning.1999.Domain-speciﬁc key phrase extraction.In Proceedings ofthe Sixteenth International Joint Conference on Artiﬁcial Intelligence,Stockholm,Sweden.San Francisco:Morgan Kaufmann,pp.668–673.Freitag,D.2002.Machine learning for information extraction in informal domains.Machine Learning 39(2/3):169–202.REFERENCES491P088407-REF.qxd  4/30/05  11:24 AM  Page 491Freund,Y.,and L.Mason.1999.The alternating decision-tree learning algorithm.In I.Bratko and S.Dzeroski,editors,Proceedings ofthe Sixteenth InternationalConference on Machine Learning,Bled,Slovenia.San Francisco:Morgan Kaufmann,pp.124–133.Freund,Y.,and R.E.Schapire.1996.Experiments with a new boosting algorithm.In L.Saitta,editor,Proceedings ofthe Thirteenth International Conference onMachine Learning,Bari,Italy.San Francisco:Morgan Kaufmann,pp.148–156.———.1999.Large margin classiﬁcation using the perceptron algorithm.MachineLearning37(3):277–296.Friedman,J.H.1996.Another approach to polychotomous classiﬁcation.TechnicalReport,Department ofStatistics,Stanford University,Stanford,CA.———.2001.Greedy function approximation:A gradient boosting machine.Annals ofStatistics29(5):1189–1232.Friedman,J.H.,J.L.Bentley,and R.A.Finkel.1977.An algorithm for ﬁnding bestmatches in logarithmic expected time.ACM Transactions on MathematicalSoftware3(3):209–266.Friedman,J.H.,T.Hastie,and R.Tibshirani.2000.Additive logistic regression:Astatistical view ofboosting.Annals ofStatistics28(2):337–374.Friedman,N.,D.Geiger,and M.Goldszmidt.1997.Bayesian network classiﬁers.Machine Learning29(2):131–163.Fulton,T.,S.Kasif,and S.Salzberg.1995.Efﬁcient algorithms for ﬁnding multiwaysplits for decision trees.In A.Prieditis and S.Russell,editors,Proceedings ofthe Twelfth International Conference on Machine Learning,Tahoe City,CA.SanFrancisco:Morgan Kaufmann,pp.244–251.Fürnkrantz,J.2002.Round robin classiﬁcation.Journal ofMachine LearningResearch2:721–747.Fürnkrantz,J.,and P.A.Flach.2005.ROC ’n’rule learning:Towards a better under-standing ofcovering algorithms.Machine Learning 58(1):39–77.Fürnkrantz,J.,and G.Widmer.1994.Incremental reduced-error pruning.In H.Hirsh and W.Cohen,editors,Proceedings ofthe Eleventh International Conference on Machine Learning,New Brunswick,NJ.San Francisco:MorganKaufmann,pp.70–77.Gaines,B.R.,and P.Compton.1995.Induction ofripple-down rules applied tomodeling large databases.Journal ofIntelligent Information Systems5(3):211–228.Genesereth,M.R.,and N.J.Nilsson.1987.Logical foundations ofartiﬁcial intelli-gence.San Francisco:Morgan Kaufmann.492REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 492Gennari,J.H.,P.Langley,and D.Fisher.1990.Models ofincremental concept for-mation.Artiﬁcial Intelligence40:11–61.Ghani,R.2002.Combining labeled and unlabeled data for multiclass text catego-rization.In C.Sammut and A.Hoffmann,editors,Proceedings ofthe Nine-teenth International Conference on Machine Learning,Sydney,Australia.SanFrancisco:Morgan Kaufmann,pp.187–194.Gilad-Bachrach,R.,A.Navot,and N.Tishby.2004.Margin based feature selection:Theory and algorithms.In R.Greiner and D.Schuurmans,editors,Proceed-ings ofthe Twenty-First International Conference on Machine Learning,Banff,Alberta,Canada.New York:ACM,pp.337–344.Giraud-Carrier,C.1996.FLARE:Induction with prior knowledge.In J.Nealon andJ.Hunt,editors,Research and Development in Expert Systems XIII.Cambridge,UK:SGES Publications,pp.11–24.Gluck,M.,and J.Corter.1985.Information,uncertainty,and the utility ofcate-gories.In Proceedings ofthe Annual Conference ofthe Cognitive Science Society,Irvine,CA.Hillsdale,NJ:Lawrence Erlbaum,pp.283–287.Goldberg,D.E.1989.Genetic algorithms in search,optimization,and machine learn-ing.Reading,MA:Addison-Wesley.Good,P.1994.Permutation tests: A practical guide to resampling methods for testinghypotheses.Springer-Verlag,New York,NY.Grossman,D.,and P.Domingos.2004.Learning Bayesian network classiﬁers bymaximizing conditional likelihood.In R.Greiner and D.Schuurmans,editors,Proceedings ofthe Twenty-First International Conference on Machine Learning,Banff,Alberta,Canada.New York:ACM,pp.361–368.Groth,R.1998.Data mining: A hands-on approach for business professionals.UpperSaddle River,NJ:Prentice Hall.Guo,Y.,and R.Greiner.2004.Discriminative model selection for beliefnet struc-tures.Department ofComputing Science,TR04-22,University ofAlberta,Canada.Guyon,I.,J.Weston,S.Barnhill,and V.Vapnik.2002.Gene selection for cancer classiﬁcation using support vector machines.Machine Learning46(1–3):389–422.Hall,M.2000.Correlation-based feature selection for discrete and numeric classmachine learning.In P.Langley,editor,Proceedings ofthe Seventeenth Inter-national Conference on Machine Learning,Stanford,CA.San Francisco:Morgan Kaufmann,pp.359–366.REFERENCES493P088407-REF.qxd  4/30/05  11:24 AM  Page 493Hall,M.,G.Holmes,and E.Frank.1999.Generating rule sets from model trees.InN.Y.Foo,editor,Proceedings ofthe Twelfth Australian Joint Conference on Artiﬁcial Intelligence,Sydney,Australia.Berlin:Springer-Verlag,pp.1–12.Han,J.,and M.Kamber.2001.Data mining: Concepts and techniques.San Francisco:Morgan Kaufmann.Hand,D.J.,H.Mannila,and P.Smyth.2001.Principles ofdata mining.Cambridge,MA:MIT Press.Hartigan,J.A.1975.Clustering algorithms.New York:John Wiley.Hastie,T.,and R.Tibshirani.1998.Classiﬁcation by pairwise coupling.Annals ofStatistics26(2):451–471.Hastie,T.,R.Tibshirani,and J.Friedman.2001.The elements ofstatistical learning.New York:Springer-Verlag.Heckerman,D.,D.Geiger,and D.M.Chickering.1995.Learning Bayesian networks:The combination ofknowledge and statistical data.Machine Learning20(3):197–243.Hochbaum,D.S.,and D.B.Shmoys.1985.A best possible heuristic for the k-centerproblem.Mathematics ofOperations Research10(2):180–184.Holmes,G.,and C.G.Nevill-Manning.1995.Feature selection via the discovery ofsimple classiﬁcation rules.In G.E.Lasker and X.Liu,editors,Proceedings ofthe International Symposium on Intelligent Data Analysis.Baden-Baden,Germany:International Institute for Advanced Studies in Systems Researchand Cybernetics,pp.75–79.Holmes,G.,B.Pfahringer,R.Kirkby,E.Frank,and M.Hall.2002.Multiclass alter-nating decision trees.In T.Elomaa,H.Mannila,and H.Toivonen,editors,Proceedings ofthe Thirteenth European Conference on Machine Learning,Helsinki,Finland.Berlin:Springer-Verlag,pp.161–172.Holte,R.C.1993.Very simple classiﬁcation rules perform well on most commonlyused datasets.Machine Learning11:63–91.Huffman,S.B.1996.Learning information extraction patterns from examples.InS.Wertmer,E.Riloff,and G.Scheler,editors,Connectionist,statistical,and sym-bolic approaches to learning for natural language processing,Springer Verlag,Berlin,pp.246–260.Jabbour,K.,J.F.V.Riveros,D.Landsbergen,and W.Meyer.1988.ALFA:Automatedload forecasting assistant.IEEE Transactions on Power Systems3(3):908–914.John,G.H.1995.Robust decision trees:Removing outliers from databases.In U.M.Fayyad and R.Uthurusamy,editors,Proceedings ofthe First International494REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 494Conference on Knowledge Discovery and Data Mining.Montreal,Canada.Menlo Park,CA:AAAI Press,pp.174–179.———.1997.Enhancements to the data mining process.PhD Dissertation,Com-puter Science Department,Stanford University,Stanford,CA.John,G.H.,and P.Langley.1995.Estimating continuous distributions in Bayesianclassiﬁers.In P.Besnard and S.Hanks,editors,Proceedings ofthe Eleventh Conference on Uncertainty in Artiﬁcial Intelligence,Montreal,Canada.SanFrancisco:Morgan Kaufmann,pp.338–345.John,G.H.,R.Kohavi,and P.Pﬂeger.1994.Irrelevant features and the subset selection problem.In H.Hirsh and W.Cohen,editors,Proceedings oftheEleventh International Conference on Machine Learning,New Brunswick,NJ.San Francisco:Morgan Kaufmann,pp.121–129.Johns,M.V.1961.An empirical Bayes approach to nonparametric two-way classi-ﬁcation.In H.Solomon,editor,Studies in item analysis and prediction.PaloAlto,CA:Stanford University Press.Kass,R.,and L.Wasserman.1995.A reference Bayesian test for nested hypothesesand its relationship to the Schwarz criterion.Journal ofthe American Statistical Association90:928–934.Keerthi,S.S.,S.K.Shevade,C.Bhattacharyya,and K.R.K.Murthy.2001.Improve-ments to Platt’s SMO algorithm for SVM classiﬁer design.Neural Computa-tion13(3):637–649.Kerber,R.1992.Chimerge:Discretization ofnumeric attributes.In W.Swartout,editor,Proceedings ofthe Tenth National Conference on Artiﬁcial Intelligence,San Jose,CA.Menlo Park,CA:AAAI Press,pp.123–128.Kibler,D.,and D.W.Aha.1987.Learning representative exemplars ofconcepts:An initial case study.In P.Langley,editor,Proceedings ofthe Fourth Machine Learning Workshop,Irvine,CA.San Francisco:Morgan Kaufmann,pp.24–30.Kimball,R.1996.The data warehouse toolkit.New York:John Wiley.Kira,K.,and L.Rendell.1992.A practical approach to feature selection.In D.Sleeman and P.Edwards,editors,Proceedings ofthe Ninth International Workshop on Machine Learning,Aberdeen,Scotland.San Francisco:MorganKaufmann,pp.249–258.Kittler,J.1978.Feature set search algorithms.In C.H.Chen,editor,Pattern recog-nition and signal processing.The Netherlands:Sijthoffan Noordhoff.Koestler,A.1964.The act ofcreation.London:Hutchinson.REFERENCES495P088407-REF.qxd  4/30/05  11:24 AM  Page 495Kohavi,R.1995a.A study ofcross-validation and bootstrap for accuracy estimationand model selection.In Proceedings ofthe Fourteenth International Joint Conference on Artiﬁcial Intelligence,Montreal,Canada.San Francisco:MorganKaufmann,pp.1137–1143.———.1995b.The power ofdecision tables.In N.Lavrac and S.Wrobel,editors,Proceedings ofthe Eighth European Conference on Machine Learning,Iráklion,Crete,Greece.Berlin:Springer-Verlag,pp.174–189.———.1996.Scaling up the accuracy ofNaïve Bayes classiﬁers:A decision treehybrid.In E.Simoudis,J.W.Han,and U.Fayyad,editors,Proceedings oftheSecond International Conference on Knowledge Discovery and Data Mining,Portland,OR.Menlo Park,CA:AAAI Press,pp.202–207.Kohavi,R.,and G.H.John.1997.Wrappers for feature subset selection.ArtiﬁcialIntelligence97(1–2):273–324.Kohavi,R.,and C.Kunz.1997.Option decision trees with majority votes.In D.Fisher,editor,Proceedings ofthe Fourteenth International Conference onMachine Learning,Nashville,TN.San Francisco:Morgan Kaufmann,pp.161–191.Kohavi,R.,and F.Provost,editors.1998.Machine learning:Special issue on appli-cations ofmachine learning and the knowledge discovery process.MachineLearning30(2/3):127–274.Kohavi,R.,and M.Sahami.1996.Error-based and entropy-based discretization ofcontinuous features.In E.Simoudis,J.W.Han,and U.Fayyad,editors,Proceedings ofthe Second International Conference on Knowledge Discovery andData Mining,Portland,OR.Menlo Park,CA:AAAI Press,pp.114–119.Komarek,P.,and A.Moore.2000.A dynamic adaptation ofAD trees for efﬁcientmachine learning on large data sets.In P.Langley,editor,Proceedings oftheSeventeenth International Conference on Machine Learning,Stanford,CA.SanFrancisco:Morgan Kaufmann,pp.495–502.Kononenko,I.1995.On biases in estimating multivalued attributes.In Proceedingsofthe Fourteenth International Joint Conference on Artiﬁcial Intelligence,Montreal,Canada.San Francisco:Morgan Kaufmann,pp.1034–1040.Koppel,M.,and J.Schler.2004.Authorship veriﬁcation as a one-class classiﬁcationproblem.In R.Greiner and D.Schuurmans,editors,Proceedings ofthe Twenty-First International Conference on Machine Learning,Banff,Alberta,Canada.New York:ACM,pp.489–495.Kubat,M.,R.C.Holte,and S.Matwin.1998.Machine learning for the detection ofoil spills in satellite radar images.Machine Learning30:195–215.496REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 496Kushmerick,N.,D.S.Weld,and R.Doorenbos.1997.Wrapper induction for information extraction.In Proceedings ofthe Fifteenth International Joint Conference on Artiﬁcial Intelligence,Nagoya,Japan.San Francisco:MorganKaufmann,pp.729–735.Landwehr,N.,M.Hall,and E.Frank.2003.Logistic model trees.In N.Lavrac,D.Gamberger,L.Todorovski,and H.Blockeel,editors,Proceedings ofthe Four-teenth European Conference on Machine Learning,Cavtat-Dubrovnik,Croatia.Berlin:Springer-Verlag.pp.241–252.Langley,P.1996.Elements ofmachine learning.San Francisco:Morgan Kaufmann.Langley,P.,and S.Sage.1994.Induction ofselective Bayesian classiﬁers.In R.L.de Mantaras and D.Poole,editors,Proceedings ofthe Tenth Conference onUncertainty in Artiﬁcial Intelligence,Seattle,WA.San Francisco:Morgan Kaufmann,pp.399–406.Langley,P.,and H.A.Simon.1995.Applications ofmachine learning and ruleinduction.Communications ofthe ACM38(11):55–64.Langley,P.,W.Iba,and K.Thompson.1992.An analysis ofBayesian classiﬁers.InW.Swartout,editor,Proceedings ofthe Tenth National Conference on ArtiﬁcialIntelligence,San Jose,CA.Menlo Park,CA:AAAI Press,pp.223–228.Lawson,C.L.,and R.J.Hanson.1995.Solving least-squares problems.Philadelphia:SIAM Publications.le Cessie,S.,and J.C.van Houwelingen.1992.Ridge estimators in logistic regres-sion.Applied Statistics41(1):191–201.Li,M.,and P.M.B.Vitanyi.1992.Inductive reasoning and Kolmogorov complex-ity.Journal Computer and System Sciences44:343–384.Lieberman,H.,editor.2001.Your wish is my command: Programming by example.San Francisco:Morgan Kaufmann.Littlestone,N.1988.Learning quickly when irrelevant attributes abound:A newlinear-threshold algorithm.Machine Learning2(4):285–318.———.1989.Mistake bounds and logarithmic linear-threshold learning algorithms.PhD Dissertation,University ofCalifornia,Santa Cruz,CA.Liu,H.,and R.Setiono.1996.A probabilistic approach to feature selection:A ﬁltersolution.In L.Saitta,editor,Proceedings ofthe Thirteenth International Conference on Machine Learning,Bari,Italy.San Francisco:Morgan Kaufmann,pp.319–327.———.1997.Feature selection via discretization.IEEE Transactions on Knowledgeand Data Engineering9(4):642–645.REFERENCES497P088407-REF.qxd  4/30/05  11:24 AM  Page 497Mann,T.1993.Library research models: A guide to classiﬁcation,cataloging,and com-puters.New York:Oxford University Press.Marill,T.,and D.M.Green.1963.On the effectiveness ofreceptors in recognitionsystems.IEEE Transactions on Information Theory9(11):11–17.Martin,B.1995.Instance-based learning:Nearest neighbour with generalisation.MSc Thesis,Department ofComputer Science,University ofWaikato,NewZealand.McCallum,A.,and K.Nigam.1998.A comparison ofevent models for Naïve Bayes text classiﬁcation.In Proceedings ofthe AAAI-98 Workshop on Learning for Text Categorization,Madison,WI.Menlo Park,CA:AAAI Press,pp.41–48.Mehta,M.,R.Agrawal,and J.Rissanen.1996.SLIQ:A fast scalable classiﬁer for datamining.In Apers,P.,M.Bouzeghoub,and G.Gardarin,Proceedings ofthe FifthInternational Conference on Extending Database Technology,Avignon,France.New York:Springer-Verlag.Melville,P.,and R.J.Mooney.2005.Creating diversity in ensembles using artiﬁcialdata.Information Fusion6(1):99–111.Michalski,R.S.,and R.L.Chilausky.1980.Learning by being told and learning fromexamples:An experimental comparison ofthe two methods ofknowledgeacquisition in the context ofdeveloping an expert system for soybean diseasediagnosis.International Journal ofPolicy Analysis and Information Systems4(2).Michie,D.1989.Problems ofcomputer-aided concept formation.In J.R.Quinlan,editor,Applications ofexpert systems,Vol.2.Wokingham,England:Addison-Wesley,pp.310–333.Minsky,M.,and S.Papert.1969.Perceptrons.Cambridge,MA:MIT Press.Mitchell,T.M.1997.Machine learning.New York:McGraw Hill.Mitchell,T.M.,R.Caruana,D.Freitag,J.McDermott,and D.Zabowski.1994.Experience with a learning personal assistant.Communications ofthe ACM37(7):81–91.Moore,A.W.1991.Efﬁcient memory-based learning for robot control.PhD Dissertation,Computer Laboratory,University ofCambridge,UK.———.2000.The anchors hierarchy:Using the triangle inequality to survive high-dimensional data.In C.Boutilier and M.Goldszmidt,editors,Proceedings ofthe Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence,Stanford,CA.San Francisco:Morgan Kaufmann,pp.397–405.498REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 498Moore,A.W.,and M.S.Lee.1994.Efﬁcient algorithms for minimizing cross vali-dation error.In W.W.Cohen and H.Hirsh,editors,Proceedings ofthe EleventhInternational Conference on Machine Learning,New Brunswick,NJ.San Francisco:Morgan Kaufmann,pp.190–198.———.1998.Cached sufﬁcient statistics for efﬁcient machine learning with largedatasets.Journal Artiﬁcial Intelligence Research8:67–91.Moore,A.W.,and D.Pelleg.2000.X-means:Extending k-means with efﬁcient estimation ofthe number ofclusters.In P.Langley,editor,Proceedings oftheSeventeenth International Conference on Machine Learning,Stanford,CA.SanFrancisco:Morgan Kaufmann,pp.727–734.Nadeau,C.,and Y.Bengio.2003.Inference for the generalization error.MachineLearning52(3):239–281.Nahm,U.Y.,and R.J.Mooney.2000.Using information extraction to aid the dis-covery ofprediction rules from texts.Proceedings ofthe Workshop on TextMining at the Sixth International Conference on Knowledge Discovery and DataMining,Boston,MA,pp.51–58.Nie,N.H.,C.H.Hull,J.G.Jenkins,K.Steinbrenner,and D.H.Bent.1970.Statis-tical package for the social sciences.New York:McGraw Hill.Nigam,K.,and R.Ghani.2000.Analyzing the effectiveness and applicability ofco-training.Proceedings ofthe Ninth International Conference on Information andKnowledge Management,McLean,VA.New York:ACM,pp.86–93.Nigam,K.,A.K.McCallum,S.Thrun,and T.M.Mitchell.2000.Text classiﬁcation fromlabeled and unlabeled documents using EM.Machine Learning39(2/3):103–134.Nilsson,N.J.1965.Learning machines.New York:McGraw Hill.Omohundro,S.M.1987.Efﬁcient algorithms with neural network behavior.JournalofComplex Systems1(2):273–347.Paynter,G.W.2000.Automating iterative tasks with programming by demonstration.PhD Dissertation,Department ofComputer Science,University ofWaikato,New Zealand.Piatetsky-Shapiro,G.,and W.J.Frawley,editors.1991.Knowledge discovery in data-bases.Menlo Park,CA:AAAI Press/MIT Press.Platt,J.1998.Fast training ofsupport vector machines using sequential minimaloptimization.In B.Schölkopf,C.Burges,and A.Smola,editors,Advances inkernel methods: Support vector learning.Cambridge,MA:MIT Press.Provost,F.,and T.Fawcett.1997.Analysis and visualization ofclassiﬁer perform-ance:Comparison under imprecise class and cost distributions.In D.REFERENCES499P088407-REF.qxd  4/30/05  11:24 AM  Page 499Heckerman,H.Mannila,D.Pregibon,and R.Uthurusamy,editors,Proceedings ofthe Third International Conference on Knowledge Discovery andData Mining,Huntington Beach,CA.Menlo Park,CA:AAAI Press.Pyle,D.1999.Data preparation for data mining.San Francisco:Morgan Kaufmann.Quinlan,J.R.1986.Induction ofdecision trees.Machine Learning1(1):81–106.———.1992.Learning with continuous classes.In N.Adams and L.Sterling,editors,Proceedings ofthe Fifth Australian Joint Conference on Artiﬁcial Intel-ligence,Hobart,Tasmania.Singapore:World Scientiﬁc,pp.343–348.———.1993.C4.5: Programs for machine learning.San Francisco:Morgan Kaufmann.Rennie,J.D.M.,L.Shih,J.Teevan,and D.R.Karger.2003.Tackling the poor assump-tions ofNaïve Bayes text classiﬁers.In T.Fawcett and N.Mishra,editors,Proceedings ofthe Twentieth International Conference on Machine Learning,Washington,DC.Menlo Park,CA:AAAI Press,pp.616–623.Ricci,F.,and D.W.Aha.1998.Error-correcting output codes for local learners.InC.Nedellec and C.Rouveird,editors,Proceedings ofthe European Conference onMachine Learning,Chemnitz,Germany.Berlin:Springer-Verlag,pp.280–291.Richards,D.,and P.Compton.1998.Taking up the situated cognition challenge with ripple-down rules.International Journal ofHuman-Computer Studies49(6):895–926.Ripley,B.D.1996.Pattern recognition and neural networks.Cambridge,UK:Cambridge University Press.Rissanen,J.1985.The minimum description length principle.In S.Kotz and N.L.Johnson,editors,Encyclopedia ofStatistical Sciences,Vol.5.New York:JohnWiley,pp.523–527.Rousseeuw,P.J.,and A.M.Leroy.1987.Robust regression and outlier detection.NewYork:John Wiley.Sahami,M.,S.Dumais,D.Heckerman,and E.Horvitz.1998.A Bayesian approachto ﬁltering junk email.In Proceedings ofthe AAAI-98 Workshop on Learningfor Text Categorization,Madison,WI.Menlo Park,CA:AAAI Press,pp.55–62.Saitta,L.,and F.Neri.1998.Learning in the “real world.”Machine Learning30(2/3):133–163.Salzberg,S.1991.A nearest hyperrectangle learning method.Machine Learning6(3):251–276.Schapire,R.E.,Y.Freund,P.Bartlett,and W.S.Lee.1997.Boosting the margin:Anew explanation for the effectiveness ofvoting methods.In D.H.Fisher,500REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 500editor,Proceedings ofthe Fourteenth International Conference on MachineLearning,Nashville,TN.San Francisco:Morgan Kaufmann,pp.322–330.Scheffer,T.2001.Finding association rules that trade support optimally against con-ﬁdence.In L.de Raedt and A.Siebes,editors,Proceedings ofthe Fifth EuropeanConference on Principles ofData Mining and Knowledge Discovery,Freiburg,Germany.Berlin:Springer-Verlag,pp.424–435.Schölkopf,B.,and A.J.Smola.2002.Learning with kernels: Support vector machines,regularization,optimization,and beyond.Cambridge,MA:MIT Press.Schölkopf,B.,P.Bartlett,A.J.Smola,and R.Williamson.1999.Shrinking the tube:A new support vector regression algorithm.Advances in Neural InformationProcessing Systems,Vol.11.Cambridge,MA:MIT Press,pp.330–336.Sebastiani,F.2002.Machine learning in automated text categorization.ACM Computing Surveys 34(1):1–47.Seeger,M.2001.Learning with labeled and unlabeled data.Technical Report,Institute for Adaptive and Neural Computation,University ofEdinburgh,UK.Seewald,A.K.2002.How to make stacking better and faster while also taking care ofan unknown weakness.Proceedings ofthe Nineteenth InternationalConference on Machine Learning,Sydney,Australia.San Francisco:MorganKaufmann,pp.54–561.Seewald,A.K.,and J.Fürnkranz.2001.An evaluation ofgrading classiﬁers.In F.Hoffmann,D.J.Hand,N.M.Adams,D.H.Fisher,and G.Guimarães,editors,Proceedings ofthe Fourth International Conference on Advances in IntelligentData Analysis,Cascais,Portugal.Berlin:Springer-Verlag,pp.115–124.Shafer,R.,R.Agrawal,and M.Metha.1996.SPRINT:A scalable parallel classiﬁer fordata mining.In T.M.Vijayaraman,A.P.Buchmann,C.Mohan,and N.L.Sarda,editors,Proceedings ofthe Second International Conference on Very Large Databases,Mumbai (Bombay),India.San Francisco:Morgan Kaufmann,pp.544–555.Shawe-Taylor,J.,and N Cristianini.2004.Kernel methods for pattern analysis.Cambridge,UK:Cambridge University Press.Smola,A.J.,and B.Schölkopf.2004.A tutorial on support vector regression.Statistics and Computing14(3):199–222.Soderland,S.,D.Fisher,J.Aseltine,and W.Lehnert.1995.Crystal:inducing a con-ceptual dictionary.Proceedings ofthe Fourteenth International Joint Conferenceon Artiﬁcial Intelligence,Montreal,Canada.Menlo Park,CA:AAAI Press,pp.1314–1319Stevens,S.S.1946.On the theory ofscales ofmeasurement.Science103:677–680.REFERENCES501P088407-REF.qxd  4/30/05  11:24 AM  Page 501Stone,P.,and M.Veloso.2000.Multiagent systems:A survey from a machine learn-ing perspective.Autonomous Robots8(3):345–383.Swets,J.1988.Measuring the accuracy ofdiagnostic systems.Science240:1285–1293.Ting,K.M.2002.An instance-weighting method to induce cost-sensitive trees.IEEETransactions on Knowledge and Data Engineering14(3):659–665.Ting,K.M.,and I.H.Witten.1997a.Stacked generalization:When does it work? In Proceedings ofthe Fifteenth International Joint Conference on Artiﬁcial Intelligence,Nagoya,Japan.San Francisco:Morgan Kaufmann,pp.866–871.———.1997b.Stacking bagged and dagged models.In D.H.Fisher,editor,Proceedings ofthe Fourteenth International Conference on Machine Learning,Nashville,TN.San Francisco:Morgan Kaufmann,pp.367–375.Turney,P.D.1999.Learning to extract key phrases from text.Technical Report ERB-1057,Institute for Information Technology,National Research Council ofCanada,Ottawa,Canada.U.S.House ofRepresentatives Subcommittee on Aviation.2002.Hearing on avia-tion security with a focus on passenger proﬁling,February 27,2002.[http://www.house.gov/transportation/aviation/02-27-02/02-27-02memo.html].Vafaie,H.,and K.DeJong.1992.Genetic algorithms as a tool for feature selectionin machine learning.In Proceedings ofthe International Conference on Toolswith Artiﬁcial Intelligence.Arlington,VA:IEEE Computer Society Press,pp.200–203.van Rijsbergen,C.A.1979.Information retrieval.London:Butterworths.Vapnik,V.1999.The nature ofstatistical learning theory,second edition.New York:Springer-Verlag.Wang,Y.,and I.H.Witten.1997.Induction ofmodel trees for predicting continu-ous classes.In M.van Someren and G.Widmer,editors,Proceedings ofthePoster Papers ofthe European Conference on Machine Learning.Prague:Uni-versity ofEconomics,Faculty ofInformatics and Statistics,pp.128–137.———.2002.Modeling for optimal probability prediction.In C.Sammut and A.Hoffmann,editors,Proceedings ofthe Nineteenth International Conference onMachine Learning,Sydney,Australia.San Francisco:Morgan Kaufmann,pp.650–657.Webb,G.I.2000.MultiBoosting:A technique for combining boosting and wagging.Machine Learning40(2):159–196.502REFERENCESP088407-REF.qxd  4/30/05  11:24 AM  Page 502Webb,G.I.,J.Boughton,and Z.Wang.2005.Not so Naïve Bayes:Aggregating one-dependence estimators.Machine Learning58(1):5–24.Weiser,M.1996.Open house.Review,the Web magazine ofthe InteractiveTelecommunications Program ofNew York University.March.Weiser,M.,and J.S.Brown.1997.The coming age ofcalm technology.In P.J.Denning and R.M.Metcalfe,editors,Beyond calculation: The next ﬁfty years.New York:Copernicus,pp.75–86.Weiss,S.M.,and N.Indurkhya.1998.Predictive data mining: A practical guide.SanFrancisco:Morgan Kaufmann.Wettschereck,D.,and T.G.Dietterich.1995.An experimental comparison ofthenearest-neighbor and nearest-hyperrectangle algorithms.Machine Learning19(1):5–28.Wild,C.J.,and G.A.F.Seber.1995.Introduction to probability and statistics.Depart-ment ofStatistics,University ofAuckland,New Zealand.Winston,P.H.1992.Artiﬁcial intelligence.Reading,MA:Addison-Wesley.Witten,I.H.2004.Text mining.In M.P.Singh,editor,Practical handbook ofinternet computing.Boca Raton,FL:CRC Press.Witten,I.H.,Z.Bray,M.Mahoui,and W.Teahan.1999a.Text mining:A new fron-tier for lossless compression.In J.A.Storer and M.Cohn,editors,Proceedingsofthe Data Compression Conference,Snowbird,UT.Los Alamitos,CA:IEEEComputer Society Press,pp.198–207.Witten,I.H.,A.Moffat,and T.C.Bell.1999b.Managing gigabytes: Compressing and indexing documents and images,second edition.San Francisco:MorganKaufmann.Wolpert,D.H.1992.Stacked generalization.Neural Networks5:241–259.Yang,Y.,and G.I.Webb.2001.Proportional k-interval discretization for Naïve Bayesclassiﬁers.In L.de Raedt and P.Flach,editors,Proceedings ofthe Twelfth European Conference on Machine Learning,Freiburg,Germany.Berlin:Springer-Verlag,pp.564–575.Yurcik,W.,J.Barlow,Y.Zhou,H.Raje,Y.Li,X.Yin,M.Haberman,D.Cai,and D.Searsmith.2003.Scalable data management alternatives to support data miningheterogeneous logs for computer network security.In Proceedings ofthe Workshop on Data Mining for Counter Terrorism and Security,San Francisco.Society for International and Applied Mathematics,Philadelphia,PA.Zheng,Z.,and G.Webb.2000.Lazy learning ofBayesian rules.Machine Learning41(1):53–84.REFERENCES503P088407-REF.qxd  4/30/05  11:24 AM  Page 503P088407-REF.qxd  4/30/05  11:24 AM  Page 504Aactivation function,234acuity,258AdaBoost,328AdaBoost.M1,321,416Add,395AddCluster,396,397AddExpression,397additive logistic regression,327–328additive regression,325–327AdditiveRegression,416AddNoise,400AD (all-dimensions) tree,280–283ADTree,408advanced methods.Seeimplementation—real-world schemesadversarial data mining,356–358aggregation,appropriate degree in datawarehousing,53Akaike Information Criterion (AIC),277Alberta Ingenuity Centre for MachineLearning,38algorithmsadditive logistic regression,327advanced methods,187–283.See alsoimplementation—real-world schemesassociation rule mining,112–119bagging,319basic methods,83–142.See alsoalgorithms-basic methodsBayesian network learning,277–283clustering,136–139clustering in Weka,418–419covering,105–112decision tree induction,97–105divide-and-conquer,107EM,265–266expanding examples into partial tree,208ﬁltering in Weka,393–403.See alsoﬁlteringalgorithmsincremental,346instance-based learning,128–136learning in Weka,403–414.See alsolearningalgorithmslinear models,119–128metalearning in Weka,414–4181R method,84–88perceptron learning rule,124RIPPER rule learner,206rule formation-incremental reduced-errorpruning,205separate-and-conquer,112statistical modeling,88–97stochastic,348Winnow,127See alsoindividual subject headings.all-dimensions (AD) tree,280–283alternating decision tree,329,330,343Analyzepanel,443–445analyzing purchasing patterns,27ancestor-of,48anomalies,314–315Index505P088407-INDEX.qxd  4/30/05  11:25 AM  Page 505506INDEXanomaly detection systems,357antecedent,ofrule,65AODE,405Apriori,419Apriori method,141area under the curve (AUC),173ARFF format,53–55converting ﬁles to,380–382Weka,370,371ARFFLoader,381,427arithmetic underﬂow,276assembling the data,52–53assessing performance oflearning scheme,286assignment ofkey phrases,353Associatepanel,392association learning,43association rules,69–70,112–119binary attributes,119generating rules efﬁciently,117–118item sets,113,114–115Weka,419–420association-rule learners in Weka,419–420attackers,357Attribute,451attribute(),480attribute discretization.Seediscretizingnumeric attributesattribute-efﬁcient,128attribute evaluation methods in Weka,421,422–423attribute ﬁlters in Weka,394,395–400,402–403attributeIndices,382attribute noise,313attribute-relation ﬁle format.SeeARFF formatattributes,49–52adding irrelevant,288Boolean,51class,53as columns in tables,49combinations of,65continuous,49discrete,51enumerated,51highly branching,86identiﬁcation code,86independent,267integer-valued,49nominal,49non-numeric,17numeric,49ordinal,51relevant,289rogue,59selecting,288subsets ofvalues in,80types in ARFF format,56weighting,237See alsoorderingsAttributeSelectedClassiﬁer,417attribute selection,286–287,288–296attribute evaluation methods in Weka,421,422–423backward elimination,292,294beam search,293best-ﬁrst search,293forward selection,292,294race search,295schemata search,295scheme-independent selection,290–292scheme-speciﬁc selection,294–296searching the attribute space,292–294search methods in Weka,421,423–425Weka,392–393,420–425AttributeSelection,403attribute subset evaluators in Weka,421,422AttributeSummarizer,431attribute transformations,287,305–311principal components analysis,306–309random projections,309text to attribute vectors,309–311time series,311attribute weighting method,237–238AUC (area under the curve),173audit logs,357authorship ascription,353AutoClass,269–270,271automatic data cleansing,287,312–315anomalies,314–315improving decision trees,312–313robust regression,313–314P088407-INDEX.qxd  4/30/05  11:25 AM  Page 506INDEX507automatic ﬁltering,315averaging over subnetworks,283axis-parallel class boundaries,242Bbackground knowledge,348backpropagation,227–233backtracking,209backward elimination,292,294backward pruning,34,192bagging,316–319Bagging,414–415bagging with costs,319–320bag ofwords,95balanced Winnow,128ball tree,133–135basic methods.Seealgorithms-basic methodsbatch learning,232Bayes,Thomas,141Bayesian classiﬁer.SeeNaïve BayesBayesian clustering,268–270Bayesian multinet,279–280Bayesian network,141,271–283AD tree,280–283Bayesian multinet,279–280caveats,276,277counting,280K2,278learning,276–283making predictions,272–276Markov blanket,278–279multiplication,275Naïve Bayes classiﬁer,278network scoring,277simplifying assumptions,272structure learning by conditionalindependence tests,280TAN (Tree Augmented Naïve Bayes),279Weka,403–406Bayesian network learning algorithms,277–283Bayesian option trees,328–331,343Bayesians,141Bayesian scoring metrics,277–280,283Bayes information,271BayesNet,405Bayes’s rule,90,181beam search,34,293beam width,34beer purchases,27Ben Ish Chai,358Bernoulli process,147BestFirst,423best-ﬁrst search,293best-matching node,257bias,32deﬁned,318language,32–33multilayer perceptrons,225,226overﬁtting-avoidance,34–35perceptron learning rule,124search,33–34what is it,317bias-variance decomposition,317,318big data (massive datasets),346–349binningequal-frequency,298equal-interval,298equal-width,342binomial coefﬁcient,218bits,102boolean,51,68boosting,321–325,347boosting in Weka,416bootstrap aggregating,318bootstrap estimation,152–153British Petroleum,28buildClassiﬁer(),453,472,482CC4.5,105,198–199C5.0,199calm computing,359,362capitalization conventions,310CAPPS (Computer Assisted Passenger Pre-Screening System),357CART (Classiﬁcation And Regression Tree),29,38,199,253categorical attributes,49.See alsonominalattributescategory utility,260–262P088407-INDEX.qxd  4/30/05  11:25 AM  Page 507508INDEXcausal relations,350CfsSubsetEval,422chain rule (probability theory),275character sets,310ChiSquaredAttributeEval,423chi-squared test,302circular ordering,51city-block metric,129ClassAssigner,431class attribute,43class distribution,304class hierarchy in Weka,471–483classiﬁcation,121Classiﬁcation And Regression Tree (CART),29,38,199,253classiﬁcation learning,43classiﬁcation problems,42classiﬁcation rules,65–69,200–214converting decision trees to,198criteria for choosing tests,200–201decision list,11different from association rules,42global optimization,205–207good (worthwhile) rules,202–205missing values,201–202numeric attributes,202partial decision tree,207–210pruning,203,205replicated subtree problem,66–68RIPPER rule learner,205rules with exceptions,210–213Weka,408–409Classiﬁer,453classiﬁer in Weka,366,471–483classiﬁer algorithms.See learning algorithmsclassiﬁer errors in Weka,379ClassiﬁerPerformanceEvaluator,431classiﬁerspackage,453–455ClassiﬁerSubsetEval,422Classiﬁersuperclass,480classifyInstance(),453,480–481Classifypanel,373,384classify text ﬁles into two categories,461–469Classit,271class noise,313ClassOrder,403class summary,451ClassValuePicker,431cleaning data,52,60automatic,312closed world assumption,45ClustererPerformanceEvaluator,431clustering,43,136–139,254–271anomalies,258basic assumption,270Bayesian,268–270category utility,260–262Clusterpanel (Weka),391–392document,353EM algorithm,265–266faster distance calculations,138–139hierarchical,139how many clusters?,254–255incremental,255–260k-means,137–138MDL principle,183–184merging,257mixture model,262–264,266–268output,81–82probability-based,262–265RBF network,234splitting,254–255,257unlabeled data,337–339clustering algorithms in Weka,418–419clustering for classiﬁcation,337ClusterMembership,396,397Clusterpanel,391–392Cobweb,271Cobweb in Weka,419co-EM,340column separation,336combining multiple models,287,315–336additive logistic regression,327–328additive regression,325–327bagging,316–319bagging with costs,319–320boosting,321–325error-correcting output codes,334–336logistic model trees,331option trees,328–331P088407-INDEX.qxd  4/30/05  11:25 AM  Page 508INDEX509randomization,320–321stacking,332–334command-line interface,449–459class,450,452classiﬁerspackage,453–455corepackage,451,452generic options,456–458instance,450Javadoc indices,456package,451,452scheme-speciﬁc options,458–459starting up,449–450weka.associations,455weka.attributeSelection,455weka.clusterers,455weka.estimators,455weka.ﬁlters,455command-line options,456–459comma-separated value (CSV) format,370,371comparing data mining methods,153–157ComplementNaiveBayes,405compression techniques,362computational learning theory,324computeEntropy(),480Computer Assisted Passenger Pre-ScreeningSystem (CAPPS),357computer network security,357computer software.SeeWeka workbenchconcept,42concept description,42concept description language,32concept representation,82.See alsoknowledgerepresentationconditional independence,275conditional likelihood for scoring networks,280,283conﬁdence,69,113,324Conﬁdence,420conﬁdence tests,154–157,184conﬂict resolution strategies,82confusion matrix,163conjunction,65ConjunctiveRule,408–409consensus ﬁlter,342consequent,ofrule,65ConsistencySubsetEval,422constrained quadratic optimization,217consumer music,359contact lens data,6,13–15continuous attributes,49.See alsonumericattributescontinuous monitoring,28–29converting discrete to numeric attributes,304–305convex hull,171,216Conviction,420Copy,395corepackage,451,452corrected resampled t-test,157correlation coefﬁcient,177–179cost curves,173–176cost matrix,164–165cost oferrors,161–176bagging,319–320cost curves,173–176cost-sensitive classiﬁcation,164–165cost-sensitive learning,165–166Kappa statistic,163–164lift charts,166–168recall-precision curves,171–172ROC curves,168–171cost-sensitive classiﬁcation,164–165CostSensitiveClassiﬁer,417cost-sensitive learning,165–166cost-sensitive learning in Weka,417co-training,339–340covariance matrix,267,307coverage,ofassociation rules,69covering algorithm,106–111cow culling,3–4,37,161–162CPU performance data,16–17credit approval,22–23cross-validated ROC curves,170cross-validation,149–152,326inner,286outer,286repeated,144CrossValidationFoldMaker,428,431CSV format,370,371P088407-INDEX.qxd  4/30/05  11:25 AM  Page 509510INDEXCSVLoader,381cumulative margin distribution in Weka,458curvescost,173lift,166recall-precision,171ROC,168customer support and service,28cutoffparameter,260CVParameterSelection,417cybersecurity,29Ddairy farmers (New Zealand),3–4,37,161–162data assembly,52–53data cleaning,52–60.See alsoautomatic datacleansingdata engineering.Seeengineering input andoutputdata integration,52data mining,4–5,9data ownership rights,35data preparation,52–60data transformation.SeeattributetransformationsDataVisualizer,389,390,430data warehouse,52–53date attributes,55decision list,11,67decision nodes,328decision stump,325DecisionStump,407,453,454decision table,62,295DecisionTable,408decision tree,14,62–65,97–105complexity ofinduction,196converting to rules,198data cleaning,312–313error rates,192–196highly branching attributes,102–105missing values,63,191–192multiclass case,107multivariate,199nominal attribute,62numeric attribute,62,189–191partial,207–210pruning,192–193,312replicated subtree,66rules,198subtree raising,193,197subtree replacement,192–193,197three-way split,63top-down induction,97–105,196–198two-way split,62univariate,199Weka,406–408Weka’s User Classifer facility,63–65Decorate,416deduction,350default rule,110degrees offreedom,93,155delta,311dendrograms,82denormalization,47density function,93diagnosis,25–26dichotomy,51directed acyclic graph,272direct marketing,27discrete attributes,50.See alsonominalattributesDiscretize,396,398,402discretizing numeric attributes,287,296–305chi-squared test,302converting discrete to numeric attributes,304–305entropy-based discretization,298–302error-based discretization,302–304global discretization,297local discretization,297supervised discretization,297,298unsupervised discretization,297–298Weka,398disjunction,32,65disjunctive normal form,69distance functions,128–129,239–242distributed experiments in Weka,445distribution,304distributionForInstance(),453,481divide-and-conquer.Seedecision treeP088407-INDEX.qxd  4/30/05  11:25 AM  Page 510INDEX511document classiﬁcation,94–96,352–353document clustering,353domain knowledge,20,33,349–351double-consequent rules,118duplicate data,59dynamic programming,302Eearly stopping,233easy instances,322ecological applications,23,28eigenvalue,307eigenvector,307Einstein,Albert,180electricity supply,24–25electromechanical diagnosis application,14411-point average recall,172EM,418EM algorithm,265–266EM and co-training,340–341EM procedure,337–338embedded machine learning,461–469engineering input and output,285–343attribute selection,288–296combining multiple models,315–336data cleansing,312–315discretizing numeric attributes,296–305unlabeled data,337–341See alsoindividual subject headingsentity extraction,353entropy,102entropy-based discretization,298–302enumerated attributes,50.See alsonominalattributesenumerating the concept space,31–32Epicurus,183epoch,412equal-frequency binning,298equal-interval binning,298equal-width binning,342erroneous values,59error-based discretization,302–304error-correcting output codes,334–336error log,378error ratebias,317cost oferrors.Seecost oferrorsdecision tree,192–196deﬁned,144training data,145“Essay towards solving a problem in thedoctrine ofchances,An”(Bayes),141ethics,35–37Euclidean distance,78,128,129,237evaluation,143–185bootstrap procedure,152–153comparing data mining methods,153–157cost oferrors,161–176.See alsocost oferrorscross-validation,149–152leave-one-out cross-validation,151–152MDL principle,179–184numeric prediction,176–179predicting performance,146–149predicting probabilities,157–161training and testing,144–146evaluation(),482evaluation components in Weka,430,431Evaluationpanel,431example problemscontact lens data,6,13–15CPU performance data,16–17iris dataset,15–16labor negotiations data,17–18,19soybean data,18–22weather problem,10–12exceptions,70–73,210–213exclusive-or problem,67exemplardeﬁned,236generalized,238–239noisy,236–237redundant,236exemplar generalization,238–239,243ExhaustiveSearch,424Expand all paths,408expectation,265,267expected error,174expected success rate,147P088407-INDEX.qxd  4/30/05  11:25 AM  Page 511512INDEXExperimenter,437–447advanced panel,443–445Analyzepanel,443–445analyzing the results,440–441distributing processing over severalmachines,445–447running an experiment,439–440simple setup,441–442starting up,438–441subexperiments,447Explorer,369–425ARFF,370,371,380–382Associatepanel,392association-rule learners,419–420attribute evaluation methods,421,422–423attribute selection,392–393,420–425boosting,416classiﬁer errors,379Classifypanel,384clustering algorithms,418–419Clusterpanel,391–392CSV format,370,371error log,378ﬁle format converters,380–382ﬁltering algorithms,393–403ﬁlters,382–384J4.8,373–377learning algorithms,403–414.See alsolearning algorithmsmetalearning algorithms,414–418models,377–378panels,380Preprocesspanel,380search methods,421,423–425Select attributespanel,392–393starting up,369–379supervised ﬁlters,401–403training/testing learning schemes,384–387unsupervised attribute ﬁlters,395–400unsupervised instance ﬁlters,400–401User Classiﬁer,388–391Visualizepanel,393extraction problems,353,354FFahrenheit,Daniel,51fallback heuristic,239false negative (FN),162false positive (FP),162false positive rate,163False positive rate,378Familiar,360family tree,45tabular representation of,46FarthestFirst,419features.See attributesfeature selection,341.See alsoattributeselectionfeedforward networks,233ﬁelded applications,22continuous monitoring,28–29customer support and service,28cybersecurity,29diagnosis,25–26ecological applications,23,28electricity supply,24–25hazard detection system,23–24load forecasting,24–25loan application,22–23manufacturing processes,28marketing and sales,26–28oil slick detection,23preventive maintenance ofelectromechanical devices,25–26scientiﬁc applications,28ﬁle format converters,380–382ﬁle mining,49ﬁlter,290ﬁlter in Weka,382–384FilteredClassiﬁer,401,414ﬁltering algorithms in Weka,393–403sparse instances,401supervised ﬁlters,401–403unsupervised attribute ﬁlters,395–400unsupervised instance ﬁlters,400–401ﬁltering approaches,315ﬁlters menu,383ﬁnite mixture,262,263FirstOrder,399P088407-INDEX.qxd  4/30/05  11:25 AM  Page 512INDEX513Fisher,R.A.,15ﬂat ﬁle,45F-measure,172FN (false negatives),162folds,150forward pruning,34,192forward selection,292,294forward stagewise additive modeling,325–327Fourier analysis,25FP (false positives),162freedom,degrees of93,155functional dependencies,350functions in Weka,404–405,409–410Ggain ratio,104GainRatioAttributeEval,423gambling,160garbage in,garbage out.Seecost oferrors;datacleaning;error rateGaussian-distribution assumption,92Gaussian kernel function,252generalization as search,30–35bias,32–35enumerating concept space,31–32generalized distance functions,241–242generalized exemplars,236general-to-speciﬁc search bias,34genetic algorithms,38genetic algorithm search procedures,294,341GeneticSearch,424getOptions(),482getting to know your data,60global discretization,297globalInfo(),472global optimization,205–207Gosset,William,184gradient descent,227,229,230Grading,417graphical models,283GraphViewer,431gray bar in margin oftextbook (optionalsections),30greedy search,33GreedyStepwise,423–424growing set,202HHamming distance,335hand-labeled data,338hapax legomena,310hard instances,322hash table,280hazard detection system,23–24hidden attributes,272hidden layer,226,231,232hidden units,226,231,234hierarchical clustering,139highly-branching attribute,86high-performance rule inducers,188histogram equalization,298historical literary mystery,358holdout method,146,149–150,333homeland defense,357HTML,355hypermetrope,13hyperpipes,139Hyperpipes,414hyperplane,124,125hyperrectangle,238–239hyperspheres,133hypertext markup language (HTML),355hypothesis testing,29IIB1,413IB3,237IBk,413ID3,105Id3,404identiﬁcation code,86,102–104implementation—real-world schemes,187–283Bayesian networks,271–283classiﬁcation rules,200–214clustering,254–271decision tree,189–199instance-based,236–243linear models,214–235P088407-INDEX.qxd  4/30/05  11:25 AM  Page 513514INDEXimplementation—real-world schemes(continued)numeric prediction,243–254See alsoindividual subject headingsinaccurate values,59–60.See alsocost oferrors;data cleaning;error rateincremental algorithms,346incrementalClassiﬁer,434IncrementalClassiﬁerEvaluator,431incremental clustering,255–260incremental learning in Weka,433–435incremental reduced-error pruning,203,205independent attributes,267index(),472induction,29inductive logic programming,48,60,75,351Induct system,214industrial usage.Seeimplementation—real-world schemesinferring rudimentary rules,84–88InfoGainAttributeEval,422–423informational loss function,159–160,161information-based heuristic,201information extraction,354information gain,99information retrieval,171information value,102infrequent words,353inner cross-validation,286input,41–60ARFF format,53–55assembling the data,52–53attribute,49–52attribute types,56–57concept,42–45data engineering,286–287,288–315.See alsoengineering input and outputdata preparation,52–60getting to know your data,60inaccurate values,59–60instances,45missing values,58sparse data,55–56input layer,224instance in Weka,450Instance,451instance-based learning,78,128–136,235–243ball tree,133–135distance functions,128–129,239–242ﬁnding nearest neighbors,129–135generalized distance functions,241–242generalized exemplars,236kD-trees,130–132missing values,129pruning noisy exemplars,236–237redundant exemplars,236simple method,128–136,235–236weighting attributes,237–238Weka,413–414instance-based learning methods,291instance-based methods,34instance-based representation,76–80instance ﬁlters in Weka,394,400–401,403instances,45Instances,451instance space,79instance weights,166,321–322integer-valued attributes,49intensive care patients,29interval,88interval quantities,50–51intrusion detection systems,357invertSelection,382in vitrofertilization,3iris dataset,15–16iris setosa,15iris versicolor,15iris virginica,15ISO-8601 combined date and time format,55item,113item sets,113,114–115iterative distance-based clustering,137–138JJ4.8,373–377J48,404,450Javadoc indices,456JDBC database,445JRip,409junk email ﬁltering,356–357P088407-INDEX.qxd  4/30/05  11:25 AM  Page 514INDEX515KK2,278Kappa statistic,163–164kD-trees,130–132,136Kepler’s three laws ofplanetary motion,180kerneldeﬁned,235perceptron,223polynomial,218RBF,219sigmoid,219kernel density estimation,97kernel logistic regression,223kernel perceptron,222–223k-means,137–138k-nearest-neighbor method,78Knowledge Flow interface,427–435conﬁguring/connecting components,431–433evaluation components,430,431incremental learning,433–435starting up,427visualization components,430–431knowledge representation,61–82association rules,69–70.See alsoassociationrulesclassiﬁcation rules,65–69.See alsoclassiﬁcation rulesclusters,81–82.See alsoclusteringdecision table,62decision tree,62–65.See alsodecision treeinstance-based representation,76–80rules involving relations,73–75rules with exceptions,70–73,210–213trees for numeric prediction,76KStar,413Llabor negotiations data,17–18,19language bias,32–33language identiﬁcation,353Laplace,Pierre,91Laplace estimator,91,267,269large datasets,346–349law ofdiminishing returns,347lazy classiﬁers in Weka,405,413–414LBR,414learning,7–9learning algorithms in Weka,403–404algorithm,listed,404–405Bayesian classiﬁer,403–406functions,404–405,409–410lazy classiﬁers,405,413–414miscellaneous classiﬁers,405,414neural network,411–413rules,404,408–409trees,404,406–408learning rate,229,230least-absolute-error regression,220LeastMedSq,409–410leave-one-out cross-validation,151–152levels ofmeasurement,50level-0 model,332level-1 model,332Leverage,420Lift,420lift chart,166–168,172lift factor,166linear classiﬁcation,121–128linearly separable,124linear machine,142linear models,119–128,214–235backpropagation,227–233computational complexity,218kernel perceptron,222–223linear classiﬁcation,121–128linear regression,119–121logistic regression,121–125maximum margin hyperplane,215–217multilayer perceptrons,223–226,231,233nonlinear class boundaries,217–219numeric prediction,119–120overﬁtting,217–218perceptron,124–126RBF network,234support vector regression,219–222Winnow,126–128linear regression,77,119–121LinearRegression,387,409linear threshold unit,142P088407-INDEX.qxd  4/30/05  11:25 AM  Page 515516INDEXlistOptions(),482literary mystery,358LMT,408load forecasting,24–25loan application,22–23local discretization,297locally weighted linear regression,244,251–253,253–254,323locally weighted Naïve Bayes,252–253Logbutton,380logic programs,75logistic model trees,331logistic regression,121–125LogitBoost,328,330,331LogitBoost,416logit transformation,121log-likelihood,122–123,276,277log-normal distribution,268log-odds distribution,268LWL,414MM5¢program,384M5P,408M5Rules,409machine learning,6main(),453majority voting,343MakeDensityBasedClusterer,419MakeIndicator,396,398makeTree(),472,480Manhattan metric,129manufacturing processes,28margin,324margin curve,324market basket analysis,27market basket data,55marketing and sales,26–28Markov blanket,278–279Markov network,283massive datasets,346–349maximization,265,267maximum margin hyperplane,215–217maxIndex(),472MDL metric,277MDL principle,179–184mean absolute error,177–179mean-squared error,177,178measurement errors,59membership function,121memorization,76MergeTwoValues,398merging,257MetaCost,319,320MetaCost,417metadata,51,349,350metadata extraction,353metalearner,332metalearning algorithms in Weka,414–418metric tree,136minimum description length (MDL) principle,179–184miscellaneous classiﬁers in Weka,405,414missing values,58classiﬁcation rules,201–202decision tree,63,191–192instance-based learning,1291R,86mixture model,267–268model tree,246–247statistical modeling,92–94mixed-attribute problem,11mixture model,262–264,266–268MLnet,38ModelPerformanceChart,431model tree,76,77,243–251building the tree,245missing values,246–247nominal attributes,246pruning,245–246pseudocode,247–250regression tree induction,compared,243replicated subtree problem,250rules,250–251smoothing,244,251splitting,245,247what is it,250momentum,233monitoring,continuous,28–29MultiBoostAB,416P088407-INDEX.qxd  4/30/05  11:25 AM  Page 516INDEX517multiclass alternating decision trees,329,330,343MultiClassClassiﬁer,418multiclass learning problems,334MultilayerPerceptron,411–413multilayer perceptrons,223–226,231,233multinomial distribution,95multinomial Naïve Bayes,95,96multiple linear regression,326multiresponse linear regression,121,124multistage decision property,102multivariate decision trees,199MultiScheme,417myope,13NNaiveBayes,403,405Naïve Bayes,91,278clustering for classiﬁcation,337–338co-training,340document classiﬁcation,94–96limitations,96–97locally weighted,252–253multinomial,95,96power,96scheme-speciﬁc attribute selection,295–296selective,296TAN (Tree Augmented Naïve Bayes),279what can go wrong,91NaiveBayesMultinominal,405NaiveBayesSimple,403NaiveBayesUpdateable,405NBTree,408nearest-neighbor learning,78–79,128–136,235,242nested exceptions,213nested generalized exemplars,239network scoring,277network security,357neural networks,39,233,235,253neural networks in Weka,411–413n-gram proﬁles,353,361Nnge,409noisedata cleansing,312exemplars,236–237hand-labeled data,338robustness oflearning algorithm,306noisy exemplars,236–237nominal attributes,49,50,56–57,119Cobweb,271convert to numeric attributes,304–305decision tree,62mixture model,267model tree,246subset,88nominal quantities,50NominalToBinary,398–399,403non-axis-parallel class boundaries,242Non-Bayesians,141nonlinear class boundaries,217–219NonSparseToSparse,401normal-distribution assumption,92normalization,56Normalize,398,400normalize(),480normalized expected cost,175nuclear family,47null hypothesis,155numeric attribute,49,50,56–57axis-parallel class boundaries,242classiﬁcation rules,202Classit,271converting discrete attributes to,304–305decision tree,62,189–191discretizing,296–305.See alsoDiscretizingnumeric attributesinstance-based learning,128,129interval,88linear models,119linear ordering,349mixture model,2681R,86statistical modeling,92numeric-attribute problem,11numeric prediction,43–45,243–254evaluation,176–179forward stagewise additive modeling,325linear regression,119–120locally weighted linear regression,251–253P088407-INDEX.qxd  4/30/05  11:25 AM  Page 517518INDEXnumeric prediction (continued)model tree,244–251.See alsomodel treerules,251stacking,334trees,76,243NumericToBinary,399NumericTransform,397OO(n),196O(n2),196Obfuscate,396,400object editor,366,381,393Occam’s razor,180,183oil slick detection,231R procedure,84–88,139OneR,408OneRAttributeEval,423one-tailed,148online documentation,368Open DB,382optimizing performance in Weka,417OptionHandler,451,482option nodes,328option trees,328–331orderingscircular,349partial,349order-independent rules,67,112OrdinalClassClassiﬁer,418ordinal attributes,51ordinal quantities,50orthogonal,307outer cross-validation,286outliers,313,342outputdata engineering,287–288,315–341.See alsoengineering input and outputknowledge representation,61–82.See alsoknowledge representationoverﬁtting,86Bayesian clustering,268category utility,261forward stagewise additive regression,326MDL principle,181multilayer perceptrons,2331R,87statistical tests,30support vectors,217–218overﬁtting-avoidance bias,34overgeneralization,239,243overlapping hyperrectangles,239overlay data,53Ppace regression in Weka,410PaceRegression,410paired t-test,154,294pairwise classiﬁcation,123,410pairwise coupling,123pairwise plots,60parabola,240parallelization,347parameter tuning,286Part,409partial decision tree,207–210partial ordering,51partitioning instance space,79pattern recognition,39Percentage split,377perceptrondeﬁned,126kernel,223learning rule,124,125linear classiﬁcation,124–126multilayer,223–226,233voted,223perceptron learning rule,124,125permutation tests,362PKIDiscretize,396,398Poisson distribution,268Polygon,389Polyline,389polynomial kernel,218popular music,359postal ZIP code,57postpruning,34,192precision,171predicate calculus,82P088407-INDEX.qxd  4/30/05  11:25 AM  Page 518INDEX519predicting performance,146–149.See alsoevaluationpredicting probabilities,157–161PredictionAppender,431prediction nodes,329predictive accuracy in Weka,420PredictiveApriori,420Preprocesspanel,372,380prepruning,34,192presbyopia,13preventive maintenance ofelectromechanicaldevices,25–26principal components,307–308PrincipalComponents,423principal components analysis,306–309principle ofmultiple explanations,183prior knowledge,349–351prior probability,90PRISM,110–111,112,213Prism,409privacy,357–358probabilistic EM procedure,265–266probability-based clustering,262–265probability cost function,175probability density function,93programming.SeeWeka workbenchprogramming by demonstration,360promotional offers,27proportional k-interval discretization,298propositional calculus,73,82propositional rules,69pruningclassiﬁcation rules,203,205decision tree,192–193,312massive datasets,348model tree,245–246noisy exemplars,236–237overﬁtting-avoidance bias,34reduced-error,203pruning set,202pseudocodebasic rule learner,111model tree,247–2501R,85punctuation conventions,310Qquadratic loss function,158–159,161quadratic optimization,217Quinlan,J.Ross,29,105,198RR.R.Donnelly,28RacedIncrementalLogitBoost,416race search,295RaceSearch,424radial basis function (RBF) kernel,219,234radial basis function (RBF) network,234RandomCommittee,415RandomForest,407random forest metalearner in Weka,416randomization,320–321Randomize,400RandomProjection,400random projections,309RandomSearch,424RandomTree,407Ranker,424–425RankSearch,424ratio quantities,51RBF (Radial Basis Function) kernel,219,234RBF (Radial Basis Function) network,234RBFNetwork,410real-life applications.Seeﬁelded applicationsreal-life datasets,10real-world implementations.Seeimplementation—real-world schemesrecall,171recall-precision curves,171–172Rectangle,389rectangular generalizations,80recurrent neural networks,233recursion,48recursive feature elimination,291,341reduced-error pruning,194,203redundant exemplars,236regression,17,76RegressionByDiscretization,418regression equation,17regression tree,76,77,243reinforcement learning,38P088407-INDEX.qxd  4/30/05  11:25 AM  Page 519520INDEXrelational data,49relational rules,74relations,73–75relative absolute error,177–179relative error ﬁgures,177–179relative squared error,177,178RELIEF,341ReliefFAttributeEval,422religious discrimination,illegal,35remoteEngine.jar,446remote.policy,446Remove,382RemoveFolds,400RemovePercentage,401RemoveRange,401RemoveType,397RemoveUseless,397RemoveWithValues,401repeated holdout,150ReplaceMissingValues,396,398replicated subtree problem,66–68REPTree,407–408Resample,400,403residuals,325resubstitution error,145Ridor,409RIPPER rule learner,205–214ripple-down rules,214robo-soccer,358robust regression,313–314ROC curve,168–171,172root mean-squared error,178,179root relative squared error,178,179root squared error measures,177–179rote learning,76,354row separation,336ruleantecedent,65association,69–70,112–119classiﬁcation.Seeclassiﬁcation rulesconsequent,65decision lists,111–112double-consequent,118exceptions,with,70–72,210–213good (worthwhile),202–205nearest-neighbor,78–79numeric prediction,251order of(decision list),67partial decision trees,207–210propositional,73relational,74relations,and,73–75single-consequent,118trees,and,107,198Weka,408–409rule-based programming,82rules involving relations,73–75rules with exceptions,70–73,210–213Ssample problems.Seeexample problemssampling with replacement,152satellite images,evaluating,23ScatterPlotMatrix,430schemata search,295scheme-independent attribute selection,290–292scheme-speciﬁc attribute selection,294–296scientiﬁc applications,28scoring networks,277–280,283SDR (Standard Deviation Reduction),245search bias,33–34search engine spam,357search methods in Weka,421,423–425segment-challenge.arff,389segment-test.arff,389Select attributespanel,392–393selective Naïve Bayes,296semantic relation,349semantic Web,355semisupervised learning,337sensitivity,173separate-and-conquer technique,112,200sequential boosting-like scheme,347sequential minimal optimization (SMO)algorithm,410setOptions(),482sexual discrimination,illegal,35shapes problem,73sigmoid function,227,228P088407-INDEX.qxd  4/30/05  11:25 AM  Page 520INDEX521sigmoid kernel,219Simple CLI,371,449,450SimpleKMeans,418–419simple linear regression,326SimpleLinearRegression,409SimpleLogistic,410simplest-ﬁrst ordering,34simplicity-ﬁrst methodology,83,183single-attribute evaluators in Weka,421,422–423single-consequent rules,118single holdout procedure,150sister-of-relation,46–47SMO,410smoothinglocally weighted linear regression,252model tree,244,251SMOreg,410software programs.SeeWeka workbenchsorting,avoiding repeated,190soybean data,18–22spam,356–357sparse data,55–56sparse instance in Weka,401SparseToNonSparse,401speciﬁcity,173speciﬁc-to-general search bias,34splitData(),480splitter nodes,329splittingclustering,254–255,257decision tree,62–63entropy-based discretization,301massive datasets,347model tree,245,247subexperiments,447surrogate,247SpreadSubsample,403squared-error loss function,227squared error measures,177–179stacked generalization,332stacking,332–334Stacking,417StackingC,417stale data,60standard deviation reduction (SDR),245standard deviations from the mean,148Standardize,398standardizing,56statistical modeling,88–97document classiﬁcation,94–96missing values,92–94normal-distribution assumption,92numeric attributes,92–94statistics,29–30Statusbox,380step function,227,228stochastic algorithms,348stochastic backpropagation,232stopping criterion,293,300,326stopwords,310,352stratiﬁcation,149,151stratiﬁed holdout,149StratiﬁedRemoveFolds,403stratiﬁed cross-validation,149StreamableFilter,456string attributes,54–55string conversion in Weka,399string table,55StringToNominal,399StringToWordVector,396,399,401,462StripChart,431structural patterns,6structure learning by conditional independencetests,280student’s distribution with k–1 degrees offreedom,155student’s t-test,154,184subexperiments,447subsampling in Weka,400subset evaluators in Weka,421,422subtree raising,193,197subtree replacement,192–193,197success rate,173supervised attribute ﬁlters in Weka,402–403supervised discretization,297,298supervised ﬁlters in Weka,401–403supervised instance ﬁlters in Weka,402,403supervised learning,43support,69,113P088407-INDEX.qxd  4/30/05  11:25 AM  Page 521522INDEXsupport vector,216support vector machine,39,188,214,340support vector machine (SVM) classiﬁer,341support vector machines with Gaussiankernels,234support vector regression,219–222surrogate splitting,247SVMAttributeEval,423SVM classiﬁer (Support Vector Machine),341SwapValues,398SymmetricalUncertAttributeEval,423symmetric uncertainty,291systematic data errors,59–60Ttabular input format,119TAN (Tree Augmented Naïve Bayes),279television preferences/channels,28–29tenfold cross-validation,150,151Tertius,420test set,145TestSetMaker,431text mining,351–356text summarization,352text to attribute vectors,309–311TextViewer,430TF ¥IDF,311theory,180threat detection systems,3573-point average recall,172threefold cross-validation,150ThresholdSelector,418time series,311TimeSeriesDelta,400TimeSeriesTranslate,396,399–400timestamp,311TN (True Negatives),162tokenization,310tokenization in Weka,399top-down induction ofdecision trees,105toSource(),453toString(),453,481,483toy problems.Seeexample problemsTP (True Positives),162training and testing,144–146training set,296TrainingSetMaker,431TrainTestSplitMaker,431transformations.Seeattribute transformationstransforming a multiclass problem into a two-class one,334–335treeAD (All Dimensions),280–283alternating decision,329,330,343ball,133–135decision.Seedecision treelogistic model,331metric,136model,76,243.See alsomodel treenumeric prediction,76option,328–331regression,76,243Tree Augmented Naïve Bayes (TAN),279tree classiﬁer in Weka,404,406–408tree diagrams,82Trees(subpackages),451,453Tree Visualizer,389,390true negative (TN),162true positive (TP),162true positive rate,162–163True positive rate,378t-statistic,156t-test,154TV preferences/channels,28–29two-class mixture model,264two-class problem,73two-tailed test,156two-way split,63typographic errors,59Uubiquitous data mining,358–361unacceptable contracts,17Unclassiﬁed instances,377Undo,383unit,224univariate decision tree,199universal language,32P088407-INDEX.qxd  4/30/05  11:25 AM  Page 522INDEX523unlabeled data,337–341clustering for classiﬁcation,337co-training,339–340EM and co-training,340–341unmasking,358unsupervised attribute ﬁlters in Weka,395–400unsupervised discretization,297–298unsupervised instance ﬁlters in Weka,400–401unsupervised learning,84UpdateableClassiﬁer,456,482updateClassiﬁer(),482User Classiﬁer,63–65,388–391UserClassiﬁer,388user interfaces,367–368Use training set,377utility,category,260–262Vvalidation data,146variance,154,317Venn diagram,81very large datasets,346–349“Very simple classiﬁcation rules perform wellon most commonly used datasets”(Holte),88VFI,414visualization components in Weka,430–431Visualize classiﬁer errors,387Visualizepanel,393Visualize threshold curve,378Vote,417voted perceptron,223VotedPerceptron,410voting,315,321,347voting feature intervals,136Wweak learners,325weather problem example,10–12association rules for,115–117attribute space for,292–293as a classiﬁcation problem,42as a clustering problem,43–44converting data to ARFF format,370cost matrix for,457evaluating attributes in,85–86inﬁnite rules for,30item sets,113–115as a numeric prediction problem,43–44web mining,355–356weight decay,233weighted instances,252WeightedInstancesHandler,482weighting attributes,237–238weighting models,316weka.associations,455weka.attributeSelection,455weka.classiﬁers,453weka.classiﬁers.bayes.NaiveBayesSimple,472weka.classiﬁers.Classiﬁer,453weka.classiﬁers.lazy.IB1,472weka.classiﬁers.lazy.IBk,482,483weka.classiﬁers.rules.Prism,472weka.classiﬁers.trees,453weka.classiﬁers.trees.Id3,471,472weka.clusterers,455weka.core,451,452,482–483weka.estimators,455weka.ﬁlters,455Weka workbench,365–483class hierarchy,471–483classiﬁers,366,471–483command-line interface,449–459.See alsocommand-line interfaceelementary learning schemes,472embedded machine learning,461–469example application (classify text ﬁles intotwo categories),461–469Experimenter,437–447Explorer,369–425.See alsoExplorerimplementing classiﬁers,471–483introduction,365–368Knowledge Flow interface,427–435neural-network GUI,411object editor,366online documentation,368user interfaces,367–368William ofOccam,180P088407-INDEX.qxd  4/30/05  11:25 AM  Page 523524INDEXWinnow,410Winnow algorithm,126–128wisdom,deﬁned,37Wolpert,David,334word conversions,310World Wide Web mining,354–356wrapper,290,341,355wrapper induction,355WrapperSubsetEval,422writing classiﬁers in Weka,471–483Z0-1 loss function,1580.632 bootstrap,1521R method,84–88zero-frequency problem,160zero point,inherently deﬁned,51ZeroR,409ZIP code,57P088407-INDEX.qxd  4/30/05  11:25 AM  Page 524About the AuthorsIan H.Witten is a professor ofcomputer science at the University ofWaikato in New Zealand.He is a fellow ofthe Association for ComputingMachinery and the Royal Society ofNew Zealand.He received the 2004 IFIPNamur Award,a biennial honor accorded for outstanding contribution withinternational impact to the awareness ofsocial implications ofinformation andcommunication technology.His books include Managing gigabytes(1999) andHow to build a digital library(2003),and he has written many journal articlesand conference papers.Eibe Frankis a senior lecturer in computer science at the University ofWaikato.He has published extensively in the area ofmachine learning and sits on the edi-torial boards ofthe Machine Learning Journaland the Journal ofArtiﬁcial Intel-ligence Research.He has also served on the programming committees ofmanydata mining and machine learning conferences.As one ofthe core developersofthe Weka machine learning software that accompanies this book,he enjoysmaintaining and improving it.525P088407-EM.qxd  4/30/05  11:23 AM  Page 525